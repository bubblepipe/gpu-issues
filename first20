### [DISABLED test_comprehensive_reciprocal_cpu_float32 (__main__.TestInductorOpInfoCPU)](https://github.com/pytorch/pytorch/issues/136026)

**Created:** 2024-09-13T18:42:32Z

**Tags:** `triaged module: flaky-tests module: macos skipped oncall: pt2 module: inductor`

**Content:**

> Platforms: mac, macos
> 
> This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_comprehensive_reciprocal_cpu_float32&suite=TestInductorOpInfoCPU&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/30108333110).
> 
> Over the past 3 hours, it has been determined flaky in 5 workflow(s) with 18 failures and 5 successes.
> 
> **Debugging instructions (after clicking on the recent samples link):**
> DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
> To find relevant log snippets:
> 1. Click on the workflow logs linked above
> 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
> 3. Grep for `test_comprehensive_reciprocal_cpu_float32`
> 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.
> 
> 
> 
> <details><summary>Sample error message</summary>
> 
> ```
> Traceback (most recent call last):
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 1140, in test_wrapper
>     return test(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 1409, in only_fn
>     return fn(self, *args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2133, in wrapper
>     fn(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 1210, in dep_fn
>     return fn(slf, *args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 1210, in dep_fn
>     return fn(slf, *args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 1210, in dep_fn
>     return fn(slf, *args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1526, in wrapper
>     fn(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1466, in wrapper
>     fn(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/unittest/mock.py", line 1336, in patched
>     return func(*newargs, **newkeywargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/contextlib.py", line 79, in inner
>     return func(*args, **kwds)
>   File "/Users/ec2-user/runner/_work/pytorch/pytorch/test/inductor/test_torchinductor_opinfo.py", line 952, in inner
>     raise e
>   File "/Users/ec2-user/runner/_work/pytorch/pytorch/test/inductor/test_torchinductor_opinfo.py", line 944, in inner
>     fn(self, device, dtype, op)
>   File "/Users/ec2-user/runner/_work/pytorch/pytorch/test/inductor/test_torchinductor_opinfo.py", line 1190, in test_comprehensive
>     raise e
>   File "/Users/ec2-user/runner/_work/pytorch/pytorch/test/inductor/test_torchinductor_opinfo.py", line 1172, in test_comprehensive
>     self.check_model(
>   File "/Users/ec2-user/runner/_work/pytorch/pytorch/test/inductor/test_torchinductor.py", line 430, in check_model
>     actual = run(*example_inputs, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 465, in _fn
>     return fn(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 1278, in __call__
>     return self._torchdynamo_orig_callable(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 526, in __call__
>     return _compile(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 924, in _compile
>     guarded_code = compile_inner(code, one_graph, hooks, transform)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 666, in compile_inner
>     return _compile_inner(code, one_graph, hooks, transform)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_utils_internal.py", line 87, in wrapper_function
>     return function(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 699, in _compile_inner
>     out_code = transform_code_object(code, transform)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/bytecode_transformation.py", line 1322, in transform_code_object
>     transformations(instructions, code_options)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 219, in _fn
>     return fn(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py", line 634, in transform
>     tracer.run()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 2796, in run
>     super().run()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
>     while self.step():
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
>     self.dispatch_table[inst.opcode](self, inst)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 2987, in RETURN_VALUE
>     self._return(inst)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/symbolic_convert.py", line 2972, in _return
>     self.output.compile_subgraph(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1117, in compile_subgraph
>     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1369, in compile_and_call_fx_graph
>     compiled_fn = self.call_user_compiler(gm)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1416, in call_user_compiler
>     return self._call_user_compiler(gm)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1465, in _call_user_compiler
>     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/output_graph.py", line 1446, in _call_user_compiler
>     compiled_fn = compiler_fn(gm, self.example_inputs())
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/repro/after_dynamo.py", line 129, in __call__
>     compiled_gm = compiler_fn(gm, example_inputs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/repro/after_dynamo.py", line 129, in __call__
>     compiled_gm = compiler_fn(gm, example_inputs)
>   File "/Users/ec2-user/runner/_work/pytorch/pytorch/test/inductor/test_torchinductor.py", line 422, in compile_fx_wrapper
>     return compile_fx(model_, example_inputs_)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1525, in compile_fx
>     return aot_autograd(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 72, in __call__
>     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1071, in aot_module_simplified
>     compiled_fn = dispatch_and_compile()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 1056, in dispatch_and_compile
>     compiled_fn, _ = create_aot_dispatcher_function(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 522, in create_aot_dispatcher_function
>     return _create_aot_dispatcher_function(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_functorch/aot_autograd.py", line 759, in _create_aot_dispatcher_function
>     compiled_fn, fw_metadata = compiler_fn(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 586, in aot_dispatch_autograd
>     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1354, in fw_compiler_base
>     return _fw_compiler_base(model, example_inputs, is_inference)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1425, in _fw_compiler_base
>     return inner_compile(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 476, in compile_fx_inner
>     return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 85, in debug_wrapper
>     inner_compiled_fn = compiler_fn(gm, example_inputs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 662, in _compile_fx_inner
>     compiled_graph = FxGraphCache.load(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 1334, in load
>     compiled_graph = compile_fx_fn(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 571, in codegen_and_compile
>     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 879, in fx_codegen_and_compile
>     compiled_fn = graph.compile_to_fn()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1952, in compile_to_fn
>     return self.compile_to_module().call
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1878, in compile_to_module
>     return self._compile_to_module()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1906, in _compile_to_module
>     mod = PyCodeCache.load_by_key_path(
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2856, in load_by_key_path
>     mod = _reload_python_module(key, path)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/runtime/compile_tasks.py", line 45, in _reload_python_module
>     exec(code, mod.__dict__, mod.__dict__)
>   File "/var/folders/bm/fnn3xd1d39lcpbxrgwys1c140000gn/T/tmp15wqrrrq/sw/cswwmy5qv365dtxss4i4m5r4d4wdb3uz7ir3gcetczbmbkdipquy.py", line 44, in <module>
>     async_compile.wait(globals())
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/async_compile.py", line 286, in wait
>     scope[key] = result.result()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 3333, in result
>     return self.result_fn()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2357, in future
>     result = get_result()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2154, in load_fn
>     future.result()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/concurrent/futures/_base.py", line 446, in result
>     return self.__get_result()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
>     raise self._exception
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/concurrent/futures/thread.py", line 58, in run
>     result = self.fn(*self.args, **self.kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2195, in _worker_compile_cpp
>     cpp_builder.build()
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/cpp_builder.py", line 1517, in build
>     status = run_compile_cmd(build_cmd, cwd=_build_tmp_dir)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/cpp_builder.py", line 352, in run_compile_cmd
>     return _run_compile_cmd(cmd_line, cwd)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/_inductor/cpp_builder.py", line 346, in _run_compile_cmd
>     raise exc.CppCompileError(cmd, output) from e
> torch._dynamo.exc.BackendCompilerFailed: backend='compile_fx_wrapper' raised:
> CppCompileError: C++ compile error
> 
> Command:
> clang++ /var/folders/bm/fnn3xd1d39lcpbxrgwys1c140000gn/T/tmp15wqrrrq/k2/ck2iyh7epdx53mw6hotspffkoc5fvoh764apxjrpje2petezxxj3.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -shared -fPIC -undefined dynamic_lookup -O3 -DNDEBUG -ffast-math -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/include/python3.9 -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/include/python3.9 -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include/TH -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include/THC -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/include -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include/TH -I/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/include/THC -D_GLIBCXX_USE_CXX11_ABI=0 -lc10 -lomp -L/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib -L/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/lib -L/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/lib -L/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib -L/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/lib -o /var/folders/bm/fnn3xd1d39lcpbxrgwys1c140000gn/T/tmp15wqrrrq/k2/ck2iyh7epdx53mw6hotspffkoc5fvoh764apxjrpje2petezxxj3.so
> 
> Output:
> 
> 
> Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information
> 
> 
> You can suppress this exception and fall back to eager by setting:
>     import torch._dynamo
>     torch._dynamo.config.suppress_errors = True
> 
> 
> The above exception was the direct cause of the following exception:
> 
> Traceback (most recent call last):
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 2979, in wrapper
>     method(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 448, in instantiated_test
>     result = test(self, **param_kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py", line 1526, in wrapper
>     fn(*args, **kwargs)
>   File "/Users/ec2-user/runner/_work/_temp/conda_environment_10849472039/lib/python3.9/site-packages/torch/testing/_internal/common_device_type.py", line 1152, in test_wrapper
>     raise e_tracked from e
> Exception: Caused by sample input at index 2: SampleInput(input=Tensor[size=(), device="cpu", dtype=torch.float32], args=(), kwargs={}, broadcasts_input=False, name='')
> 
> To execute this test, run the following from the base repo dir:
>     PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=2 python test/inductor/test_torchinductor_opinfo.py TestInductorOpInfoCPU.test_comprehensive_reciprocal_cpu_float32
> 
> This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
> ```
> 
> </details>
> 
> 
> Test file path: `inductor/test_torchinductor_opinfo.py`
> 
> cc @clee2000 @malfet @albanD @ezyang @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire

---

### [FX code should have shape annotations on each node](https://github.com/pytorch/pytorch/issues/86445)

**Created:** 2022-10-07T04:15:41Z

**Tags:** `triaged module: aotdispatch`

**Content:**

> ### 🐛 Describe the bug
> 
> Today, when we trace and print out `fx_g.code`, we have
> ```
> def forward(self, x_1):
>     cos = torch.ops.aten.cos.default(x_1);  x_1 = None
>     cat = torch.ops.aten.cat.default([cos, cos]);  cos = None
>     return cat
> ```
> 
> It would be nice if this printed out something like
> ```
> def forward(self, x_1: f32[10]):
>     cos: f32[10] = torch.ops.aten.cos.default(x_1);  x_1 = None
>     cat: f32[10] = torch.ops.aten.cat.default([cos, cos]);  cos = None
>     return cat
> ```
> Even better if it worked with symbolic shapes.
> 
> I think Jax's read-out is quite nice.
> 
> <img width="485" alt="image" src="https://user-images.githubusercontent.com/6355099/194466839-23194e55-9950-4dff-9e79-c80fac120faf.png">
> 
> 
> ### Versions
> 
> N/A

---

### [DISABLED test_vjpvmap_nn_functional_conv3d_cuda_float32 (__main__.TestOperatorsCUDA)](https://github.com/pytorch/pytorch/issues/114630)

**Created:** 2023-11-27T21:42:44Z

**Tags:** `triaged module: flaky-tests skipped module: unknown module: functorch`

**Content:**

> Platforms: linux, rocm
> 
> This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vjpvmap_nn_functional_conv3d_cuda_float32&suite=TestOperatorsCUDA) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/19064481505).
> 
> Over the past 3 hours, it has been determined flaky in 2 workflow(s) with 3 failures and 2 successes.
> 
> **Debugging instructions (after clicking on the recent samples link):**
> DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
> To find relevant log snippets:
> 1. Click on the workflow logs linked above
> 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
> 3. Grep for `test_vjpvmap_nn_functional_conv3d_cuda_float32`
> 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.
> 
> 
> Test file path: `functorch/test_ops.py`
> 
> ResponseError: Client network socket disconnected before secure TLS connection was established, GET https://raw.githubusercontent.com/pytorch/pytorch/main/test/functorch/test_ops.py -1 (connected: true, keepalive socket: false, socketHandledRequests: 1, socketHandledResponses: 0)
> headers: {}
> 
> cc @zou3519 @Chillee @samdow @kshitij12345 @janeyx99

---

### [[torch.compile] raises AssertionError in `pattern_matcher.py` on cuda but succeeds on cpu](https://github.com/pytorch/pytorch/issues/100806)

**Created:** 2023-05-06T17:58:37Z

**Tags:** `triaged oncall: pt2 inductor_pattern_match`

**Content:**

> ### 🐛 Describe the bug
> 
> `torch.compile` raises AssertionError in `pattern_matcher.py` on cuda but succeeds on cpu
> 
> ```py
> import torch
> 
> torch.manual_seed(420)
> 
> class Model(torch.nn.Module):
> 
>     def __init__(self):
>         super(Model, self).__init__()
>         self.linear1 = torch.nn.Linear(10, 20)
>         self.linear2 = torch.nn.Linear(20, 30)
>         self.relu = torch.nn.ReLU()
> 
>     def forward(self, x):
>         x = self.linear1(x)
>         x = self.linear2(x)
>         x = torch.cat((x, x), dim=1)
>         x = x.view(-1, 2, 30)
>         x = x[:, 1, :]
>         x = self.relu(x)
>         return x
> 
> device = 'cuda'
> batch_size = 2
> x = torch.randn(batch_size, 10).to(device)
> func = Model().to(device)
> 
> with torch.no_grad():
>     func.train(False)
>     jit_func = torch.compile(func)
> 
>     res1 = func(x) # without jit
>     print(res1)
>     # succeed
> 
>     res2 = jit_func(x)
>     # /_inductor/pattern_matcher.py", line 869, in stable_topological_sort
>     # assert not waiting and len(ready) == len(graph.nodes)
>     # torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
>     # AssertionError:
> ```
> 
> If you set `device` to `cpu`, it will not raise any error
> 
> ### Versions
> 
> <details>
> <summary>Click to expand</summary>
> 
> ```
> Collecting environment information...
> PyTorch version: 2.1.0.dev20230503+cu118
> Is debug build: False
> CUDA used to build PyTorch: 11.8
> ROCM used to build PyTorch: N/A
> 
> OS: Ubuntu 22.04.1 LTS (x86_64)
> GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
> Clang version: 14.0.0-1ubuntu1
> CMake version: Could not collect
> Libc version: glibc-2.35
> 
> Python version: 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] (64-bit runtime)
> Python platform: Linux-5.19.5-051905-generic-x86_64-with-glibc2.35
> Is CUDA available: True
> CUDA runtime version: 11.5.119
> CUDA_MODULE_LOADING set to: LAZY
> GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3060
> Nvidia driver version: 510.108.03
> cuDNN version: Could not collect
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> Is XNNPACK available: True
> 
> CPU:
> Architecture:                    x86_64
> CPU op-mode(s):                  32-bit, 64-bit
> Address sizes:                   46 bits physical, 48 bits virtual
> Byte Order:                      Little Endian
> CPU(s):                          24
> On-line CPU(s) list:             0-23
> Vendor ID:                       GenuineIntel
> Model name:                      12th Gen Intel(R) Core(TM) i9-12900K
> CPU family:                      6
> Model:                           151
> Thread(s) per core:              2
> Core(s) per socket:              16
> Socket(s):                       1
> Stepping:                        2
> CPU max MHz:                     6700.0000
> CPU min MHz:                     800.0000
> BogoMIPS:                        6374.40
> Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault cat_l2 invpcid_single cdp_l2 ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdt_a rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities
> Virtualization:                  VT-x
> L1d cache:                       640 KiB (16 instances)
> L1i cache:                       768 KiB (16 instances)
> L2 cache:                        14 MiB (10 instances)
> L3 cache:                        30 MiB (1 instance)
> NUMA node(s):                    1
> NUMA node0 CPU(s):               0-23
> Vulnerability Itlb multihit:     Not affected
> Vulnerability L1tf:              Not affected
> Vulnerability Mds:               Not affected
> Vulnerability Meltdown:          Not affected
> Vulnerability Mmio stale data:   Not affected
> Vulnerability Retbleed:          Not affected
> Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
> Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
> Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence
> Vulnerability Srbds:             Not affected
> Vulnerability Tsx async abort:   Not affected
> 
> Versions of relevant libraries:
> [pip3] mypy-extensions==1.0.0
> [pip3] numpy==1.24.2
> [pip3] pytorch-triton==2.1.0+7d1a95b046
> [pip3] torch==2.1.0.dev20230503+cu118
> [pip3] torchaudio==2.1.0.dev20230503+cu118
> [pip3] torchvision==0.16.0.dev20230503+cu118
> [conda] numpy                     1.24.2                   pypi_0    pypi
> [conda] pytorch-triton            2.1.0+7d1a95b046          pypi_0    pypi
> [conda] torch                     2.1.0.dev20230503+cu118          pypi_0    pypi
> [conda] torchaudio                2.1.0.dev20230503+cu118          pypi_0    pypi
> [conda] torchvision               0.16.0.dev20230503+cu118          pypi_0    pypi
> ```
> 
> </details>
> 
> cc @ezyang @soumith @msaroufim @wconstab @ngimel @bdhirsh @anijain2305

---

### [Bug of pytorch 1.10 for NVIDIA RTX a6000](https://github.com/pytorch/pytorch/issues/71753)

**Created:** 2022-01-25T05:59:17Z

**Tags:** `needs reproduction module: cuda triaged`

**Content:**

> ### 🐛 Describe the bug
> 
> Hi there,
> 
> I ran my code below on a6000 with 2 GPUs or 4 GPUs. However, the CE loss becomes nan after just a few iterations. Then I check, it because the learnable parameters become nan after almost the first backpropagation. Then I test my code on other GPUs such as TITAN RTX, TITAN V, and Tesla V100 (32G). They work well on other GPUs except for RTX a6000. I am wondering if there is a bug on PyTorch for A6000. Could you guys please re-test my code on A6000 again to see if there is a PyTorch bug on A6000 or not?
> 
> The command I ran my code is  
> `python -m torch.distributed.launch --master_port=6396 --nproc_per_node=2 debug_train_dist.py --ngpu 2 --reduction 8 -lr 0.001 -epoch 80 -nb_worker 8 -bs 50`
> 
> The project code is below which helps you to reproduce. By the way, my PyTorch version on a6000 is 1.10.0.dev20210831. The PyTorch version I test on other GPUs is 1.6.0.
> 
> If you need other information, please do not hesitate to leave a message. Thanks in advance!
> 
> ### Versions
> 
> Collecting environment information...
> PyTorch version: 1.10.0.dev20210831
> Is debug build: False
> CUDA used to build PyTorch: 11.1
> ROCM used to build PyTorch: N/A
> 
> OS: Ubuntu 16.04.6 LTS (x86_64)
> GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
> Clang version: Could not collect
> CMake version: Could not collect
> Libc version: glibc-2.23
> 
> Python version: 3.8.11 (default, Aug  3 2021, 15:09:35)  [GCC 7.5.0] (64-bit runtime)
> Python platform: Linux-4.4.0-142-generic-x86_64-with-glibc2.17
> Is CUDA available: True
> CUDA runtime version: Could not collect
> GPU models and configuration: 
> GPU 0: NVIDIA RTX A6000
> GPU 1: NVIDIA RTX A6000
> GPU 2: NVIDIA RTX A6000
> GPU 3: NVIDIA RTX A6000
> 
> Nvidia driver version: 465.19.01
> cuDNN version: Could not collect
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> 
> Versions of relevant libraries:
> [pip3] numpy==1.20.3
> [pip3] torch==1.10.0.dev20210831
> [pip3] torchaudio==0.10.0.dev20210831
> [pip3] torchvision==0.11.0.dev20210831
> [conda] Could not collect
> 
> cc @ngimel

---

### [Obey sm_carveout (limit on number of SMs) in inductor persistent kernel](https://github.com/pytorch/pytorch/issues/145115)

**Created:** 2025-01-17T22:07:11Z

**Tags:** `triaged oncall: pt2 module: inductor`

**Content:**

> ### 🚀 The feature, motivation and pitch
> 
> See https://github.com/pytorch/pytorch/pull/144974#issuecomment-2599011250
> 
> cc @chauhang @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @aakhundov @eellison @lw 
> 
> ### Alternatives
> 
> _No response_
> 
> ### Additional context
> 
> _No response_

---

### [DISABLED test_dim_dynamic_specialization_serdes_nonstrict (__main__.SerDesExportNonStrictTestExport)](https://github.com/pytorch/pytorch/issues/154964)

**Created:** 2025-06-03T06:48:30Z

**Tags:** `triaged module: flaky-tests skipped oncall: pt2 oncall: export`

**Content:**

> Platforms: rocm, asan, linux, mac, macos, slow
> 
> 
> 
>   This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_dim_dynamic_specialization_serdes_nonstrict&suite=SerDesExportNonStrictTestExport&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/43348443177).
> 
>   Over the past 3 hours, it has been determined flaky in 3 workflow(s) with 6 failures and 3 successes.
> 
>   **Debugging instructions (after clicking on the recent samples link):**
>   DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
>   To find relevant log snippets:
>   1. Click on the workflow logs linked above
>   2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
>   3. Grep for `test_dim_dynamic_specialization_serdes_nonstrict`
>   4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.
>   
>   
>   Test file path: `export/test_serdes.py`
> 
>   For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.
> 
> cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @clee2000 @chauhang @penguinwu @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4

---

### [`torch.Tensor.index_select` Trigger heap-buffer-overflow with AddressSanitizer](https://github.com/pytorch/pytorch/issues/88940)

**Created:** 2022-11-12T16:19:59Z

**Tags:** `triaged module: sanitizers`

**Content:**

> ### 🐛 Describe the bug
> 
> A test case for `torch.Tensor.index_select` triggers heap-buffer-overflow error with address sanitizer. Without sanitizers, the test terminates normally.
> Test:
> ```python
> import torch
> 
> def test():
>     arg_1 = torch.rand([], dtype=torch.float32).clone()
>     arg_3 = torch.zeros([2], dtype=torch.int64).clone()
>     res = torch.Tensor.index_select(arg_1,0,arg_3,)
> 
> test()
> ```
> Error log:
> ```
> ==100125==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000284 at pc 0x7fb6f1b2e155 bp 0x7fff89752850 sp 0x7fff89752848
> WRITE of size 4 at 0x609000000284 thread T0
>     #0 0x7fb6f1b2e154 in at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&)::$_10::operator()() const TensorAdvancedIndexing.cpp
>     #1 0x7fb6f1b23bec in at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0x9d18bec) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #2 0x7fb6f1b33666 in at::native::index_select_cpu_(at::Tensor const&, long, at::Tensor const&) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0x9d28666) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #3 0x7fb6f388a01a in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (at::Tensor const&, long, at::Tensor const&), &at::(anonymous namespace)::(anonymous namespace)::wrapper__index_select(at::Tensor const&, long, at::Tensor const&)>, at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, long, at::Tensor const&>>, at::Tensor (at::Tensor const&, long, at::Tensor const&)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) RegisterCPU.cpp
>     #4 0x7fb6f27a3466 in at::Tensor c10::Dispatcher::redispatch<at::Tensor, at::Tensor const&, long, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, long, at::Tensor const&)> const&, c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) const (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0xa998466) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #5 0x7fb6f2546bd8 in at::_ops::index_select::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0xa73bbd8) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #6 0x7fb6f71eaca1 in torch::autograd::VariableType::(anonymous namespace)::index_select(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) VariableType_0.cpp
>     #7 0x7fb6f71e9ff3 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&), &torch::autograd::VariableType::(anonymous namespace)::index_select(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&)>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&>>, at::Tensor (c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&) VariableType_0.cpp
>     #8 0x7fb6f2546625 in at::_ops::index_select::call(at::Tensor const&, long, at::Tensor const&) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0xa73b625) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #9 0x7fb7131f6487 in torch::autograd::THPVariable_index_select(_object*, _object*, _object*) python_variable_methods.cpp
>     #10 0x55bcca48db9b in method_vectorcall_VARARGS_KEYWORDS /opt/conda/conda-bld/python-split_1649141344976/work/Objects/descrobject.c:348
>     #11 0x55bcca3cd754 in _PyObject_VectorcallTstate /opt/conda/conda-bld/python-split_1649141344976/work/Include/cpython/abstract.h:118
>     #12 0x55bcca3cd754 in PyObject_Vectorcall /opt/conda/conda-bld/python-split_1649141344976/work/Include/cpython/abstract.h:127
>     #13 0x55bcca3cd754 in call_function /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:5077
>     #14 0x55bcca3cd754 in _PyEval_EvalFrameDefault.cold.2984 /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:3489
>     #15 0x55bcca465283 in _PyEval_EvalFrame /opt/conda/conda-bld/python-split_1649141344976/work/Include/internal/pycore_ceval.h:40
>     #16 0x55bcca465283 in function_code_fastcall /opt/conda/conda-bld/python-split_1649141344976/work/Objects/call.c:330
>     #17 0x55bcca465283 in _PyFunction_Vectorcall /opt/conda/conda-bld/python-split_1649141344976/work/Objects/call.c:367
>     #18 0x55bcca3cbae5 in _PyObject_VectorcallTstate /opt/conda/conda-bld/python-split_1649141344976/work/Include/cpython/abstract.h:118
>     #19 0x55bcca3cbae5 in PyObject_Vectorcall /opt/conda/conda-bld/python-split_1649141344976/work/Include/cpython/abstract.h:127
>     #20 0x55bcca3cbae5 in call_function /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:5077
>     #21 0x55bcca3cbae5 in _PyEval_EvalFrameDefault.cold.2984 /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:3520
>     #22 0x55bcca464662 in _PyEval_EvalFrame /opt/conda/conda-bld/python-split_1649141344976/work/Include/internal/pycore_ceval.h:40
>     #23 0x55bcca464662 in _PyEval_EvalCode /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:4329
>     #24 0x55bcca51145b in _PyEval_EvalCodeWithName /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:4361
>     #25 0x55bcca51145b in PyEval_EvalCodeEx /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:4377
>     #26 0x55bcca46545a in PyEval_EvalCode /opt/conda/conda-bld/python-split_1649141344976/work/Python/ceval.c:828
>     #27 0x55bcca51150a in run_eval_code_obj /opt/conda/conda-bld/python-split_1649141344976/work/Python/pythonrun.c:1221
>     #28 0x55bcca541f74 in run_mod /opt/conda/conda-bld/python-split_1649141344976/work/Python/pythonrun.c:1242
>     #29 0x55bcca3e2986 in pyrun_file.cold.3080 /opt/conda/conda-bld/python-split_1649141344976/work/Python/pythonrun.c:1140
>     #30 0x55bcca547a2e in pyrun_simple_file /opt/conda/conda-bld/python-split_1649141344976/work/Python/pythonrun.c:450
>     #31 0x55bcca547a2e in PyRun_SimpleFileExFlags /opt/conda/conda-bld/python-split_1649141344976/work/Python/pythonrun.c:483
>     #32 0x55bcca54810a in pymain_run_file /opt/conda/conda-bld/python-split_1649141344976/work/Modules/main.c:379
>     #33 0x55bcca54810a in pymain_run_python /opt/conda/conda-bld/python-split_1649141344976/work/Modules/main.c:604
>     #34 0x55bcca54810a in Py_RunMain /opt/conda/conda-bld/python-split_1649141344976/work/Modules/main.c:683
>     #35 0x55bcca548308 in Py_BytesMain /opt/conda/conda-bld/python-split_1649141344976/work/Modules/main.c:1129
>     #36 0x7fb71a321d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
>     #37 0x7fb71a321e3f in __libc_start_main csu/../csu/libc-start.c:392:3
>     #38 0x55bcca4cf09f in _start (/home/yuyao/anaconda3/bin/python3.9+0x20109f)
> 
> 0x609000000284 is located 0 bytes to the right of 4-byte region [0x609000000280,0x609000000284)
> allocated by thread T0 here:
>     #0 0x7fb71a706817 in posix_memalign (/usr/lib/llvm-15/lib/clang/15.0.4/lib/linux/libclang_rt.asan-x86_64.so+0xd0817) (BuildId: c3e83b91f2d38fca7fefc78874591b233340d131)
>     #1 0x7fb6e7cb88e9 in c10::alloc_cpu(unsigned long) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libc10.so+0x12d8e9) (BuildId: 04808d6a15270bc9548529381a08b11ae3ad24c7)
>     #2 0x7fb6e7c4b325 in c10::DefaultCPUAllocator::allocate(unsigned long) const (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libc10.so+0xc0325) (BuildId: 04808d6a15270bc9548529381a08b11ae3ad24c7)
>     #3 0x7fb6f1a0e5e6 in at::native::resize_impl_cpu_(c10::TensorImpl*, c10::ArrayRef<long>, c10::OptionalArrayRef<long>, bool) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0x9c035e6) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #4 0x7fb6f1a0bd5c in at::native::resize_(at::Tensor const&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0x9c00d5c) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
>     #5 0x7fb6f1a0bbf5 in at::native::resize_output(at::Tensor const&, c10::ArrayRef<long>) (/home/yuyao/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.so+0x9c00bf5) (BuildId: c7876a8b2ea547346cadf9022e3da8df32f4bd9b)
> 
> SUMMARY: AddressSanitizer: heap-buffer-overflow TensorAdvancedIndexing.cpp in at::native::index_select_out_cpu_(at::Tensor const&, long, at::Tensor const&, at::Tensor&)::$_10::operator()() const
> Shadow bytes around the buggy address:
>   0x0c127fff8000: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
>   0x0c127fff8010: fa fa fa fa fa fa fa fa 04 fa fa fa fa fa fa fa
>   0x0c127fff8020: fa fa fa fa fa fa fa fa fd fd fa fa fa fa fa fa
>   0x0c127fff8030: fa fa fa fa fa fa fa fa 00 00 fa fa fa fa fa fa
>   0x0c127fff8040: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
> =>0x0c127fff8050:[04]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
>   0x0c127fff8060: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
>   0x0c127fff8070: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
>   0x0c127fff8080: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
>   0x0c127fff8090: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
>   0x0c127fff80a0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
> Shadow byte legend (one shadow byte represents 8 application bytes):
>   Addressable:           00
>   Partially addressable: 01 02 03 04 05 06 07 
>   Heap left redzone:       fa
>   Freed heap region:       fd
>   Stack left redzone:      f1
>   Stack mid redzone:       f2
>   Stack right redzone:     f3
>   Stack after return:      f5
>   Stack use after scope:   f8
>   Global redzone:          f9
>   Global init order:       f6
>   Poisoned by user:        f7
>   Container overflow:      fc
>   Array cookie:            ac
>   Intra object redzone:    bb
>   ASan internal:           fe
>   Left alloca redzone:     ca
>   Right alloca redzone:    cb
> ==100125==ABORTING
> ```
> 
> ### Versions
> 
> ```
> PyTorch version: 1.14.0a0+git6e5f736
> Is debug build: False
> CUDA used to build PyTorch: Could not collect
> ROCM used to build PyTorch: N/A
> 
> OS: Ubuntu 22.04.1 LTS (x86_64)
> GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0
> Clang version: 11.1.0-6
> CMake version: version 3.22.1
> Libc version: glibc-2.35
> 
> Python version: 3.9.12 (main, Apr  5 2022, 06:56:58)  [GCC 7.5.0] (64-bit runtime)
> Python platform: Linux-5.15.0-52-generic-x86_64-with-glibc2.35
> Is CUDA available: False
> CUDA runtime version: 11.8.89
> CUDA_MODULE_LOADING set to: N/A
> GPU models and configuration: 
> GPU 0: NVIDIA GeForce RTX 3090
> GPU 1: NVIDIA GeForce RTX 3090
> GPU 2: NVIDIA GeForce RTX 3090
> 
> Nvidia driver version: 515.65.01
> cuDNN version: Probably one of the following:
> /usr/lib/x86_64-linux-gnu/libcudnn.so.8.4.1
> /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.4.1
> /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.4.1
> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.4.1
> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.4.1
> /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.4.1
> /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.4.1
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> Is XNNPACK available: True
> 
> Versions of relevant libraries:
> [pip3] mypy-extensions==0.4.3
> [pip3] numpy==1.21.5
> [pip3] numpydoc==1.2
> [pip3] torch==1.14.0a0+git6e5f736
> [conda] blas                      1.0                         mkl  
> [conda] cudatoolkit               11.3.1               h2bc3f7f_2  
> [conda] mkl                       2021.4.0           h06a4308_640  
> [conda] mkl-service               2.4.0            py39h7f8727e_0  
> [conda] mkl_fft                   1.3.1            py39hd3c417c_0  
> [conda] mkl_random                1.2.2            py39h51133e4_0  
> [conda] numpy                     1.21.5           py39he7a7128_1  
> [conda] numpy-base                1.21.5           py39hf524024_1  
> [conda] numpydoc                  1.2                pyhd3eb1b0_0  
> [conda] torch                     1.14.0a0+git6e5f736          pypi_0    pypi
> ```

---

### [Non-contiguous tensor fails on MPS backend for `.cat` and `.stack` ](https://github.com/pytorch/pytorch/issues/78043)

**Created:** 2022-05-21T13:25:21Z

**Tags:** `triaged module: mps`

**Content:**

> ### 🐛 Describe the bug
> 
> > moved from comments on https://github.com/pytorch/pytorch/issues/77886
> ```python
> generator = transformers.pipeline(task="text-generation", model=model.to('mps'), tokenizer=tokenizer, device=torch.device("mps"))
> generator("This shall brake. ", max_length=200, use_cache=True)
> ```
> 
> would crash in:
> ```python
> File /opt/homebrew/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:72, in rotate_every_two(x)
>      70 x1 = x[:, :, :, ::2]
>      71 x2 = x[:, :, :, 1::2]
> ---> 72 x = torch.stack((-x2, x1), axis=-1)
>      73 return x.flatten(-2)
> 
> RuntimeError: Placeholder buffer size (14336) is not large enough to contain the Tensor storage of size 114688
> ```
> 
> 
> changing to this resolves this issue.
> ```python
>     x1 = x[:, :, :, ::2].contiguous()
>     x2 = x[:, :, :, 1::2]
> ```
> 
> this also happened to `.cat`
> ```python
> File /opt/homebrew/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:233, in GPTJAttention.forward(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)
>     230     q_rot = apply_rotary_pos_emb(q_rot, sincos, offset=offset)
>     232     key = torch.cat([k_rot, k_pass], dim=-1)
> --> 233     query = torch.cat([q_rot, q_pass], dim=-1)
>     234 else:
>     235     sincos = fixed_pos_embedding(key, 1, seq_len=seq_len)
> 
> RuntimeError: Placeholder buffer size (86016) is not large enough to contain the Tensor storage of size 114688
> ```
> 
> ### Versions
> ```
> Collecting environment information...
> PyTorch version: 1.13.0.dev20220521
> Is debug build: False
> CUDA used to build PyTorch: None
> ROCM used to build PyTorch: N/A
> 
> OS: macOS 12.4 (arm64)
> GCC version: Could not collect
> Clang version: 13.1.6 (clang-1316.0.21.2.5)
> CMake version: version 3.23.1
> Libc version: N/A
> 
> Python version: 3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ] (64-bit runtime)
> Python platform: macOS-12.4-arm64-arm-64bit
> Is CUDA available: False
> CUDA runtime version: No CUDA
> GPU models and configuration: No CUDA
> Nvidia driver version: No CUDA
> cuDNN version: No CUDA
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> Is XNNPACK available: True
> 
> Versions of relevant libraries:
> [pip3] numpy==1.22.4
> [pip3] torch==1.13.0.dev20220521
> [conda] numpy                     1.22.4                   pypi_0    pypi
> [conda] torch                     1.13.0.dev20220521          pypi_0    pypi
> ```

---

### [Option to hard fail for cudagraphs and/or more cudagraph logging](https://github.com/pytorch/pytorch/issues/124506)

**Created:** 2024-04-19T18:14:12Z

**Tags:** `triaged module: cuda graphs oncall: pt2`

**Content:**

> ### 🚀 The feature, motivation and pitch
> 
> We lack a way to know how much of the torch.compiled model has been cudagraphed since we silently fallback ("skipping cudagraphs due to"). The debugging workflow today relies on grepping through logs for cudagraph logs and matching them against graph id logs. 
> 
> Something like a hard fail config for users who want to ensure their model is always fully cudagraphed, or logging a final cudagraph report can help.
> 
> ```
> > TORCH_LOGS="+cudagraphs" wp python benchmarks/dynamo/torchbench.py --performance --training --amp --backend inductor --device cuda --print-memory --use-warm-peak-memory --compiled-autograd --only hf_Whisper
> loading model: 0it [00:08, ?it/s]
> cuda train hf_Whisper                         
> I0419 11:12:04.474000 140576568861824 torch/_inductor/cudagraph_trees.py:360] [__cudagraphs] recording cudagraph tree for graph without symints
> V0419 11:12:04.669000 140576568861824 torch/_inductor/cudagraph_trees.py:1984] [__cudagraphs] Running warmup of function 0
> I0419 11:12:08.337000 140576568861824 torch/_inductor/cudagraph_trees.py:360] [__cudagraphs] recording cudagraph tree for graph without symints
> V0419 11:12:08.337000 140576568861824 torch/_inductor/cudagraph_trees.py:1984] [__cudagraphs] Running warmup of function 1
> I0419 11:12:11.532000 140576568861824 torch/_inductor/cudagraph_trees.py:360] [__cudagraphs] recording cudagraph tree for graph without symints
> V0419 11:12:11.532000 140576568861824 torch/_inductor/cudagraph_trees.py:1984] [__cudagraphs] Running warmup of function 2
> I0419 11:12:12.893000 140576568861824 torch/_inductor/cudagraph_trees.py:360] [__cudagraphs] recording cudagraph tree for graph without symints
> V0419 11:12:12.893000 140576568861824 torch/_inductor/cudagraph_trees.py:1984] [__cudagraphs] Running warmup of function 3
> I0419 11:12:13.406000 140576568861824 torch/_inductor/cudagraph_trees.py:360] [__cudagraphs] recording cudagraph tree for graph without symints
> V0419 11:12:13.406000 140576568861824 torch/_inductor/cudagraph_trees.py:1984] [__cudagraphs] Running warmup of function 4
> skipping cudagraphs due to skipping cudagraphs due to cpu device
> W0419 11:12:28.727000 140576568861824 torch/_logging/_internal.py:1016] [18/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
> I0419 11:12:42.339000 140576568861824 torch/_inductor/cudagraph_trees.py:360] [__cudagraphs] recording cudagraph tree for graph without symints
> V0419 11:12:42.340000 140576568861824 torch/_inductor/cudagraph_trees.py:1984] [__cudagraphs] Running warmup of function 5
> V0419 11:12:42.363000 140576568861824 torch/_inductor/cudagraph_trees.py:1947] [__cudagraphs] Recording function 0 of graph recording id 0
> V0419 11:12:42.756000 140576568861824 torch/_inductor/cudagraph_trees.py:1947] [__cudagraphs] Recording function 1 of graph recording id 1
> V0419 11:12:43.127000 140576568861824 torch/_inductor/cudagraph_trees.py:1947] [__cudagraphs] Recording function 2 of graph recording id 2
> V0419 11:12:43.482000 140576568861824 torch/_inductor/cudagraph_trees.py:1947] [__cudagraphs] Recording function 3 of graph recording id 3
> V0419 11:12:43.843000 140576568861824 torch/_inductor/cudagraph_trees.py:1947] [__cudagraphs] Recording function 4 of graph recording id 4
> skipping cudagraphs due to skipping cudagraphs due to cpu device
> V0419 11:12:57.067000 140576568861824 torch/_inductor/cudagraph_trees.py:1947] [__cudagraphs] Recording function 5 of graph recording id 5
> skipping cudagraphs due to skipping cudagraphs due to cpu device
> memory: eager: 1.12 GB, dynamo: 0.76 GB, ratio: 1.47
> running benchmark: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 30.27it/s]
> 1.360x
> ```
> 
> ### Alternatives
> 
> _No response_
> 
> ### Additional context
> 
> _No response_
> 
> cc @mcarilli @ezyang @eellison @peterbell10 @msaroufim @bdhirsh @anijain2305 @chauhang @BoyuanFeng 

---

### [DISABLED test_binary_op_float_inf_nan__foreach_add_cuda_bfloat16 (__main__.TestForeachCUDA)](https://github.com/pytorch/pytorch/issues/125427)

**Created:** 2024-05-02T20:58:58Z

**Tags:** `triaged skipped module: mta`

**Content:**

> Platforms: linux
> 
> https://hud.pytorch.org/flakytest?name=test_binary_op_float_inf_nan__foreach_add_cuda_bfloat16&suite=TestForeachCUDA&limit=100
> https://github.com/pytorch/pytorch/actions/runs/8916429717/job/24489182393
> 
> https://github.com/pytorch/pytorch/actions/runs/8914161802/job/24482095352
> 
> ```
> 
> 2024-05-01T21:10:01.0535782Z 
> 2024-05-01T21:10:01.0536049Z ==================================== RERUNS ====================================
> 2024-05-01T21:10:01.0536924Z ___ TestForeachCUDA.test_binary_op_float_inf_nan__foreach_add_cuda_bfloat16 ____
> 2024-05-01T21:10:01.0537728Z Traceback (most recent call last):
> 2024-05-01T21:10:01.0538675Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 796, in test_binary_op_float_inf_nan
> 2024-05-01T21:10:01.0539619Z     self._binary_test(
> 2024-05-01T21:10:01.0540345Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 256, in _binary_test
> 2024-05-01T21:10:01.0541257Z     actual = op(inputs, self.is_cuda, is_fastpath)
> 2024-05-01T21:10:01.0542131Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 90, in __call__
> 2024-05-01T21:10:01.0543057Z     assert mta_called == (expect_fastpath and (not zero_size))
> 2024-05-01T21:10:01.0543708Z AssertionError
> 2024-05-01T21:10:01.0544364Z ___ TestForeachCUDA.test_binary_op_float_inf_nan__foreach_add_cuda_bfloat16 ____
> 2024-05-01T21:10:01.0545155Z Traceback (most recent call last):
> 2024-05-01T21:10:01.0546094Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 796, in test_binary_op_float_inf_nan
> 2024-05-01T21:10:01.0547153Z     self._binary_test(
> 2024-05-01T21:10:01.0547896Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 256, in _binary_test
> 2024-05-01T21:10:01.0548804Z     actual = op(inputs, self.is_cuda, is_fastpath)
> 2024-05-01T21:10:01.0549677Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 90, in __call__
> 2024-05-01T21:10:01.0551033Z     assert mta_called == (expect_fastpath and (not zero_size))
> 2024-05-01T21:10:01.0551683Z AssertionError
> 2024-05-01T21:10:01.0552172Z =================================== FAILURES ===================================
> 2024-05-01T21:10:01.0553052Z ___ TestForeachCUDA.test_binary_op_float_inf_nan__foreach_add_cuda_bfloat16 ____
> 2024-05-01T21:10:01.0553842Z Traceback (most recent call last):
> 2024-05-01T21:10:01.0554777Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 796, in test_binary_op_float_inf_nan
> 2024-05-01T21:10:01.0555705Z     self._binary_test(
> 2024-05-01T21:10:01.0556420Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 256, in _binary_test
> 2024-05-01T21:10:01.0557344Z     actual = op(inputs, self.is_cuda, is_fastpath)
> 2024-05-01T21:10:01.0558218Z   File "/var/lib/jenkins/workspace/test/test_foreach.py", line 90, in __call__
> 2024-05-01T21:10:01.0559137Z     assert mta_called == (expect_fastpath and (not zero_size))
> 2024-05-01T21:10:01.0559939Z AssertionError
> 2024-05-01T21:10:01.0561151Z - generated xml file: /var/lib/jenkins/workspace/test/test-reports/python-pytest/test_foreach/test_foreach-cb56312bfbd65887.xml -
> ```
> 
> 
> cc @crcrpar @mcarilli @janeyx99

---

### [Dataloader codeowner](https://github.com/pytorch/pytorch/issues/124473)

**Created:** 2024-04-19T12:37:52Z

**Tags:** `module: dataloader triaged`

**Content:**

> ### 🐛 Describe the bug
> 
> Do we have really a dataloader codeowner?
> https://github.com/pytorch/pytorch/blob/8e280862ffa0ff34e73f8763f84e4b77925a3570/CODEOWNERS#L118-L119
> 
> More in general for devx how we are handling the codeowners outboarding? I think that it is important to be transparent about MIA or vancancy after the outboarding and include codeowners handling in every outboarding process so it is clear what module are orphans.
> 
> ### Versions
> 
> n/a
> 
> cc @SsnL @VitalyFedyunin @ejguan @dzhulgakov

---

### [Segmentation fault with torch.compile in torchchat](https://github.com/pytorch/pytorch/issues/126669)

**Created:** 2024-05-20T06:00:08Z

**Tags:** `needs reproduction triaged oncall: pt2 module: aotinductor`

**Content:**

> ### 🐛 Describe the bug
> 
>   https://github.com/pytorch/torchchat/actions/runs/9154013584/job/25163857442?pr=824
>   
>   Average tokens/sec: 38.80
>   Memory used: 0.00 GB
>   + python generate.py --checkpoint-path checkpoints/stories15M/stories15M.pt --device cpu --dtype float32 --temperature 1.0 --sequential-prefill --compile
>   Using device=cpu AMD EPYC 7R32
>   Loading model...
>   Time to load model: 0.02 seconds
>   -----------------------------------------------------------
>   /exec: line 35:  4135 Segmentation fault      python generate.py --checkpoint-path ${MODEL_PATH} --device cpu --dtype ${DTYPE} --temperature 1.0 --sequential-prefill --compile
> 
> ### Versions
> 
> Pytorch CI
> 
> cc @ezyang @msaroufim @bdhirsh @anijain2305 @chauhang @desertfire @chenyang78

---

### [nn.Linear layer initialization formula wrong in docs](https://github.com/pytorch/pytorch/issues/149474)

**Created:** 2025-03-19T02:22:10Z

**Tags:** `module: docs module: nn triaged actionable`

**Content:**

> ### 📚 The doc issue
> 
> ![Image](https://github.com/user-attachments/assets/452341a2-05b3-48da-838a-6f4a9aa7eb1a)
> 
> But in implementation it's:
> 
> ![Image](https://github.com/user-attachments/assets/5e781175-b122-41b8-923a-27e7978b6533)
> 
> 
> 
> ### Suggest a potential alternative/fix
> 
> It should be:
> 
> ![Image](https://github.com/user-attachments/assets/d8ca945c-851d-44c9-afde-a31136d5b943)
> 
> cc @svekars @sekyondaMeta @AlannaBurke @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki

---

### [Runtime Error raised by `torch._native_multi_head_attention` working with `torch.cuda.amp.autocast`](https://github.com/pytorch/pytorch/issues/84396)

**Created:** 2022-09-01T04:48:12Z

**Tags:** `module: nn triaged module: amp (automated mixed precision)`

**Content:**

> ### 🐛 Describe the bug
> 
> Minimal reproducing example:
> ```python
> import torch
> import torch.nn as nn
> 
> class NativeMHA(nn.Module):
>     """
>     Modified from https://github.com/pytorch/pytorch/blob/v1.12.1/test/test_native_mha.py#L144-L166
>     """
> 
>     def __init__(self, embed_dim, num_heads, qkv, proj):
>         super().__init__()
>         self.qkv = qkv
>         self.proj = proj
>         self.embed_dim = embed_dim
>         self.num_heads = num_heads
> 
>     def forward(
>         self, q, k, v, key_padding_mask=None, need_weights=False, average_attn_weights=True
>     ):
>         return torch._native_multi_head_attention(
>             q,
>             k,
>             v,
>             self.embed_dim,
>             self.num_heads,
>             self.qkv.weight,
>             self.qkv.bias,
>             self.proj.weight,
>             self.proj.bias,
>             key_padding_mask,
>             need_weights=need_weights,
>             average_attn_weights=average_attn_weights,
>         )
> 
> device = "cuda:0"
> dtype = torch.float32
> batch_size = 2
> seq_length = 4
> embed_dim = 16
> num_heads = 4
> 
> qkv = torch.nn.Linear(embed_dim, 3 * embed_dim, device=device, dtype=dtype)
> proj = torch.nn.Linear(embed_dim, embed_dim, device=device, dtype=dtype)
> m = NativeMHA(embed_dim, num_heads, qkv, proj)
> x = torch.rand(batch_size, seq_length, embed_dim, device=device, dtype=dtype)
> 
> m(x, x, x)  # everything is ok
> with torch.autocast("cuda"):
>     m(x, x, x)  # raise runtime error
> ```
> 
> Expected results:
> ```
> RuntimeError: expected scalar type Half but found Float
> ```
> 
> ### Versions
> 
> ```
> Collecting environment information...
> PyTorch version: 1.12.1+cu113
> Is debug build: False
> CUDA used to build PyTorch: 11.3
> ROCM used to build PyTorch: N/A
> 
> OS: Ubuntu 20.04.5 LTS (x86_64)
> GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
> Clang version: Could not collect
> CMake version: version 3.16.3
> Libc version: glibc-2.31
> 
> Python version: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] (64-bit runtime)
> Python platform: Linux-5.15.0-1016-gcp-x86_64-with-glibc2.29
> Is CUDA available: True
> CUDA runtime version: 11.4.152
> GPU models and configuration:
> GPU 0: NVIDIA A100-SXM4-40GB
> GPU 1: NVIDIA A100-SXM4-40GB
> GPU 2: NVIDIA A100-SXM4-40GB
> GPU 3: NVIDIA A100-SXM4-40GB
> GPU 4: NVIDIA A100-SXM4-40GB
> GPU 5: NVIDIA A100-SXM4-40GB
> GPU 6: NVIDIA A100-SXM4-40GB
> GPU 7: NVIDIA A100-SXM4-40GB
> 
> Nvidia driver version: 470.141.03
> cuDNN version: Probably one of the following:
> /usr/lib/x86_64-linux-gnu/libcudnn.so.8.5.0
> /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.5.0
> /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.5.0
> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.5.0
> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.5.0
> /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.5.0
> /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.5.0
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> Is XNNPACK available: True
> 
> Versions of relevant libraries:
> [pip3] mypy-extensions==0.4.3
> [pip3] numpy==1.22.4
> [pip3] open-clip-torch==1.3.0
> [pip3] pytorchvideo==0.1.5
> [pip3] torch==1.12.1+cu113
> [pip3] torchaudio==0.12.1+cu113
> [pip3] torchvision==0.13.1+cu113
> [conda] Could not collect
> ```
> 
> cc @albanD @mruberry @jbschlosser @walterddr @kshitij12345 @saketh-are @bhosmer @cpuhrsch @erichan1 @mcarilli @ptrblck

---

### [DISABLED test_builtin_score_mods_different_block_size_float16_score_mod2_BLOCK_SIZE3_cuda_float16 (__main__.TestFlexAttentionCUDA)](https://github.com/pytorch/pytorch/issues/151393)

**Created:** 2025-04-16T01:03:17Z

**Tags:** `triaged module: flaky-tests skipped oncall: pt2 module: inductor`

**Content:**

> Platforms: linux
> 
> This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_builtin_score_mods_different_block_size_float16_score_mod2_BLOCK_SIZE3_cuda_float16&suite=TestFlexAttentionCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/40609417022).
> 
> Over the past 3 hours, it has been determined flaky in 11 workflow(s) with 22 failures and 11 successes.
> 
> **Debugging instructions (after clicking on the recent samples link):**
> DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
> To find relevant log snippets:
> 1. Click on the workflow logs linked above
> 2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
> 3. Grep for `test_builtin_score_mods_different_block_size_float16_score_mod2_BLOCK_SIZE3_cuda_float16`
> 4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.
> 
> 
> Test file path: `inductor/test_flex_attention.py`
> 
> cc @clee2000 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

---

### [DISABLED test_vmap_exhaustive_nn_functional_conv2d_strided_padding_dilation_no_bias_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA)](https://github.com/pytorch/pytorch/issues/157899)

**Created:** 2025-07-09T06:49:59Z

**Tags:** `module: rocm triaged module: flaky-tests skipped module: functorch`

**Content:**

> Platforms: rocm
> 
>   This test was disabled because it is failing in CI. See [recent examples](https://hud.pytorch.org/flakytest?name=test_vmap_exhaustive_nn_functional_conv2d_strided_padding_dilation_no_bias_cuda_float32&suite=TestVmapOperatorsOpInfoCUDA&limit=100) and the most recent trunk [workflow logs](https://github.com/pytorch/pytorch/runs/45610078739).
> 
>   Over the past 6 hours, it has been determined flaky in 7 workflow(s) with 0 failures and 7 successes.
> 
>   **Debugging instructions (after clicking on the recent samples link):**
>   DO NOT ASSUME THINGS ARE OKAY IF THE CI IS GREEN. We now shield flaky tests from developers so CI will thus be green but it will be harder to parse the logs.
>   To find relevant log snippets:
>   1. Click on the workflow logs linked above
>   2. Click on the Test step of the job so that it is expanded. Otherwise, the grepping will not work.
>   3. Grep for `test_vmap_exhaustive_nn_functional_conv2d_strided_padding_dilation_no_bias_cuda_float32`
>   4. There should be several instances run (as flaky tests are rerun in CI) from which you can study the logs.
>   
>   
>   Test file path: `functorch/test_vmap.py`
> 
>   For all disabled tests (by GitHub issue), see https://hud.pytorch.org/disabled.
> 
> cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @clee2000 @zou3519 @Chillee @samdow @kshitij12345

---

### [LazyTensorCore DataCache segfaults on Add when cache size is zero](https://github.com/pytorch/pytorch/issues/113672)

**Created:** 2023-11-14T19:00:09Z

**Tags:** `high priority triage review module: crash triaged module: lazy`

**Content:**

> ### 🐛 Describe the bug
> 
> LazyTensorCore's DataCacheArena has a default cache size of 128, but that can be changed by setting `FLAGS_torch_lazy_device_data_cache_size`. When the cache size is zero, adding a new element to the cache occasionally causes a segfault. Inspecting with valgrind shows a few invalid reads.
> 
> Upon closer inspection, it's happening because when the cache size is zero, we invalidate the iterator here (https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/core/cache.h#L46-L47) and then try to access the iterator to return the object (https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/core/cache.h#L49). Accessing the iterator after it's been erased from the std container is undefined behavior.
> 
> Here's a simple repro of the valgrind invalid read:
> ```c++
> #include <torch/csrc/lazy/core/cache.h>
> 
> using namespace torch::lazy;
> using DataPtr = std::shared_ptr<int>;
> 
> int main(int argc, char* argv[]) {
>     Cache<std::string, int> c(0);
>     DataPtr p = c.Get("foo");
>     if (p == nullptr) {
>         std::string bar = "bar";
>         DataPtr v = std::make_shared<int>(10);
>         c.Add(std::move(bar), v);
>     }
>     return 0;
> }
> ```
> 
> And here's the valgrind output of this program:
> ```
> ==9065== Memcheck, a memory error detector
> ==9065== Copyright (C) 2002-2022, and GNU GPL'd, by Julian Seward et al.
> ==9065== Using Valgrind-3.19.0 and LibVEX; rerun with -h for copyright info
> ==9065== Command: torch_cache_test
> ==9065== 
> ==9065== Invalid read of size 8
> ==9065==    at 0x40209C: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==  Address 0x5cfad70 is 16 bytes inside a block of size 32 free'd
> ==9065==    at 0x483AE6B: operator delete(void*) (vg_replace_malloc.c:923)
> ==9065==    by 0x402042: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==  Block was alloc'd at
> ==9065==    at 0x4838E0F: operator new(unsigned long) (vg_replace_malloc.c:422)
> ==9065==    by 0x401DB9: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065== 
> ==9065== Invalid read of size 8
> ==9065==    at 0x401F0D: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==  Address 0x5cfad10 is 48 bytes inside a block of size 64 free'd
> ==9065==    at 0x483AE6B: operator delete(void*) (vg_replace_malloc.c:923)
> ==9065==    by 0x40209B: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==  Block was alloc'd at
> ==9065==    at 0x4838E0F: operator new(unsigned long) (vg_replace_malloc.c:422)
> ==9065==    by 0x401CEA: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065== 
> ==9065== Invalid read of size 8
> ==9065==    at 0x401F14: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==  Address 0x5cfad18 is 56 bytes inside a block of size 64 free'd
> ==9065==    at 0x483AE6B: operator delete(void*) (vg_replace_malloc.c:923)
> ==9065==    by 0x40209B: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==  Block was alloc'd at
> ==9065==    at 0x4838E0F: operator new(unsigned long) (vg_replace_malloc.c:422)
> ==9065==    by 0x401CEA: torch::lazy::Cache<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::Add(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::shared_ptr<int>) (in /workspace/obj-default/bin/torch_cache_test)
> ==9065==    by 0x401381: main (in /workspace/obj-default/bin/torch_cache_test)
> ==9065== 
> ==9065== 
> ==9065== HEAP SUMMARY:
> ==9065==     in use at exit: 0 bytes in 0 blocks
> ==9065==   total heap usage: 5 allocs, 5 frees, 72,848 bytes allocated
> ==9065== 
> ==9065== All heap blocks were freed -- no leaks are possible
> ==9065== 
> ==9065== For lists of detected and suppressed errors, rerun with: -s
> ==9065== ERROR SUMMARY: 3 errors from 3 contexts (suppressed: 0 from 0)
> ```
> 
> ### Versions
> 
> PyTorch version: 2.1.0
> Is debug build: True
> CUDA used to build PyTorch: None
> ROCM used to build PyTorch: N/A
> 
> OS: CentOS Linux release 7.9.2009 (Core) (x86_64)
> GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
> Clang version: 3.4.2 (tags/RELEASE_34/dot2-final)
> CMake version: version 2.8.12.2
> Libc version: glibc-2.17
> 
> Python version: 3.8.13 (default, Aug 16 2022, 12:16:29)  [GCC 9.3.1 20200408 (Red Hat 9.3.1-2)] (64-bit runtime)
> Python platform: Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-glibc2.2.5
> Is CUDA available: False
> CUDA runtime version: No CUDA
> CUDA_MODULE_LOADING set to: N/A
> GPU models and configuration: No CUDA
> Nvidia driver version: No CUDA
> cuDNN version: No CUDA
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> Is XNNPACK available: True
> 
> CPU:
> Architecture:          x86_64
> CPU op-mode(s):        32-bit, 64-bit
> Byte Order:            Little Endian
> CPU(s):                8
> On-line CPU(s) list:   0-7
> Thread(s) per core:    2
> Core(s) per socket:    4
> Socket(s):             1
> NUMA node(s):          1
> Vendor ID:             GenuineIntel
> CPU family:            6
> Model:                 85
> Model name:            Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz
> Stepping:              4
> CPU MHz:               2499.998
> BogoMIPS:              4999.99
> Hypervisor vendor:     KVM
> Virtualization type:   full
> L1d cache:             32K
> L1i cache:             32K
> L2 cache:              1024K
> L3 cache:              33792K
> NUMA node0 CPU(s):     0-7
> Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single rsb_ctxsw fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 ida arat pku ospke
> 
> Versions of relevant libraries:
> [pip3] torch==2.1.0a0
> [conda] Could not collect
> 
> cc @ezyang @gchanan @zou3519 @kadeng

---

### [`reinterpret_cast<float&>` violates strict aliasing in inductor cpp codegen](https://github.com/pytorch/pytorch/issues/110807)

**Created:** 2023-10-08T03:53:13Z

**Tags:** `triaged`

**Content:**

> >`reinterpret_cast<float&>` on an int is UB because it violates strict aliasing. What is this trying to do exactly? How about adding a `c10::bit_cast` that uses `std::bit_cast` if c++20 is available and the suggested `std::memcpy` polyfill at https://en.cppreference.com/w/cpp/numeric/bit_cast otherwise?
> 
> _Originally posted by @swolchok in https://github.com/pytorch/pytorch/issues/102920#issuecomment-1747679443_
>             
> 
> cc @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @aakhundov @ColinPeppler

---

### [[Dynamo] Key Mismatch When Loading Checkpoints Trained with Dynamo](https://github.com/pytorch/pytorch/issues/94575)

**Created:** 2023-02-10T05:17:32Z

**Tags:** `high priority module: serialization triaged oncall: pt2 module: dynamo`

**Content:**

> ### 🐛 Describe the bug
> 
> We find that the state key of a model trained with TorchDynamo and TorchInductor changed. More specifically, the original model has a state key like `model.param` while a model trained with TorchDynamo has a state key like `model._orig_mod.param`. This can be an issue when someone else is trying to load the checkpoint, and it is even worse if the one loading the checkpoint doesn't perform strict state key check.
> 
> One might argue that it's the user's responsibility to save the checkpoint with `model._orig_mod` instead of `model`. But as the name with leading underscore `_orig_mod` suggested, I think the user shouldn't care about which model he is trying to save. Instead, I'd suggest removing the key `_orig_mod` inside PyTorch when loading the checkpoint.
> 
> Here's a minaml example to reproduce the issue:
> 
> ```python
> #!/usr/bin/env python
> 
> import torch
> from torch._dynamo import optimize
> torch.manual_seed(0)
> 
> class Model(torch.nn.Module):
>     def __init__(self, channels):
>         super(Model, self).__init__()
>         self.layers = torch.nn.Sequential(
>             torch.nn.Conv2d(channels, channels, 1),
>             torch.nn.ReLU(),
>             torch.nn.Conv2d(channels, channels, 1),
>             torch.nn.ReLU(),
>         )
> 
>     def forward(self, x):
>         return self.layers(x)
> 
> n, c, h, w = 8, 640, 16, 16
> x = torch.randn((n, c, h, w))
> 
> model = Model(c)
> jit_model = optimize("inductor")(model)
> jit_model(x)
> 
> torch.save(jit_model.state_dict(), "model.pt")
> 
> # Someone else is trying to load the checkpoint
> model = Model(c)
> model.load_state_dict(torch.load("model.pt"))
> ```
> 
> The error message I got:
> 
> ```console
> $ python bug.py
> Traceback (most recent call last):
>   File "bug-5.py", line 31, in <module>
>     model.load_state_dict(torch.load("model.pt"))
>   File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 2001, in load_state_dict
>     raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
> RuntimeError: Error(s) in loading state_dict for Model:
>         Missing key(s) in state_dict: "layers.0.weight", "layers.0.bias", "layers.2.weight", "layers.2.bias".
>         Unexpected key(s) in state_dict: "_orig_mod.layers.0.weight", "_orig_mod.layers.0.bias", "_orig_mod.layers.2.weight", "_orig_mod.layers.2.bias".
> ```
> 
> ### Versions
> 
> ```text
> Collecting environment information...                                                                                                                                                                    [34/17196]
> PyTorch version: 1.14.0a0+44dac51
> Is debug build: False
> CUDA used to build PyTorch: 12.0
> ROCM used to build PyTorch: N/A
> 
> OS: Ubuntu 20.04.5 LTS (x86_64)
> GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
> Clang version: Could not collect
> CMake version: version 3.24.1
> Libc version: glibc-2.31
> 
> Python version: 3.8.10 (default, Nov 14 2022, 12:59:47)  [GCC 9.4.0] (64-bit runtime)
> Python platform: Linux-5.4.0-99-generic-x86_64-with-glibc2.29
> Is CUDA available: True
> CUDA runtime version: 12.0.140
> CUDA_MODULE_LOADING set to: LAZY
> GPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB
> Nvidia driver version: 515.65.01
> cuDNN version: Probably one of the following:
> /usr/lib/x86_64-linux-gnu/libcudnn.so.8.7.0
> /usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.7.0
> /usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.7.0
> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.7.0
> /usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.7.0
> /usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.7.0
> /usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.7.0
> HIP runtime version: N/A
> MIOpen runtime version: N/A
> Is XNNPACK available: True
> 
> CPU:
> Architecture:                    x86_64
> CPU op-mode(s):                  32-bit, 64-bit
> Byte Order:                      Little Endian
> Address sizes:                   43 bits physical, 48 bits virtual
> CPU(s):                          128
> On-line CPU(s) list:             0-127
> Thread(s) per core:              1
> Core(s) per socket:              64
> Socket(s):                       2
> NUMA node(s):                    8
> Vendor ID:                       AuthenticAMD
> CPU family:                      23
> Model:                           49
> Model name:                      AMD EPYC 7742 64-Core Processor
> Stepping:                        0
> Frequency boost:                 enabled
> CPU MHz:                         3087.518
> CPU max MHz:                     2250.0000
> CPU min MHz:                     1500.0000
> BogoMIPS:                        4491.73
> Virtualization:                  AMD-V
> 
> L1d cache:                       4 MiB
> L1i cache:                       4 MiB
> L2 cache:                        64 MiB
> L3 cache:                        512 MiB
> NUMA node0 CPU(s):               0-15
> NUMA node1 CPU(s):               16-31
> NUMA node2 CPU(s):               32-47
> NUMA node3 CPU(s):               48-63
> NUMA node4 CPU(s):               64-79
> NUMA node5 CPU(s):               80-95
> NUMA node6 CPU(s):               96-111
> NUMA node7 CPU(s):               112-127
> Vulnerability Itlb multihit:     Not affected
> Vulnerability L1tf:              Not affected
> Vulnerability Mds:               Not affected
> Vulnerability Meltdown:          Not affected
> Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl
> Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization
> Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled
> Vulnerability Srbds:             Not affected
> Vulnerability Tsx async abort:   Not affected
> Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonst
> op_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ib
> s skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt cl
> wb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausef
> ilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca
> 
> Versions of relevant libraries:
> [pip3] numpy==1.22.2
> [pip3] pytorch-quantization==2.1.2
> [pip3] torch==1.14.0a0+44dac51
> [pip3] torch-tensorrt==1.4.0.dev0
> [pip3] torchtext==0.13.0a0+fae8e8c
> [pip3] torchvision==0.15.0a0
> [conda] Could not collect
> ```
> 
> cc @ezyang @gchanan @zou3519 @mruberry @soumith @msaroufim @wconstab @ngimel @bdhirsh @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire @mlazos @yanboliang

---

### [Infinite loop in the subgraph.cc](https://github.com/tensorflow/tensorflow/issues/60621)

**Created:** 2023-05-18T01:53:41Z

**Tags:** `stat:awaiting tensorflower type:bug comp:lite comp:runtime`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Have you reproduced the bug with TF nightly?
> 
> No
> 
> ### Source
> 
> source
> 
> ### Tensorflow Version
> 
> tf 2.14.0
> 
> ### Custom Code
> 
> No
> 
> ### OS Platform and Distribution
> 
> Ubuntu 18.04.6
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> Python 3.8.3
> 
> ### Bazel version
> 
> bazel 5.3.0
> 
> ### GCC/Compiler version
> 
> gcc 7.5.0
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current Behaviour?
> 
> When the `GetRegistrationFromOpCode` function parses a maliciously crafted model structure, if the `builtin_code` is `tflite::BuiltinOperator_CALL_ONCE`, it will enter an infinite loop in the subsequent inference process:`tflite::Subgraph::Invoke -> tflite::Subgraph::InvokeImpl -> tflite::Subgraph::OpInvoke -> tflite::ops::builtin::call_once_kernel::Eval`.
> 
> ```c
> // op_resolver.cc
> TfLiteStatus GetRegistrationFromOpCode(
>     const OperatorCode* opcode, const OpResolver& op_resolver,
>     ErrorReporter* error_reporter, const TfLiteRegistration** registration) {
>   TfLiteStatus status = kTfLiteOk;
>   *registration = nullptr;
>   auto builtin_code = GetBuiltinCode(opcode);
>   int version = opcode->version();
> 
>   if (builtin_code > BuiltinOperator_MAX) {
>     TF_LITE_REPORT_ERROR(
>         error_reporter,
>         "Op builtin_code out of range: %d. Are you using old TFLite binary "
>         "with newer model?",
>         builtin_code);
>     status = kTfLiteError;
>   } else if (builtin_code != BuiltinOperator_CUSTOM) {
>     *registration = op_resolver.FindOp(builtin_code, version); // here
> ```
> 
> At the time of the crash, the call stack would look like this:
> 
> ```
> #67807 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67808 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67809 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67810 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67811 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67812 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67813 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67814 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67815 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67816 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67817 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67818 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67819 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67820 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67821 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67822 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67823 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67824 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67825 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67826 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67827 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67828 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67829 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67830 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67831 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67832 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67833 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67834 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67835 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67836 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67837 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67838 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67839 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67840 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67841 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67842 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67843 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67844 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67845 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67846 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67847 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67848 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67849 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67850 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67851 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67852 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> #67853 0x00005555555ee3fe in tflite::Subgraph::Invoke() ()
> #67854 0x000055555566cc30 in tflite::ops::builtin::call_once_kernel::Eval(TfLiteContext*, TfLiteNode*) ()
> #67855 0x00005555555ee106 in tflite::Subgraph::InvokeImpl() ()
> ```
> 
> [subgraph_infinite_loop.zip](https://github.com/tensorflow/tensorflow/files/11503605/subgraph_infinite_loop.zip)
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOS（coredump).
> 
> 
> ❯ ./benchmark_model --graph=./subgraph_infinite_loop.tflite
> INFO: STARTING!
> INFO: Log parameter values verbosely: [0]
> INFO: Graph: [./subgraph_infinite_loop.tflite]
> INFO: Loaded model ./subgraph_infinite_loop.tflite
> INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
> INFO: The input model file size (MB): 0.000488
> INFO: Initialized session in 0.731ms.
> INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
> [1]    6892 segmentation fault (core dumped)  ./benchmark_model --graph=./subgraph_infinite_loop.tflite
> ```
> ```
> 
> 
> ### Relevant log output
> 
> _No response_</details>

---

### [TF-lite conversion of complex abs layer not working with integer quantization with fallbacks](https://github.com/tensorflow/tensorflow/issues/55349)

**Created:** 2022-03-23T19:34:34Z

**Tags:** `stat:awaiting response type:bug stale TFLiteConverter TF 2.7`

**Content:**

> I'm trying to optimize a speech-to-text Conformer model for tflite usage. Quantization of the model is working for _default_ optimization mode, but not when a _representative dataset_ is used. In this case it fails in inference with: `type != kTfLiteFloat32 (INT8 != FLOAT32) Node number 46 (COMPLEX_ABS) failed to prepare`
> 
> Shouldn't the model use the _float32_ fallback in this case, since I'm not enforcing _int8_ operators?
> 
> ### 1. System information
> 
> - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20:04
> - TensorFlow installation (pip package or built from source): Nvidia-Docker
> - TensorFlow library (version, if pip package or github SHA, if built from source): 2.7
> 
> 
> ### 2. Failure after conversion
> 
> ```
> Transcribing the audio ...
> Label:             they were subjected to constant surveillance and periodic searches
> Traceback (most recent call last):
>   File "/Scribosermo/exporting/testing_file.py", line 161, in <module>
>     main()
>   File "/Scribosermo/exporting/testing_file.py", line 155, in main
>     test_tflite(checkpoint_tflite_quant)
>   File "/Scribosermo/exporting/testing_file.py", line 118, in test_tflite
>     prediction = predict(interpreter, audio)
>   File "/Scribosermo/exporting/testing_file.py", line 92, in predict
>     interpreter.invoke()
>   File "/usr/local/lib/python3.8/dist-packages/tflite_runtime/interpreter.py", line 923, in invoke
>     self._interpreter.Invoke()
> RuntimeError: /workspace/tensorflow/lite/kernels/complex_support.cc:43 output-> \
>   type != kTfLiteFloat32 (INT8 != FLOAT32)Node number 46 (COMPLEX_ABS) failed to prepare.
> ```
> 
> ### 3. Code snippets
> 
> ```python3
> # Conversion settings
> def export_tflite(model, save_path, optimize):
> 
>     converter = tf.lite.TFLiteConverter.from_keras_model(model)
> 
>     if optimize:
>         converter.optimizations = [tf.lite.Optimize.DEFAULT]
>         converter.representative_dataset = representative_dataset
> 
>     tflite_model = converter.convert()
> 
>     with open(save_path, "wb+") as file:
>         file.write(tflite_model)
> 
> # Spectrogram definition
> def audio_to_spect(self, audio):
>         """Calculate the spectrogram"""
> 
>         # Pytorch uses a slightly different spectrogram calculation which is matched to librosa
>         # unlike the default tensorflow implementation
>         n_fft = 512
>         nbatch = tf.shape(audio)[0]
> 
>         # Add center padding
>         signal = tf.reshape(audio, [nbatch, -1])
>         pad_amount = int(self.audio_window_samples // 2)
>         signal = tf.pad(signal, [[0, 0], [pad_amount, pad_amount]], "REFLECT")
>         signal = tf.reshape(signal, [nbatch, 1, -1])
> 
>         # Calculate short-time Fourier transforms with a differnt windowing approach
>         f = tf.signal.frame(
>             signal, self.audio_window_samples, self.audio_step_samples, pad_end=False
>         )
>         w = tf.signal.hann_window(self.audio_window_samples, periodic=False)
>         stfts = tf.signal.rfft(f * w, fft_length=[n_fft])
> 
>         # Obtain the magnitude of the STFT.
>         spectrogram = tf.abs(stfts) ** 2
>         spectrogram = tf.squeeze(spectrogram, axis=1)
> 
>         return spectrogram
> ```
> 
> <br>
> 
> Related issue: https://github.com/tensorflow/tensorflow/issues/53393#issuecomment-1009703703 (was closed without solution)

---

### [Need Help with a Softmax Warning in TensorFlow 2.16](https://github.com/tensorflow/tensorflow/issues/67758)

**Created:** 2024-05-16T18:35:33Z

**Tags:** `stat:awaiting response type:bug stale subtype:windows TF 2.16`

**Content:**

> ### Issue type
> 
> Bug
> 
> ### Have you reproduced the bug with TensorFlow Nightly?
> 
> No
> 
> ### Source
> 
> source
> 
> ### TensorFlow version
> 
> 2.16.1
> 
> ### Custom code
> 
> Yes
> 
> ### OS platform and distribution
> 
> Windows 11
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.12.3
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current behavior?
> 
> Hey everyone,
> I'm running into a bit of a headache with TensorFlow 2.16 and could really use some help. I'm getting this annoying warning about a Softmax operation over an axis with size 1. This pops up when I'm using a custom TransformerBlock layer that includes MultiHeadAttention.
> 
> What I've Tried:
> Debugging Dimensions:
> 
> Added print statements to check tensor shapes at different stages.
> Used tf.squeeze to remove dimensions of size 1 before passing the tensor to MultiHeadAttention.
> 
> What I Need:
> Is this a bug in TensorFlow 2.16? If yes, any workarounds or patches?
> Best practices for handling tensor dimensions in MultiHeadAttention to avoid this?
> Should I downgrade or wait for an update? If yes, which version should I try?
> 
> Additional Info:
> Using LSTM and GRU layers followed by the custom TransformerBlock.
> Running on Windows with Python 3.12.
> 
> Any help or pointers would be greatly appreciated! Thanks!
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> class TransformerBlock(tf.keras.layers.Layer):
>     def __init__(self, t_num_heads, t_key_dim, t_ff_dim, dropout_rate=0.1, activation_function='relu',
>                  initializer='glorot_uniform', **kwargs):
>         super(TransformerBlock, self).__init__(**kwargs)
>         self.att = MultiHeadAttention(num_heads=t_num_heads, key_dim=t_key_dim)
>         self.ffn = tf.keras.Sequential([
>             Dense(t_ff_dim, activation=activation_function, kernel_initializer=initializer),
>             Dense(t_key_dim, kernel_initializer=initializer),
>         ])
>         self.layernorm1 = LayerNormalization(epsilon=1e-6)
>         self.layernorm2 = LayerNormalization(epsilon=1e-6)
>         self.dropout1 = Dropout(dropout_rate)
>         self.dropout2 = Dropout(dropout_rate)
>         self.dense_proj = Dense(t_key_dim, kernel_initializer=initializer)
> 
>     def call(self, inputs, training=None, *args, **kwargs):
>         inputs_proj = self.dense_proj(inputs)
>         print(f"inputs_proj shape: {inputs_proj.shape}")
> 
>         if len(inputs_proj.shape) == 4 and inputs_proj.shape[2] == 1:
>             inputs_proj = tf.squeeze(inputs_proj, axis=2)
>             print(f"inputs_proj after squeeze shape: {inputs_proj.shape}")
> 
>         attn_output = self.att(inputs_proj, inputs_proj)
>         print(f"attn_output shape: {attn_output.shape}")
>         attn_output = self.dropout1(attn_output, training=training)
>         out1 = self.layernorm1(inputs_proj + attn_output)
>         ffn_output = self.ffn(out1)
>         print(f"ffn_output shape: {ffn_output.shape}")
>         ffn_output = self.dropout2(ffn_output, training=training)
>         return self.layernorm2(out1 + ffn_output)
> 
>     def compute_output_shape(self, input_shape):
>         return input_shape
> 
>     def get_config(self):
>         config = super(TransformerBlock, self).get_config()
>         config.update({
>             't_num_heads': self.att.num_heads,
>             't_key_dim': self.att.key_dim,
>             't_ff_dim': self.ffn.layers[0].units,
>             'dropout_rate': self.dropout1.rate,
>             'activation_function': self.ffn.layers[0].activation.__name__,
>             'initializer': self.ffn.layers[0].kernel_initializer.__class__.__name__
>         })
>         return config
> 
>     @classmethod
>     def from_config(cls, config):
>         return cls(**config)
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?
> ```
> 

---

### [tensorflow allreduce  train problem.](https://github.com/tensorflow/tensorflow/issues/53098)

**Created:** 2021-11-17T14:43:02Z

**Tags:** `stat:awaiting response type:bug stale 2.6.0`

**Content:**

> **System information**
> - Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
> - 18.04ubuntu
> - TensorFlow version 2.6.0
> - Python version:3.6.8
> 
> **Describe the current behavior**
>  there are sometimes error, not always, small chance happen, not easy to reproduce.......
> only using cpu train. 
> 
> 2 workers. using tfjob kubeflow operator.
> here are the work-0 logs: （note: and each worker restart themself 3times, then reaches backofflimit . job failed..)
> ```bash
> 
> 
>   |   | 2021-11-17 12:30:26 | 20211117
> -- | -- | -- | --
>   |   |   | 2021-11-17 12:30:26 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
>   |   |   | 2021-11-17 12:30:27 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
>   |   |   | 2021-11-17 12:30:27 | 2021-11-17 12:30:27,792 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
>   |   |   | 2021-11-17 12:30:27 | INFO:tensorflow:Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:27 | 2021-11-17 12:30:27,812 [INFO] Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Cluster is ready.
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,907 [INFO] Cluster is ready.
>   |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,913 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:32 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 1/3
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,914 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 1/3
>   |   |   | 2021-11-17 12:30:32 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 2/3
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,915 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 2/3
>   |   |   | 2021-11-17 12:30:32 | ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
>   |   |   | 2021-11-17 12:30:32 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
>   |   |   | 2021-11-17 12:30:32 | :{"created":"@1637123432.915347775","description":"Failed to pick subchannel","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc","file_line":3941,"referenced_errors":[{"created":"@1637123432.913304158","description":"Resolver transient failure","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc","file_line":262,"referenced_errors":[{"created":"@1637123432.913302221","description":"DNS resolution failed","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc","file_line":202,"grpc_status":14,"referenced_errors":[{"created":"@1637123432.913259195","description":"Name or service not known","errno":-2,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc","file_line":108,"os_error":"Name or service not known","syscall":"getaddrinfo","target_address":"deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222"}]}]}]}
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,915 [ERROR] Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
>   |   |   | 2021-11-17 12:30:32 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
>   |   |   | 2021-11-17 12:30:32 | :{"created":"@1637123432.915347775","description":"Failed to pick subchannel","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc","file_line":3941,"referenced_errors":[{"created":"@1637123432.913304158","description":"Resolver transient failure","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc","file_line":262,"referenced_errors":[{"created":"@1637123432.913302221","description":"DNS resolution failed","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc","file_line":202,"grpc_status":14,"referenced_errors":[{"created":"@1637123432.913259195","description":"Name or service not known","errno":-2,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc","file_line":108,"os_error":"Name or service not known","syscall":"getaddrinfo","target_address":"deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222"}]}]}]}
>   |   |   | 2021-11-17 12:30:33 | Traceback (most recent call last):
>   |   |   | 2021-11-17 12:30:33 | File "./train_atp_day.py", line 115, in <module>
>   |   |   | 2021-11-17 12:30:33 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
>   |   |   | 2021-11-17 12:30:33 | File "/workspace/script/atp_test_v2/model.py", line 92, in __init__
>   |   |   | 2021-11-17 12:30:33 | super(DeepFM, self).__init__()
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 259, in __init__
>   |   |   | 2021-11-17 12:30:33 | self._init_batch_counters()
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 267, in _init_batch_counters
>   |   |   | 2021-11-17 12:30:33 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 270, in __call__
>   |   |   | 2021-11-17 12:30:33 | return cls._variable_v2_call(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 263, in _variable_v2_call
>   |   |   | 2021-11-17 12:30:33 | shape=shape)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 68, in getter
>   |   |   | 2021-11-17 12:30:33 | return captured_getter(captured_previous, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py", line 2159, in creator_with_resource_vars
>   |   |   | 2021-11-17 12:30:33 | created = self._create_variable(next_creator, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 530, in _create_variable
>   |   |   | 2021-11-17 12:30:33 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py", line 308, in create_mirrored_variable
>   |   |   | 2021-11-17 12:30:33 | value_list = real_mirrored_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 522, in _real_mirrored_creator
>   |   |   | 2021-11-17 12:30:33 | v = next_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 244, in <lambda>
>   |   |   | 2021-11-17 12:30:33 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py", line 2675, in default_variable_creator_v2
>   |   |   | 2021-11-17 12:30:33 | shape=shape)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 272, in __call__
>   |   |   | 2021-11-17 12:30:33 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1607, in __init__
>   |   |   | 2021-11-17 12:30:33 | distribute_strategy=distribute_strategy)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1734, in _init_from_args
>   |   |   | 2021-11-17 12:30:33 | initial_value = initial_value()
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 572, in initial_value_fn
>   |   |   | 2021-11-17 12:30:33 | group_size, group_key, collective_instance_key)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py", line 268, in broadcast_send
>   |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 246, in collective_bcast_send
>   |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 301, in collective_bcast_send_eager_fallback
>   |   |   | 2021-11-17 12:30:33 | attrs=_attrs, ctx=ctx, name=name)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
>   |   |   | 2021-11-17 12:30:33 | inputs, attrs, num_outputs)
>   |   |   | 2021-11-17 12:30:33 | tensorflow.python.framework.errors_impl.UnavailableError: [_Derived_]Collective ops is aborted by: cluster check alive failed, /job:worker/replica:0/task:1 is down
>   |   |   | 2021-11-17 12:30:33 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastSend]
>   |   |   | 2021-11-17 12:30:33 | Exception ignored in: <bound method CollectiveAllReduceExtended.__del__ of <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceExtended object at 0x7fc2fa04cf28>>
>   |   |   | 2021-11-17 12:30:33 | Traceback (most recent call last):
> 2x |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 526, in __del__
>   |   |   | 2021-11-17 12:30:33 | AttributeError: 'NoneType' object has no attribute 'info'
> 2x |   |   | 2021-11-17 12:30:33 | failed
>   |   |   | 2021-11-17 12:30:34 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
>   |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
>   |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,687 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
>   |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,707 [INFO] Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:Cluster is ready.
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,081 [INFO] Cluster is ready.
>   |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,087 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:37 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 1/3
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,088 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 1/3
>   |   |   | 2021-11-17 12:30:37 | WARNING:tensorflow:/job:worker/replica:0/task:1 seems down, retrying 2/3
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,088 [WARNING] /job:worker/replica:0/task:1 seems down, retrying 2/3
>   |   |   | 2021-11-17 12:30:37 | ERROR:tensorflow:Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
>   |   |   | 2021-11-17 12:30:37 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
>   |   |   | 2021-11-17 12:30:37 | :{"created":"@1637123437.089088483","description":"Failed to pick subchannel","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc","file_line":3941,"referenced_errors":[{"created":"@1637123437.086762179","description":"Resolver transient failure","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc","file_line":262,"referenced_errors":[{"created":"@1637123437.086759773","description":"DNS resolution failed","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc","file_line":202,"grpc_status":14,"referenced_errors":[{"created":"@1637123437.086713372","description":"Name or service not known","errno":-2,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc","file_line":108,"os_error":"Name or service not known","syscall":"getaddrinfo","target_address":"deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222"}]}]}]}
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,089 [ERROR] Cluster check alive failed, /job:worker/replica:0/task:1 is down, aborting collectives: DNS resolution failed
>   |   |   | 2021-11-17 12:30:37 | Additional GRPC error information from remote target /job:worker/replica:0/task:1:
>   |   |   | 2021-11-17 12:30:37 | :{"created":"@1637123437.089088483","description":"Failed to pick subchannel","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/client_channel.cc","file_line":3941,"referenced_errors":[{"created":"@1637123437.086762179","description":"Resolver transient failure","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolving_lb_policy.cc","file_line":262,"referenced_errors":[{"created":"@1637123437.086759773","description":"DNS resolution failed","file":"external/com_github_grpc_grpc/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc","file_line":202,"grpc_status":14,"referenced_errors":[{"created":"@1637123437.086713372","description":"Name or service not known","errno":-2,"file":"external/com_github_grpc_grpc/src/core/lib/iomgr/resolve_address_posix.cc","file_line":108,"os_error":"Name or service not known","syscall":"getaddrinfo","target_address":"deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222"}]}]}]}
>   |   |   | 2021-11-17 12:30:37 | Traceback (most recent call last):
>   |   |   | 2021-11-17 12:30:37 | File "./train_atp_day.py", line 115, in <module>
>   |   |   | 2021-11-17 12:30:37 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
>   |   |   | 2021-11-17 12:30:37 | File "/workspace/script/atp_test_v2/model.py", line 92, in __init__
>   |   |   | 2021-11-17 12:30:37 | super(DeepFM, self).__init__()
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 259, in __init__
>   |   |   | 2021-11-17 12:30:37 | self._init_batch_counters()
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 267, in _init_batch_counters
>   |   |   | 2021-11-17 12:30:37 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 270, in __call__
>   |   |   | 2021-11-17 12:30:37 | return cls._variable_v2_call(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 263, in _variable_v2_call
>   |   |   | 2021-11-17 12:30:37 | shape=shape)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 68, in getter
>   |   |   | 2021-11-17 12:30:37 | return captured_getter(captured_previous, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py", line 2159, in creator_with_resource_vars
>   |   |   | 2021-11-17 12:30:37 | created = self._create_variable(next_creator, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 530, in _create_variable
>   |   |   | 2021-11-17 12:30:37 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py", line 308, in create_mirrored_variable
>   |   |   | 2021-11-17 12:30:37 | value_list = real_mirrored_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 522, in _real_mirrored_creator
>   |   |   | 2021-11-17 12:30:37 | v = next_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 244, in <lambda>
>   |   |   | 2021-11-17 12:30:37 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py", line 2675, in default_variable_creator_v2
>   |   |   | 2021-11-17 12:30:37 | shape=shape)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 272, in __call__
>   |   |   | 2021-11-17 12:30:37 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1607, in __init__
>   |   |   | 2021-11-17 12:30:37 | distribute_strategy=distribute_strategy)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1734, in _init_from_args
>   |   |   | 2021-11-17 12:30:37 | initial_value = initial_value()
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 572, in initial_value_fn
>   |   |   | 2021-11-17 12:30:37 | group_size, group_key, collective_instance_key)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py", line 268, in broadcast_send
>   |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 246, in collective_bcast_send
>   |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 301, in collective_bcast_send_eager_fallback
>   |   |   | 2021-11-17 12:30:37 | attrs=_attrs, ctx=ctx, name=name)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
>   |   |   | 2021-11-17 12:30:37 | inputs, attrs, num_outputs)
>   |   |   | 2021-11-17 12:30:37 | tensorflow.python.framework.errors_impl.UnavailableError: [_Derived_]Collective ops is aborted by: cluster check alive failed, /job:worker/replica:0/task:1 is down
>   |   |   | 2021-11-17 12:30:37 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastSend]
>   |   |   | 2021-11-17 12:30:37 | Exception ignored in: <bound method CollectiveAllReduceExtended.__del__ of <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceExtended object at 0x7f56042d7f28>>
>   |   |   | 2021-11-17 12:30:37 | Traceback (most recent call last):
> 2x |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 526, in __del__
>   |   |   | 2021-11-17 12:30:37 | AttributeError: 'NoneType' object has no attribute 'info'
> 2x |   |   | 2021-11-17 12:30:37 | failed
>   |   |   | 2021-11-17 12:30:52 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
>   |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
>   |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,287 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']
>   |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,311 [INFO] Waiting for the cluster, timeout = inf
> 
> 
> ```
> 
> and the  worker-1's log is:
> 
> ```bash
> 
> 2021-11-17 12:30:31 | 20211117
> -- | --
>   |   |   | 2021-11-17 12:30:31 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
>   |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,903 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
>   |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,914 [INFO] Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:Cluster is ready.
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,924 [INFO] Cluster is ready.
>   |   |   | 2021-11-17 12:30:32 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:32 | 2021-11-17 12:30:32,926 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:33 | Traceback (most recent call last):
>   |   |   | 2021-11-17 12:30:33 | File "./train_atp_day.py", line 115, in <module>
>   |   |   | 2021-11-17 12:30:33 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
>   |   |   | 2021-11-17 12:30:33 | File "/workspace/script/atp_test_v2/model.py", line 92, in __init__
>   |   |   | 2021-11-17 12:30:33 | super(DeepFM, self).__init__()
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 259, in __init__
>   |   |   | 2021-11-17 12:30:33 | self._init_batch_counters()
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:33 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 267, in _init_batch_counters
>   |   |   | 2021-11-17 12:30:33 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 270, in __call__
>   |   |   | 2021-11-17 12:30:33 | return cls._variable_v2_call(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 263, in _variable_v2_call
>   |   |   | 2021-11-17 12:30:33 | shape=shape)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 68, in getter
>   |   |   | 2021-11-17 12:30:33 | return captured_getter(captured_previous, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py", line 2159, in creator_with_resource_vars
>   |   |   | 2021-11-17 12:30:33 | created = self._create_variable(next_creator, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 530, in _create_variable
>   |   |   | 2021-11-17 12:30:33 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py", line 308, in create_mirrored_variable
>   |   |   | 2021-11-17 12:30:33 | value_list = real_mirrored_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 522, in _real_mirrored_creator
>   |   |   | 2021-11-17 12:30:33 | v = next_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 244, in <lambda>
>   |   |   | 2021-11-17 12:30:33 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py", line 2675, in default_variable_creator_v2
>   |   |   | 2021-11-17 12:30:33 | shape=shape)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 272, in __call__
>   |   |   | 2021-11-17 12:30:33 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1607, in __init__
>   |   |   | 2021-11-17 12:30:33 | distribute_strategy=distribute_strategy)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1734, in _init_from_args
>   |   |   | 2021-11-17 12:30:33 | initial_value = initial_value()
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 579, in initial_value_fn
>   |   |   | 2021-11-17 12:30:33 | collective_instance_key)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py", line 345, in broadcast_recv
>   |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 59, in collective_bcast_recv
>   |   |   | 2021-11-17 12:30:33 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 115, in collective_bcast_recv_eager_fallback
>   |   |   | 2021-11-17 12:30:33 | attrs=_attrs, ctx=ctx, name=name)
>   |   |   | 2021-11-17 12:30:33 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
>   |   |   | 2021-11-17 12:30:33 | inputs, attrs, num_outputs)
>   |   |   | 2021-11-17 12:30:33 | tensorflow.python.framework.errors_impl.FailedPreconditionError: [_Derived_]Collective ops is aborted by: group 1 failed to resolve. This normally means the server has restarted
>   |   |   | 2021-11-17 12:30:33 | Additional GRPC error information from remote target /job:worker/replica:0/task:0:
>   |   |   | 2021-11-17 12:30:33 | :{"created":"@1637123433.061968995","description":"Error received from peer ipv4:100.91.109.177:2222","file":"external/com_github_grpc_grpc/src/core/lib/surface/call.cc","file_line":1056,"grpc_message":"group 1 failed to resolve. This normally means the server has restarted","grpc_status":9}
>   |   |   | 2021-11-17 12:30:33 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastRecv]
> 2x |   |   | 2021-11-17 12:30:33 | failed
>   |   |   | 2021-11-17 12:30:34 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
>   |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
>   |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,075 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
>   |   |   | 2021-11-17 12:30:36 | INFO:tensorflow:Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:36 | 2021-11-17 12:30:36,086 [INFO] Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:Cluster is ready.
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,098 [INFO] Cluster is ready.
>   |   |   | 2021-11-17 12:30:37 | INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:37 | 2021-11-17 12:30:37,098 [INFO] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['deepfm-open-v1-1637123400-worker-0.ai-ctr.svc:2222', 'deepfm-open-v1-1637123400-worker-1.ai-ctr.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 2, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO
>   |   |   | 2021-11-17 12:30:37 | Traceback (most recent call last):
>   |   |   | 2021-11-17 12:30:37 | File "./train_atp_day.py", line 115, in <module>
>   |   |   | 2021-11-17 12:30:37 | model = DeepFM(onehot_feature_len, input_length=input_length, embed_dim=embed_dim, hidden_units=hidden_units, dnn_dropout=dnn_dropout)
>   |   |   | 2021-11-17 12:30:37 | File "/workspace/script/atp_test_v2/model.py", line 92, in __init__
>   |   |   | 2021-11-17 12:30:37 | super(DeepFM, self).__init__()
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 259, in __init__
>   |   |   | 2021-11-17 12:30:37 | self._init_batch_counters()
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py", line 530, in _method_wrapper
>   |   |   | 2021-11-17 12:30:37 | result = method(self, *args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py", line 267, in _init_batch_counters
>   |   |   | 2021-11-17 12:30:37 | self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 270, in __call__
>   |   |   | 2021-11-17 12:30:37 | return cls._variable_v2_call(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 263, in _variable_v2_call
>   |   |   | 2021-11-17 12:30:37 | shape=shape)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 68, in getter
>   |   |   | 2021-11-17 12:30:37 | return captured_getter(captured_previous, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py", line 2159, in creator_with_resource_vars
>   |   |   | 2021-11-17 12:30:37 | created = self._create_variable(next_creator, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 530, in _create_variable
>   |   |   | 2021-11-17 12:30:37 | distribute_utils.VARIABLE_POLICY_MAPPING, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py", line 308, in create_mirrored_variable
>   |   |   | 2021-11-17 12:30:37 | value_list = real_mirrored_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py", line 522, in _real_mirrored_creator
>   |   |   | 2021-11-17 12:30:37 | v = next_creator(**kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 244, in <lambda>
>   |   |   | 2021-11-17 12:30:37 | previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py", line 2675, in default_variable_creator_v2
>   |   |   | 2021-11-17 12:30:37 | shape=shape)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py", line 272, in __call__
>   |   |   | 2021-11-17 12:30:37 | return super(VariableMetaclass, cls).__call__(*args, **kwargs)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1607, in __init__
>   |   |   | 2021-11-17 12:30:37 | distribute_strategy=distribute_strategy)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py", line 1734, in _init_from_args
>   |   |   | 2021-11-17 12:30:37 | initial_value = initial_value()
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 579, in initial_value_fn
>   |   |   | 2021-11-17 12:30:37 | collective_instance_key)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/collective_ops.py", line 345, in broadcast_recv
>   |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 59, in collective_bcast_recv
>   |   |   | 2021-11-17 12:30:37 | timeout_seconds=timeout_seconds, name=name, ctx=_ctx)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_collective_ops.py", line 115, in collective_bcast_recv_eager_fallback
>   |   |   | 2021-11-17 12:30:37 | attrs=_attrs, ctx=ctx, name=name)
>   |   |   | 2021-11-17 12:30:37 | File "/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py", line 60, in quick_execute
>   |   |   | 2021-11-17 12:30:37 | inputs, attrs, num_outputs)
>   |   |   | 2021-11-17 12:30:37 | tensorflow.python.framework.errors_impl.FailedPreconditionError: [_Derived_]Collective ops is aborted by: group 1 failed to resolve. This normally means the server has restarted
>   |   |   | 2021-11-17 12:30:37 | Additional GRPC error information from remote target /job:worker/replica:0/task:0:
>   |   |   | 2021-11-17 12:30:37 | :{"created":"@1637123437.205532722","description":"Error received from peer ipv4:100.91.109.177:2222","file":"external/com_github_grpc_grpc/src/core/lib/surface/call.cc","file_line":1056,"grpc_message":"group 1 failed to resolve. This normally means the server has restarted","grpc_status":9}
>   |   |   | 2021-11-17 12:30:37 | The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastRecv]
> 2x |   |   | 2021-11-17 12:30:37 | failed
>   |   |   | 2021-11-17 12:30:52 | python ./train_atp_day.py --data_path /workspace/data/data --featmap_path /workspace/data/featMap/part-00000 --checkpoint_path /workspace/model/deepfm_v1 --start_date 20211117 --embed_dim 8 --input_length 42
>   |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
>   |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,563 [INFO] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
>   |   |   | 2021-11-17 12:30:54 | INFO:tensorflow:Waiting for the cluster, timeout = inf
>   |   |   | 2021-11-17 12:30:54 | 2021-11-17 12:30:54,574 [INFO] Waiting for the cluster, timeout = inf
> 
> ```
> 
> can anyone help to analysis this behavior. 
> 
> i have checked  dns log, there not any error log , and all the logs is ok..
> 
> i have read some code, i doubt that  "cluster is ready"  check condition is not very convincing.... ready ,but have problem....
> 
> **Describe the expected behavior**
>   not happen this type error ...

---

### [Node: 'model/conv1d/Conv1D' DNN library is not found.](https://github.com/tensorflow/tensorflow/issues/55309)

**Created:** 2022-03-21T13:56:05Z

**Tags:** `stat:awaiting response type:bug comp:gpu TF 2.8`

**Content:**

> **System information**
> - OS: Linux Ubuntu 20.04:
> - TensorFlow installed from pip
> - TensorFlow version: v2.8.0-rc1-32-g3f878cff5b6 2.8.0
> - Python version: Python 3.8.10
> - CUDA/cuDNN version: NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6   
> - GPU model : [GeForce GTX 1650] 
> 
> Hello, everyone.
> I'm trying to run a  convolutional neural network on tensorflow but I'm receiving the current error:
> 
> > 2022-03-21 10:40:20.665473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:20.687599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:20.687804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.544819: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> > 2022-03-21 10:40:22.545204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.545428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.545579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.815601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.815824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.815975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> > 2022-03-21 10:40:22.816100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2607 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5
> > Epoch 1/999999
> > 2022-03-21 10:40:23.481069: E tensorflow/stream_executor/cuda/cuda_dnn.cc:361] Loaded runtime CuDNN library: 8.0.4 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
> > 2022-03-21 10:40:23.481615: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_ops.cc:1120 : UNIMPLEMENTED: DNN library is not found.
> > Traceback (most recent call last):
> >   File "CNN_regression.py", line 367, in <module>
> >     results = pd.concat([results,main(mode)], ignore_index=True)
> >   File "CNN_regression.py", line 124, in main
> >     history = model.fit(X_train, Y_train,
> >   File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 67, in error_handler
> >     raise e.with_traceback(filtered_tb) from None
> >   File "/home/italocaliari/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 54, in quick_execute
> >     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
> > tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:
> > 
> > Detected at node 'model/conv1d/Conv1D' defined at (most recent call last):
> >     File "CNN_regression.py", line 367, in <module>
> >       results = pd.concat([results,main(mode)], ignore_index=True)
> >     File "CNN_regression.py", line 124, in main
> >       history = model.fit(X_train, Y_train,
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 64, in error_handler
> >       return fn(*args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py", line 1384, in fit
> >       tmp_logs = self.train_function(iterator)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py", line 1021, in train_function
> >       return step_function(self, iterator)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py", line 1010, in step_function
> >       outputs = model.distribute_strategy.run(run_step, args=(data,))
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py", line 1000, in run_step
> >       outputs = model.train_step(data)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/training.py", line 859, in train_step
> >       y_pred = self(x, training=True)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 64, in error_handler
> >       return fn(*args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/base_layer.py", line 1096, in __call__
> >       outputs = call_fn(inputs, *args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 92, in error_handler
> >       return fn(*args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/functional.py", line 451, in call
> >       return self._run_internal_graph(
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/functional.py", line 589, in _run_internal_graph
> >       outputs = node.layer(*args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 64, in error_handler
> >       return fn(*args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/engine/base_layer.py", line 1096, in __call__
> >       outputs = call_fn(inputs, *args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 92, in error_handler
> >       return fn(*args, **kwargs)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/layers/convolutional.py", line 248, in call
> >       outputs = self.convolution_op(inputs, self.kernel)
> >     File "/home/italocaliari/.local/lib/python3.8/site-packages/keras/layers/convolutional.py", line 233, in convolution_op
> >       return tf.nn.convolution(
> > Node: 'model/conv1d/Conv1D'
> > DNN library is not found.
> > 	 [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_660]
> 
> 
> 
> I'm running the exact same code with another computer with a Nvidia MX110 and is working just fine. I think this might be a configuration/installation issue related to:
> 
> >  "Loaded runtime CuDNN library: 8.0.4 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration." 
> 
> I could not find any solution to this problem.
> 
> Thank you all in advance.
> 
> 

---

### [TFLITE does not compile with CMake in Visual Studio 2019](https://github.com/tensorflow/tensorflow/issues/62228)

**Created:** 2023-10-25T12:16:10Z

**Tags:** `stat:awaiting tensorflower type:bug type:build/install comp:lite subtype:windows`

**Content:**

> ### Issue type
> 
> Bug
> 
> ### Have you reproduced the bug with TensorFlow Nightly?
> 
> Yes
> 
> ### Source
> 
> source
> 
> ### TensorFlow version
> 
> tf-master
> 
> ### Custom code
> 
> Yes
> 
> ### OS platform and distribution
> 
> Windows 7 SP1 x64
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.11
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> NVIDIA GeForce 1660 SUPER 6 GB
> 
> ### Current behavior?
> 
> Cannot find #include <sys/mman.h> in "tensorflow/lite/kernels/internal/optimized/fully_connected_4bit.h"
> Expected successful build.
> 
> P.S. There's TFLITE_MMAP_DISABLED condition in this file, but I didn't find it in CMakeLists.txt or anywhere else (except Bazel config files that are not used while building with CMake).
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> 1. Open latest VS 2019 on 64-bit Windows 7 (maybe newer versions too).
> 2. Download tensorflow-master as zip, unpack.
> 3. Open tensoflow/lite as CMake project.
> 4. Right click on root CMakeLists.txt and choose "Build".
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> Cannot find #include <sys/mman.h> in "tensorflow/lite/kernels/internal/optimized/fully_connected_4bit.h"
> ```
> 

---

### [fit() with generator and multiprocessing leaks sockets/files](https://github.com/tensorflow/tensorflow/issues/55623)

**Created:** 2022-04-14T15:55:37Z

**Tags:** `stat:awaiting response type:bug comp:keras TF 2.5`

**Content:**

> **System information**
> - Have I written custom code - yes
> - OS Platform and Distribution - Linux CentOS7
> - TensorFlow installed from binary
> - TensorFlow version 2.5.0
> - Python version: 3.8
> - 
> **Describe the current behavior**
> 
> If I run model.fit() with a generator in a loop, for a long time, I eventually get `OSError: [Errno 24] Too many open files`.
> 
> **Describe the expected behavior**
> 
> Run stably forever.
> 
> The actual code is on an offline machine, but this is the basic idea:
> 
> ```
> while True:
>    model.fit(mySeqGen(xx), ..., use_multiprocessing=True, workers=8)
>    <stuff>
>    model.save_weights(fname)
>    gc.collect()
> ```
> This will run for a long time, but eventually it will generate the error every iteration.  The stack trace is all about "python3.8/multiprocessing/..." so I'm sure it's not related to actual files, but rather sockets or whatever the multiprocessor generates.  I thought the `gc.collect()` would solve the problem but does not appear to do so, or it can't keep up.  Each loop iteration takes about a minute as I have it configured now.  Note also that the seqGen is a wrapper around a simulator and does not go to the file system.

---

### [Runtime error](https://github.com/tensorflow/tensorflow/issues/59384)

**Created:** 2023-01-21T23:50:14Z

**Tags:** `stat:awaiting response type:bug stale comp:ops TF 2.11`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Have you reproduced the bug with TF nightly?
> 
> No
> 
> ### Source
> 
> binary
> 
> ### Tensorflow Version
> 
> 2.11.0
> 
> ### Custom Code
> 
> Yes
> 
> ### OS Platform and Distribution
> 
> Ubuntu 22.04
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.9
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/Compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current Behaviour?
> 
> ```shell
> Probably due to large tensor or negative argument.
> ```
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> import tensorflow as tf
> import os
> import numpy as np
> from tensorflow.python.ops import array_ops
> try:
>   arg_0_tensor = tf.saturate_cast(tf.constant(-152265577042003, shape=[2, 3, 4], dtype=tf.int64,),dtype=tf.uint64)
>   arg_0 = tf.identity(arg_0_tensor)
>   arg_1 = -67
>   arg_2 = 0
>   range_given = False
>   round_mode = "HALF_UP"
>   axis = None
>   out = array_ops.quantize_and_dequantize_v2(arg_0,arg_1,arg_2,range_given=range_given,round_mode=round_mode,axis=axis,)
> except Exception as e:
>   print("Error:"+str(e))
> ```
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> 2023-01-21 18:47:28.285233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2023-01-21 18:47:28.904062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
> 2023-01-21 18:47:28.904236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/lib/:/home/nimashiri/anaconda3/envs/cuda11.2/lib/
> 2023-01-21 18:47:28.904243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
> 2023-01-21 18:47:29.447639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.452132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.452261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.452556: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2023-01-21 18:47:29.452998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.453105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.453198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.850356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.850491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.850591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
> 2023-01-21 18:47:29.850679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4252 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
> Error:can't convert negative int to unsigned
> 
> ```
> ```
> </details>

---

### [Model runs without error in Tensorflow, but crashes with a segmentation fault in TFLite](https://github.com/tensorflow/tensorflow/issues/61067)

**Created:** 2023-06-23T08:51:18Z

**Tags:** `stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.12`

**Content:**

> ### 1. System information
> 
> - OS Platform and Distribution: Ubuntu 20.04.4 LTS
> - TensorFlow installation: pip
> - TensorFlow library: 2.12.0
> - TFLite runtime: 2.12.0
> 
> ### 2. Code
> 
> The model is exported from PyTorch using ONNX. I have not included the PyTorch code below for brevity's sake (and because it is used for an active Kaggle competition); you can download the saved Keras model [here](https://cloud.ilabt.imec.be/index.php/s/Dgpi9SQTcyc23wm). The TFLite conversion code is given below, but you can also download the TFLite model [here](https://cloud.ilabt.imec.be/index.php/s/Dgpi9SQTcyc23wm) (same link).
> 
> Below is the code to create and save the Keras model from two PyTorch models `feat_gen` and `model`, converted using ONNX:
> 
> ```python
> class TFInferModel(tf.Module):
>     def __init__(self):
>         super(TFInferModel, self).__init__()
>         self.feat_gen = tf.saved_model.load("feat_gen.pb")
>         self.model = tf.saved_model.load("model.pb")
> 
>         self.feat_gen.trainable = False
>         self.model.trainable = False
> 
>     @tf.function(input_signature=[tf.TensorSpec(shape=[None, 126], dtype=tf.float32, name="inputs")])
>     def call(self, inputs):
>         output_tensors = {}
> 
>         # Add batch dimension.
>         inputs = inputs[None]
> 
>         # Process using ported PyTorch model.
>         features = self.feat_gen(inputs=inputs)["outputs"]
>         outputs = self.model(inputs=features)["outputs"]
> 
>         # Remove batch dimension.
>         outputs = outputs[0]
> 
>         output_tensors["outputs"] = outputs
>         return output_tensors
> 
> tf_model = TFInferModel()
> tf.saved_model.save(tf_model, "tf_model", signatures={"serving_default": tf_model.call})
> ```
> 
> The model can be loaded in Keras and run:
> 
> ```python
> model = tf.saved_model.load("tf_model")
> inputs = tf.zeros((100, 126), dtype=tf.float32)
> output = model.call(inputs=inputs)
> ```
> 
> It can also be converted to TFLite:
> 
> ```python
> converter = tf.lite.TFLiteConverter.from_saved_model("tf_model")
> 
> tf_lite_model = converter.convert()
> output_path = "model.tflite"
> with open(output_path, "wb") as f:
>     f.write(tf_lite_model)
> ```
> 
> And finally the code for TFLite inference:
> 
> ```python
> interpreter = tflite.Interpreter(model_path="model.tflite")
> prediction_fn = interpreter.get_signature_runner("serving_default")
> inputs = np.zeros((100, 126), dtype=np.float32)
> output = prediction_fn(inputs=inputs)
> ```
> 
> ### 3. Failure after conversion
> 
> The Keras inference code runs without issue. The TFLite inference code crashes immediately with a segmentation fault (no further info is given).

---

### [Inconsistent results with or without tf.function](https://github.com/tensorflow/tensorflow/issues/55814)

**Created:** 2022-04-30T15:03:19Z

**Tags:** `stat:awaiting tensorflower type:bug comp:tf.function TF 2.8`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Source
> 
> source
> 
> ### Tensorflow Version
> 
> tf 2.8.0
> 
> ### Custom Code
> 
> No
> 
> ### OS Platform and Distribution
> 
> Google Colab
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> _No response_
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/Compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current Behaviour?
> 
> ```shell
> The function with tf.function decorator should produce the same results the one without the decorator. In addition, adding one tf.print statement in the function with tf.function decorator will change the behavior.
> ```
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> I created a google colab notebook. Please check this link.
> 
> https://colab.research.google.com/drive/1FOvjw0pPZHQw2bZtuFE8fnTAZk8_n0X8?usp=sharing
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> TensorFlow version: 2.8.0
> loss_object tf.Tensor(26.064999, shape=(), dtype=float32)
> loss_object_1 tf.Tensor(nan, shape=(), dtype=float32)
> [[1.5240801e-14 4.78756791e-12 7.77465369e-15 ... 1 4.88135581e-29 0]]
> loss_object_2 tf.Tensor(nan, shape=(), dtype=float32)
> ```
> </details>

---

### [When I set unroll of LSTM to True, conversion problems occurs.](https://github.com/tensorflow/tensorflow/issues/57015)

**Created:** 2022-08-05T03:59:51Z

**Tags:** `type:bug comp:lite TF 2.9`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Source
> 
> binary
> 
> ### Tensorflow Version
> 
> tf 2.9
> 
> ### Custom Code
> 
> Yes
> 
> ### OS Platform and Distribution
> 
> windows10
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.9
> 
> ### Current Behaviour?
> 
> ```shell
> My model uses the LSTM layer.During training,parameters of LSTM, unroll=False and stateful=False.
> After training, I convert the model to tflite.
> If I still set unroll=False and stateful=False during conversion, then there is a UnidirectionalSequenceLSTM layer in the tflite file. 
> Unfortunately, I need to use tensorflow lite for micro. 
> UnidirectionalSequenceLSTM layer is not supported in tensorflow lite for micro now.
> So I need to set unroll=True and stateful=False during conversion.
> This setting can avoid using UnidirectionalSequenceLSTM.
> Test result displays that tensorflow lite for micro support the model with parameters unroll=True and stateful=False. 
> However,the result of inference with "unroll=True and stateful=False" is different from that of inference with "unroll=False and stateful=False".
> How to make the two results consistent.
> ```
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> During training:
>   P = Dense(units=self.nbin, activation='tanh')(P)
>   P = Dense(units=self.nbin, activation='tanh')(P)
>   P = LSTM(units=self.nbin, activation='tanh', return_sequences=True, stateful=False)(P)
>   P = Dense(units=self.nbin, activation='softplus')(P)
> 
>   Be = Lambda(self.forward)(e)
>   E = Dense(units=self.nbin, activation='linear')(Be)
>   E = LSTM(units=self.nbin, activation='tanh', return_sequences=True, stateful=False)(E)
>   Z = E*P
>   Z = Dense(units=self.wlen_z, activation='linear')(Z)
> 
> During converting:
>   P = Dense(units=self.nbin, activation='tanh')(P)
>   P = Dense(units=self.nbin, activation='tanh')(P)
>   P = LSTM(units=self.nbin, activation='tanh', return_sequences=True, unroll=True,stateful=False)(P)
>   P = Dense(units=self.nbin, activation='softplus')(P)
> 
>   Be = Lambda(self.forward)(e)
>   E = Dense(units=self.nbin, activation='linear')(Be)
>   E = LSTM(units=self.nbin, activation='tanh', return_sequences=True, unroll=True,stateful=False)(E)
>   Z = E*P
>   Z = Dense(units=self.wlen_z, activation='linear')(Z)
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> The result of inference with "unroll=True and stateful=False" is different from that of inference with "unroll=False and stateful=False".
> ```
> </details>

---

### [Forward-mode Autodiff gives wrong `nan` when argument `segment_ids[0]` is not zero  for API `tf.math.segment_max`](https://github.com/tensorflow/tensorflow/issues/57002)

**Created:** 2022-08-03T15:21:51Z

**Tags:** `stat:awaiting response type:bug stale comp:ops TF 2.9`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Source
> 
> binary
> 
> ### Tensorflow Version
> 
> tf 2.9
> 
> ### Custom Code
> 
> Yes
> 
> ### OS Platform and Distribution
> 
> Linux Ubuntu 20.04
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.9
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/Compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current Behaviour?
> 
> If the first element of `segment_ids` is not zero, the forward mode autodiff will give nan as gradient, which is incorrect. It should give the same result as reverse-mode did.
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> import tensorflow as tf
> 
> data = tf.constant(
> [[0.47682607, 0.2620497, 0.22771002],
>  [0.59265005, 0.62705662, 0.81160802]], shape=(2, 3), dtype=tf.float64)
> 
> 
> with tf.GradientTape(persistent=True) as g:
>  g.watch(data)
>  res_backward = tf.math.segment_max(data,[1,1])
> grad_backward = g.gradient(res_backward,data)
> print("reverse-mode:\n",grad_backward)
> 
> grad_fwd_arr = []
> 
> for i in range(tf.size(data)):
>     tangents = tf.reshape(tf.one_hot(i,tf.size(data),dtype=tf.float64),shape=data.shape)
>     with tf.autodiff.ForwardAccumulator(data, tangents) as acc:
>         res_forward = tf.math.segment_max(data,[1,1])
>         jvp = acc.jvp(res_forward)
>         grad_fwd_arr.append(tf.reduce_sum(jvp))
> 
> grad_fwd = tf.reshape(tf.convert_to_tensor(grad_fwd_arr),shape=data.shape)
> print("forward-mode:\n",grad_fwd)
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> reverse-mode:
>  tf.Tensor(
> [[0. 0. 0.]
>  [1. 1. 1.]], shape=(2, 3), dtype=float64)
> forward-mode:
>  tf.Tensor(
> [[nan nan nan]
>  [nan nan nan]], shape=(2, 3), dtype=float64)
> ```
> </details>

---

### [RoBERTa example from tfhub produces error "During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string"](https://github.com/tensorflow/tensorflow/issues/57820)

**Created:** 2022-09-23T10:23:05Z

**Tags:** `type:bug comp:keras TF 2.8`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Source
> 
> source
> 
> ### Tensorflow Version
> 
> 2.8.2
> 
> ### Custom Code
> 
> Yes
> 
> ### OS Platform and Distribution
> 
> _No response_
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.7.14 
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/Compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current Behaviour?
> 
> ```shell
> A bug happened!
> I would like to use the roberta-base model from tfhub. I am trying to run the example below, although I get an error when I try to feed sentences to model as input. I get the following error Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string. I am using python 3.7, tensorflow 2.8
> ```
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> # define a text embedding model
> text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
> preprocessor = hub.KerasLayer("https://tfhub.dev/jeongukjae/roberta_en_cased_preprocess/1")
> encoder_inputs = preprocessor(text_input)
> 
> encoder = hub.KerasLayer("https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1", trainable=True)
> encoder_outputs = encoder(encoder_inputs)
> pooled_output = encoder_outputs["pooled_output"]      # [batch_size, 768].
> sequence_output = encoder_outputs["sequence_output"]  # [batch_size, seq_length, 768].
> 
> model = tf.keras.Model(text_input, pooled_output)
> 
> # You can embed your sentences as follows
> sentences = tf.constant(["(your text here)"])
> print(model(sentences))
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> InvalidArgumentError: Graph execution error:
> 
> 2 root error(s) found.
>   (0) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
> 	 [[{{node map/TensorArrayUnstack/TensorListFromTensor}}]]
> 	 [[model/preprocessing/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bpe_sentencepiece_tokenizer/StatefulPartitionedCall/RaggedFromRowSplits_1/RowPartitionFromRowSplits/assert_non_negative/assert_less_equal/Assert/AssertGuard/else/_18720/RaggedFromRowSplits_1/RowPartitionFromRowSplits/assert_non_negative/assert_less_equal/Assert/AssertGuard/Assert/data_0/_135]]
>   (1) INVALID_ARGUMENT:  During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
> 	 [[{{node map/TensorArrayUnstack/TensorListFromTensor}}]]
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_train_function_534484]
> ```
> </details>

---

### [TFLite with Hexagon delegate produces wrong results for a particular model](https://github.com/tensorflow/tensorflow/issues/54481)

**Created:** 2022-02-22T16:53:55Z

**Tags:** `stat:awaiting response stat:awaiting tensorflower type:bug stale TFLiteHexagonDelegate TF 2.7`

**Content:**

> **System information**
> - Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
> - OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (host), Android 11 (target device)
> - Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: device with Snapdragon 660
> - TensorFlow installed from (source or binary): source
> - TensorFlow version (use command below): 2.7.0
> - Python version: 3.8
> - Bazel version (if compiling from source): 4.2.1
> - GCC/Compiler version (if compiling from source): 8.4.0
> - CUDA/cuDNN version: n/a
> - GPU model and memory: n/a
> 
> **Describe the current behavior**
> For a custom model I am trying to use, when I enable the Hexagon delegate, the output contains incorrect values. The Hexagon delegate seems to work fine with other models I have tried.
> 
> **Describe the expected behavior**
> I expect the Hexagon delegate to produce the same results as CPU.
> 
> **Standalone code to reproduce the issue**
> The difference between CPU and Hexagon delegate can be observed with your [Inference Diff tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/inference_diff#inference-diff-tool):
> ```
> INFO: TfLiteHexagonDelegate delegate: 270 nodes delegated out of 270 nodes with 1 partitions.
> INFO: Replacing 270 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
> native : lite/tools/evaluation/stages/inference_profiler_stage.cc:77 Test interpreter has been initialized.
> native : lite/tools/evaluation/stages/tflite_inference_stage.cc:128 
> native : lite/tools/evaluation/stages/inference_profiler_stage.cc:91 Reference interpreter (1 thread on CPU) has been initialized.
> Num evaluation runs: 50
> Reference run latency: avg=863334(us), std_dev=25697(us)
> Test run latency: avg=93164.5(us), std_dev=675(us)
> OutputDiff[0]: avg_error=86.0362, std_dev=0.0619768
> OutputDiff[1]: avg_error=48.6195, std_dev=0.168915
> OutputDiff[2]: avg_error=52.9178, std_dev=0.252582
> ```
> 
> **Other info / logs** Include any logs or source code that would be helpful to
> diagnose the problem. If including tracebacks, please include the full
> traceback. Large logs and files should be attached.
> Model used: [test_model.zip](https://github.com/tensorflow/tensorflow/files/8118439/test_model.zip)
> 

---

### [Different Behavior of tf.raw_ops.RGBToHSV with jit_compile=True](https://github.com/tensorflow/tensorflow/issues/62295)

**Created:** 2023-10-31T07:42:27Z

**Tags:** `type:bug comp:ops TF2.14`

**Content:**

> ### Issue type
> 
> Bug
> 
> ### Have you reproduced the bug with TensorFlow Nightly?
> 
> No
> 
> ### Source
> 
> source
> 
> ### TensorFlow version
> 
> 2.14.0
> 
> ### Custom code
> 
> Yes
> 
> ### OS platform and distribution
> 
> _No response_
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> _No response_
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> 11.8
> 
> ### GPU model and memory
> 
> GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070
> 
> ### Current behavior?
> 
> When the **tf.raw_ops.RGBToHSV** operation is invoked within a tf.function with JIT compilation enabled **(jit_compile=True),** it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a **CPU** device.
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> import tensorflow as tf
> import traceback
> 
> class Network(tf.Module):
>     def __init__(self):
>         super().__init__()
> 
>     @tf.function(jit_compile=True)
>     def __call__(self, x):
>       
>       x = tf.raw_ops.RGBToHSV(images=x, )        
>       return x
> 
> m = Network()
> inp = {
>     "x": tf.random.normal([9, 8, 6, 3], dtype=tf.bfloat16),
> }
> 
> with tf.device('/CPU:0'):
>     tf.config.run_functions_eagerly(True)
>     no_op_res = m(**inp)
>     tf.config.run_functions_eagerly(False)
>     with tf.device('/CPU:0'):
>         op_res = m(**inp)
> 
>     tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> File "/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py", line 26, in <module>
>     tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
>   File "/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py", line 153, in error_handler
>     raise e.with_traceback(filtered_tb) from None
>   File "/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py", line 102, in Assert
>     raise errors.InvalidArgumentError(
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
> b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
> b'x (shape=(9, 8, 6, 3) dtype=float64) = '
> 0.6328125, 1.59375, 0.765625, ...
> b'y (shape=(9, 8, 6, 3) dtype=float64) = '
> 0.6328125, 1.59375, 0.765625, ...
> ```
> 

---

### [TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values](https://github.com/tensorflow/tensorflow/issues/77958)

**Created:** 2024-10-15T11:16:10Z

**Tags:** `type:bug 2.17`

**Content:**

> ### Issue type
> 
> Bug
> 
> ### Have you reproduced the bug with TensorFlow Nightly?
> 
> No
> 
> ### Source
> 
> binary
> 
> ### TensorFlow version
> 
> 2.17.0
> 
> ### Custom code
> 
> Yes
> 
> ### OS platform and distribution
> 
> Linux CentOS 7.9
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.12.4
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current behavior?
> 
> We used tensorflow for 3 class classifications. The code worked well on existed environment: Python 3.8.3 + tensorflow 2.13.1.
> But when we tried on new enviroment: Python 3.12.4 + tensorflow 2.17.0, several errors blocked.
> 
> ### Standalone code to reproduce the issue
> 
> 
> Here are the simple code to reproduce the issue.
> ```
> import tensorflow as tf
> from tensorflow.keras.layers import Dense
> import numpy as np
> 
> 
> def build_model():
>     metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]
>     loss = tf.keras.losses.sparse_categorical_crossentropy
>     num_class = 3
>     inputs = tf.keras.layers.Input(shape=(40, 36))
>     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
>     output = Dense(num_class, activation='softmax')(lstm_out)
>     model = tf.keras.Model(inputs, output)
>     model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
>     model.summary()
>     return model
> 
> def data_gen():
>     while True:
>         yield np.random.rand(128, 40, 36), np.random.randint(0, 3, (128, 1))
> 
> model = build_model()
> cw = {0: 0.3, 1:2.5, 2:3.2}
> model.fit(data_gen(), epochs=2, steps_per_epoch=10, class_weight=cw)
> ```
> 
> The first error we met:
> ```
> Traceback (most recent call last):
>   File "/data/release/infinity_stock2/tf_model.py", line 25, in <module>
>     model.fit(data_gen(), epochs=2, steps_per_epoch=10, class_weight=cw)
>   File "/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
>     raise e.with_traceback(filtered_tb) from None
>   File "/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/trainers/data_adapters/__init__.py", line 108, in get_data_adapter
>     raise ValueError(
> ValueError: Argument `class_weight` is not supported for Python generator inputs. Received: class_weight={0: 0.3, 1: 2.5, 2: 3.2}
> ```
> 
> To solve the issue, we used tf dataset to wrap the generator. 
> ```
> import tensorflow as tf
> from tensorflow.keras.layers import Dense
> import numpy as np
> 
> 
> def build_model():
>     metrics = ['accuracy', tf.keras.metrics.AUC(multi_label=True)]
>     loss = tf.keras.losses.sparse_categorical_crossentropy
>     num_class = 3
>     inputs = tf.keras.layers.Input(shape=(40, 36))
>     lstm_out = tf.keras.layers.LSTM(32, return_sequences=False)(inputs)
>     output = Dense(num_class, activation='softmax')(lstm_out)
>     model = tf.keras.Model(inputs, output)
>     model.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(), metrics=metrics)
>     model.summary()
>     return model
> 
> def data_gen():
>     while True:
>         yield np.random.rand(128, 40, 36), np.random.randint(0, 3, (128, 1))
> 
> model = build_model()
> cw = {0: 0.3, 1:2.5, 2:3.2}
> data = tf.data.Dataset.from_generator(data_gen, output_types=(tf.float32, tf.float32),  output_shapes=((None, 40, 36), (None, None)))
> model.fit(data, epochs=2, steps_per_epoch=10, class_weight=cw)
> ```
> And another error happened:
> ```
> Traceback (most recent call last):
>   File "/data/release/infinity_stock2/tf_model.py", line 26, in <module>
>     model.fit(data, epochs=2, steps_per_epoch=10, class_weight=cw)
>   File "/root/.virtualenvs/new_stock/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
>     raise e.with_traceback(filtered_tb) from None
>   File "/root/.virtualenvs/new_stock/lib/python3.12/site-packages/tensorflow/python/ops/cond_v2.py", line 876, in error
>     raise TypeError(
> TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values.
> 
> true_fn output: Tensor("cond/Identity:0", shape=(None,), dtype=int64)
> false_fn output: Tensor("cond/Identity:0", shape=(None,), dtype=int32)
> 
> Error details:
> Tensor("cond/Identity:0", shape=(None,), dtype=int64) and Tensor("cond/Identity:0", shape=(None,), dtype=int32) have different types
> ```
> It looks like class weight might not work well.
> 
> ### Relevant log output
> 
> _No response_

---

### [Inconsistencies between 2d and 3d ops](https://github.com/tensorflow/tensorflow/issues/57835)

**Created:** 2022-09-25T17:48:51Z

**Tags:** `stat:awaiting response type:bug stale comp:ops TF 2.10`

**Content:**

> <details><summary>Click to expand!</summary> 
>  
>  ### Issue Type
> 
> Bug
> 
> ### Source
> 
> source
> 
> ### Tensorflow Version
> 
> tf2.10
> 
> ### Custom Code
> 
> Yes
> 
> ### OS Platform and Distribution
> 
> Linux Ubuntu 18.04.6
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.7.6
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/Compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current Behaviour?
> 
> ```shell
> Inspired by the document of tf.nn.conv1d which writes "Internally, this op reshapes the input tensors and invokes tf.nn.conv2d", I want to check if the 2d operators can be implemented using their 3d versions by reshaping the input tensors and some arguments. In the following colab link, I compared between Conv2D and Conv3D, MaxPool2D and MaxPool3D, AveragePooling2D and AveragePooling3D. 
> 
> I added an extra dimension to the input, strides, and kernel_size/pool_size. The original 2d operators should be equivalent to the version that implements a 2d operator using its 3d version. However, when feeding them with those specific arguments and input, I detect inconsistencies in the output when using the latest version of tensorflow. 
> 
> I also checked previous versions, and found in old versions such as tf2.4 and before, those codes will show no inconsistencies between the original 2d version and the version that implemented 2d ops using 3d ops. I believe this may be a bug because of this change between versions.
> ```
> 
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> Colab link: https://colab.research.google.com/drive/15vYN__3x0BT0c5co83k38w7CMCqIqWtC?usp=sharing
> ```
> 
> 
> ### Relevant log output
> 
> _No response_</details>

---

### [tflite_model_maker not exporting to quantised int8](https://github.com/tensorflow/tensorflow/issues/57619)

**Created:** 2022-09-06T06:24:48Z

**Tags:** `type:bug comp:lite TF 2.9`

**Content:**

> ### 1. System information
> _This error happens both on my PC:_
> - OS Platform and Distribution: Windows 10(amd64) v10.0
> - TensorFlow installation (pip package or built from source): From pip, TF version v2.9.0-18-gd8ce9f9c301 2.9.1 on Python 3.9
> - TensorFlow library: 
> 
> _AND on Google Colab_
> 
> ### 2. Code
> _What I am trying to do:_
> Train a model and export a quantised int8 version for later use with the TPU USB accelerator. I export a non-quantized version for comparison.
> 
> _What I get:_
> The model which is supposed to be quantized always exports identically to the un-quantized version, with the same message when running "model.export()" that "Statistics for quantized inputs were expected, but not specified; continuing anyway."
> Link to the exported model:
> https://drive.google.com/file/d/1Gt4XT1J2Ujt3dXy-vKgwESs2fciTDqFA/view?usp=sharing
> 
> ```
> # import
> import tensorflow as tf
> 
> assert tf.__version__.startswith('2')
> from tflite_model_maker.config import QuantizationConfig
> from tflite_model_maker.image_classifier import DataLoader
> from tflite_model_maker import model_spec
> from tflite_model_maker import image_classifier
> 
> # train data folder path to my google drive, all images are .jpeg
> data = DataLoader.from_folder(train_data_folder_path)
> train_data, val_data = data.split(0.8)
> model_spec = model_spec.get('mobilenet_v2')
> model = image_classifier.create(
>     train_data,
>     model_spec=model_spec,
>     validation_data=val_data,
> )
> 
> # test data folder path goes to my google drive, all images are .jpeg
> test_data = DataLoader.from_folder(test_data_folder_path)
> 
> # First export the model with default settings
> tflite_filename = "default_model.tflite"
> model.export(export_dir=".", tflite_filename=tflite_filename)
> 
> # Then, export as quantized
> quant_tflite_filename = "int8_model.tflite"
> quantization_config = QuantizationConfig.for_int8(test_data)
> model.export(export_dir=".", tflite_filename=quant_tflite_filename, quantization_config=quantization_config)
> ```
> 
> ### 3. Failure after conversion
> If the conversion is successful, but the generated model is wrong, then state what is wrong:
> 
> - Model does not produce a quantized version. The output files are of identical file size and the log below is the same for both.
> 
> ### 5. Any other info / logs
> >>> model._export_tflite(tflite_filepath=tflite_filename, quantization_config=quantization_config)
> 
> INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets
> INFO:tensorflow:Assets written to: C:\Users\sm251\AppData\Local\Temp\tmpymme3pmk\assets
> 2022-09-06 14:35:11.483922: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
> 2022-09-06 14:35:11.484556: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
> C:\Users\sm251\AppData\Roaming\Python\Python39\site-packages\tensorflow\lite\python\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.
>   warnings.warn("Statistics for quantized inputs were expected, but not "
> 2022-09-06 14:35:21.939062: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
> 2022-09-06 14:35:21.939219: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
> fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3
> INFO:tensorflow:Label file is inside the TFLite model with metadata.
> INFO:tensorflow:Label file is inside the TFLite model with metadata.
> INFO:tensorflow:Saving labels in C:\Users\sm251\AppData\Local\Temp\tmpf7kb5uh9\labels.txt
> INFO:tensorflow:Saving labels in C:\Users\sm251\AppData\Local\Temp\tmpf7kb5uh9\labels.txt

---

### [`Check failed` in `tf.raw_ops.TensorScatterAdd` and `tf.tensor_scatter_nd_add` when the rank of `indices` > 2. ](https://github.com/tensorflow/tensorflow/issues/65671)

**Created:** 2024-04-14T16:46:04Z

**Tags:** `stat:awaiting response type:bug stale comp:ops TF 2.15`

**Content:**

> ### Issue type
> 
> Bug
> 
> ### Have you reproduced the bug with TensorFlow Nightly?
> 
> Yes
> 
> ### Source
> 
> binary
> 
> ### TensorFlow version
> 
> tf 2.15
> 
> ### Custom code
> 
> Yes
> 
> ### OS platform and distribution
> 
> Linux Ubuntu 20.04
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.10
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current behavior?
> 
> `Check failed` in `tf.raw_ops.TensorScatterAdd` and `tf.tensor_scatter_nd_add` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> https://colab.research.google.com/drive/18JWN3lSTdwvwuG2yka6Uek7CHmwH9ffi?usp=sharing
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> Check failed: d < dims() (1 vs. 1)
> ```
> 

---

### [Documentation Bug about API ActivityRegularization](https://github.com/tensorflow/tensorflow/issues/61254)

**Created:** 2023-07-12T10:05:55Z

**Tags:** `type:docs-bug stat:awaiting response awaiting review type:bug stale comp:apis TF 2.12`

**Content:**

> ### Issue type
> 
> Documentation Bug
> 
> ### Have you reproduced the bug with TensorFlow Nightly?
> 
> No
> 
> ### Source
> 
> source
> 
> ### TensorFlow version
> 
> tf2.12.0
> 
> ### Custom code
> 
> Yes
> 
> ### OS platform and distribution
> 
> MacOs
> 
> ### Mobile device
> 
> _No response_
> 
> ### Python version
> 
> 3.9
> 
> ### Bazel version
> 
> _No response_
> 
> ### GCC/compiler version
> 
> _No response_
> 
> ### CUDA/cuDNN version
> 
> _No response_
> 
> ### GPU model and memory
> 
> _No response_
> 
> ### Current behavior?
> 
> #### Document
> 
> | `l1` | L1 regularization factor (positive float). |
> | ---- | ------------------------------------------ |
> | `l2` | L2 regularization factor (positive float). |
> 
> The l1 and l2 parameters of the ActivityRegularization function are described in this document as floating point numbers and their values should be positive. But we found that they can run with values less than zero
> 
> ### Standalone code to reproduce the issue
> 
> ```shell
> from tensorflow import keras
> 
> 
> def gru(num_units=25, input_shape=10):
> # gru input layer
>     input_tensor = keras.Input(shape=input_shape)
> # gru hidden layer
>     x = keras.layers.Embedding(input_dim=100, output_dim=10, input_length=None)(input_tensor)
>     x = keras.layers.GRU(units=32, dropout=0.7338014982069313, return_sequences=True)(x)
>     x = keras.layers.ActivityRegularization(l1=-0.616784030867379, l2=-0.9646777799675004)(x)
> # gru output layer
>     output_tensor = keras.layers.Flatten()(keras.layers.Dense(units=num_units, activation="relu")(x))
>     model = keras.models.Model(inputs=input_tensor, outputs=output_tensor)
>     return model
> 
> 
> if __name__ == "__main__":
>     gru().summary()
> ```
> 
> 
> ### Relevant log output
> 
> ```shell
> Model: "model"
> _________________________________________________________________
>  Layer (type)                Output Shape              Param #   
> =================================================================
>  input_1 (InputLayer)        [(None, 10)]              0         
>                                                                  
>  embedding (Embedding)       (None, 10, 10)            1000      
>                                                                  
>  gru (GRU)                   (None, 10, 32)            4224      
>                                                                  
>  activity_regularization (Ac  (None, 10, 32)           0         
>  tivityRegularization)                                           
>                                                                  
>  dense (Dense)               (None, 10, 25)            825       
>                                                                  
>  flatten (Flatten)           (None, 250)               0         
>                                                                  
> =================================================================
> Total params: 6,049
> Trainable params: 6,049
> Non-trainable params: 0
> ```
> ```
> 

---

### [Broken link in readme. ](https://github.com/jax-ml/jax/issues/13901)

**Created:** 2023-01-06T19:15:12Z

**Tags:** `bug documentation`

**Content:**

> ### Description
> 
> The link in the readme under the current gotchas section is broken. "[Random numbers are different](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers), but for [good reasons](https://github.com/google/jax/blob/main/docs/design_notes/prng.md). The 'google/jax' repository doesn't contain the 'docs/design_notes/prng.md' path in 'main'.
> 
> 
> ### What jax/jaxlib version are you using?
> 
> _No response_
> 
> ### Which accelerator(s) are you using?
> 
> _No response_
> 
> ### Additional system info
> 
> _No response_
> 
> ### NVIDIA GPU info
> 
> _No response_

---

### [Incorrect documentation for mode='wrap' of `jax.scipy.ndimage.map_coordinates`.](https://github.com/jax-ml/jax/issues/20333)

**Created:** 2024-03-20T12:03:21Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> The documentation of [jax.scipy.ndimage.map_coordinates](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.ndimage.map_coordinates.html) currently says:
> >  Note that interpolation near boundaries differs from the scipy function, because we fixed an outstanding bug ([scipy/scipy#2640](https://github.com/scipy/scipy/issues/2640)); this function interprets the mode argument as documented by SciPy, but not as implemented by SciPy.
> 
> And also says:
> 
> > ’grid-wrap’ (a b c d | a b c d | a b c d)
> The input is extended by wrapping around to the opposite edge.
> 
> > ’wrap’ (d b c d | a b c d | b c a b)
> The input is extended by wrapping around to the opposite edge, but in a way such that the last point and initial point exactly overlap. In this case it is not well defined which sample will be chosen at the point of overlap.
> 
> However, empirically, it seems that the implementation of ’wrap’ in the jax version, actually corresponds to the implementation of  ’grid-wrap’ in the scipy version. Example:
> 
> ```
> import jax 
> import numpy as np
> import scipy
> 
> kwargs = dict(
>     input=np.array([[1., 2., 3., 4., 5.]]),
>     coordinates=np.array([
>         [0, 0.5],
>         [0, 1.5],
>         [0, 2.5],
>         [0, 3.5],
>         [0, 4.5],  
>         ]).T,
>     order=1,   
> )
> 
> print(scipy.ndimage.map_coordinates(**kwargs, mode="wrap"))  # [1.5 2.5 3.5 4.5 1.5]  # Vestigial, unintuitive scipy behavior.
> print(jax.scipy.ndimage.map_coordinates(**kwargs, mode="wrap"))  # [1.5 2.5 3.5 4.5 3. ]  # Correct intuitive behavior.
> 
> print(scipy.ndimage.map_coordinates(**kwargs, mode="grid-wrap"))  # [1.5 2.5 3.5 4.5 3. ]  # Correct intuitive behavior.
> print(jax.scipy.ndimage.map_coordinates(**kwargs, mode="grid-wrap"))  # Not implemented error.
> ```
> 
> Note JAX behavior of `mode="wrap"` is in my opinion the most intuitive one, so I am not advocating for any behavior change. However, the documentation of the scipy version was eventually changed, so this comment:
> 
> > Note that interpolation near boundaries differs from the scipy function, because we fixed an outstanding bug ([scipy/scipy#2640](https://github.com/scipy/scipy/issues/2640)); this function interprets the mode argument as documented by SciPy, but not as implemented by SciPy.
> 
> is no longer true, and in fact, now it is the jax version that has inconsistencies between the documentation and the implementation (`mode="wrap"` is documented as `mode="grid-wrap"`).
> 
> 
> ### System info (python version, jaxlib version, accelerator, etc.)
> 
> Version 0.4.26

---

### [Int8 incorrect result on gpu](https://github.com/jax-ml/jax/issues/19762)

**Created:** 2024-02-12T22:05:25Z

**Tags:** `bug XLA`

**Content:**

> ### Description
> 
> I found inconsistent results between cpu and gpu in some complicated code of mine when using int8 on gpu.
> I was able to condense it down to the following reproducer:
> 
> ```python
> import jax
> import jax.numpy as jnp
> from functools import partial
> import numpy as np
> 
> def fancy_repeat(x):
>     y1 = jnp.repeat(x[:6][None], 2, axis=0)
>     y2 = jnp.repeat(x[6:][None], 2, axis=0)
>     y = jnp.concatenate([y1[:, None], y2[:, None]], axis=-2)
>     return jax.lax.collapse(y, y.ndim-2, y.ndim)
> 
> def test(x):
>     x2 = jnp.repeat(x[None], 2, axis=0)
>     y2 = fancy_repeat(x)  
>     mask = (x2 == y2).all(axis=-1) # this mask here is wrong on gpu with int8
>     y = y2.sum(axis=-1)
>     return mask*y
> 
> x8 = np.ones(12, dtype=jnp.int8)
> x32 = np.ones(12, dtype=jnp.int32)
> 
> test_cpu = jax.jit(test, backend='cpu')
> test_gpu = jax.jit(test, backend='cuda')
> 
> print('cpu int32', test_cpu(x32))  # cpu int32 [12 12]
> print('gpu int32', test_gpu(x32))  # gpu int32 [12 12]
> print()
> print('cpu int8', test_cpu(x8))    # cpu int8 [12 12]
> print('gpu int8', test_gpu(x8))    # gpu int8 [0 0]
> ```
> 
> with jaxlib 0.4.18 it still worked as expected, starting from v0.4.19 it's returning the wrong result.
> 
> ### What jax/jaxlib version are you using?
> 
> jax v0.4.24, jaxlib v0.4.24+cuda12.cudnn89
> 
> ### Which accelerator(s) are you using?
> 
> GPU
> 
> ### Additional system info?
> 
> Google colab with updated "jax[cuda12_pip]" 
> 
> ### NVIDIA GPU info
> 
> ```
> +---------------------------------------------------------------------------------------+
> | NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
> |-----------------------------------------+----------------------+----------------------+
> | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
> |                                         |                      |               MIG M. |
> |=========================================+======================+======================|
> |   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |
> | N/A   52C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
>                                                                                          
> +---------------------------------------------------------------------------------------+
> | Processes:                                                                            |
> |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
> |        ID   ID                                                             Usage      |
> |=======================================================================================|
> |  No running processes found                                                           |
> +---------------------------------------------------------------------------------------+
> ```

---

### [MHLO doesn't contain same transformations as HLO](https://github.com/jax-ml/jax/issues/10715)

**Created:** 2022-05-15T18:46:22Z

**Tags:** `bug`

**Content:**

> More a question than a bug about whether this behaviour is desired.
> Currently, the MHLO from compiler_ir is taking the jaxpr and writing it in the MHLO dialect. It doesn't seem to have the simplification passes that HLO does. See a gather below, the HLO and compiled HLO both transform the gather into a slice operation while MHLO does not. 
> 
> ```python
> import jax
> from jax import numpy as jnp
> x = jnp.zeros((2,))
> f = lambda x: x.at[0].get()
> 
> jaxpr = jax.make_jaxpr(f)(x)
> l = jax.jit(f).lower(x)
> mhlo = l.compiler_ir('mhlo')
> hlo = l.compiler_ir('hlo').as_hlo_text()
> c = l.compile()
> compiled_hlo = c.compiler_ir()[0].to_string()
> 
> print("# jaxpr")
> display(jaxpr)
> print('# MHLO\n', mhlo)
> print('# HLO\n', hlo)
> print('# Compiled HLO\n', compiled_hlo)
> # Output
> # jaxpr
> { lambda ; a:f32[2]. let
>     b:bool[] = lt 0 0
>     c:i32[] = add 0 2
>     d:i32[] = select_n b 0 c
>     e:i32[] = convert_element_type[new_dtype=int32 weak_type=False] d
>     f:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e
>     g:f32[] = gather[
>       dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))
>       fill_value=None
>       indices_are_sorted=True
>       mode=GatherScatterMode.PROMISE_IN_BOUNDS
>       slice_sizes=(1,)
>       unique_indices=True
>     ] a f
>   in (g,) }
> # MHLO
>  module @jit__lambda_.2 {
>   func public @main(%arg0: tensor<2xf32>) -> tensor<f32> {
>     %0 = mhlo.constant dense<0> : tensor<i32>
>     %1 = mhlo.constant dense<0> : tensor<i32>
>     %2 = "mhlo.compare"(%0, %1) {compare_type = #mhlo<"comparison_type SIGNED">, comparison_direction = #mhlo<"comparison_direction LT">} : (tensor<i32>, tensor<i32>) -> tensor<i1>
>     %3 = mhlo.constant dense<0> : tensor<i32>
>     %4 = mhlo.constant dense<2> : tensor<i32>
>     %5 = mhlo.add %3, %4 : tensor<i32>
>     %6 = mhlo.constant dense<0> : tensor<i32>
>     %7 = "mhlo.select"(%2, %5, %6) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
>     %8 = mhlo.convert %7 : tensor<i32>
>     %9 = "mhlo.broadcast_in_dim"(%8) {broadcast_dimensions = dense<> : tensor<0xi64>} : (tensor<i32>) -> tensor<1xi32>
>     %10 = "mhlo.gather"(%arg0, %9) {dimension_numbers = #mhlo.gather<collapsed_slice_dims = [0], start_index_map = [0]>, indices_are_sorted = true, slice_sizes = dense<1> : tensor<1xi64>} : (tensor<2xf32>, tensor<1xi32>) -> tensor<f32>
>     return %10 : tensor<f32>
>   }
> }
> 
> # HLO
>  HloModule jit__lambda_.2
> 
> ENTRY main.4 {
>   Arg_0.1 = f32[2]{0} parameter(0)
>   slice.2 = f32[1]{0} slice(Arg_0.1), slice={[0:1]}
>   ROOT reshape.3 = f32[] reshape(slice.2)
> }
> 
> 
> # Compiled HLO
>  HloModule jit__lambda_.2
> 
> %fused_computation (param_0.1: f32[2]) -> f32[] {
>   %param_0.1 = f32[2]{0} parameter(0)
>   %slice.0 = f32[1]{0} slice(f32[2]{0} %param_0.1), slice={[0:1]}, metadata={op_name="jit(<lambda>)/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]" source_file="<ipython-input-2-ec9c6db7d6e5>" source_line=4}
>   ROOT %reshape.0 = f32[] reshape(f32[1]{0} %slice.0), metadata={op_name="jit(<lambda>)/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]" source_file="<ipython-input-2-ec9c6db7d6e5>" source_line=4}
> }
> 
> ENTRY %main.4 (Arg_0.1: f32[2]) -> f32[] {
>   %Arg_0.1 = f32[2]{0} parameter(0)
>   ROOT %fusion = f32[] fusion(f32[2]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name="jit(<lambda>)/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]" source_file="<ipython-input-2-ec9c6db7d6e5>" source_line=4}
> }
> ```
> 
> One could produce the MHLO simplified from the xla computation as below rather than parsing the jaxpr
> ```python
> from jax._src.lib.mlir import ir
> from jax._src.lib import xla_client as xc
> from jax.interpreters import mlir
> 
> context = mlir.make_ir_context()
> xla_comp = l._xla_computation()
> module_str = xc._xla.mlir.xla_computation_to_mlir_module(xla_comp)
> m = ir.Module.parse(module_str, context=context)
> print('# MHLO converted from underlying xla computation\n', m)
> # Output
> # MHLO converted from HLO
>  module @jit__wrapped.6 {
>   func @main(%arg0: tensor<2x3xf32>) -> tensor<3xf32> {
>     %0 = "mhlo.slice"(%arg0) {limit_indices = dense<[1, 3]> : tensor<2xi64>, start_indices = dense<0> : tensor<2xi64>, strides = dense<1> : tensor<2xi64>} : (tensor<2x3xf32>) -> tensor<1x3xf32>
>     %1 = "mhlo.reshape"(%0) : (tensor<1x3xf32>) -> tensor<3xf32>
>     return %1 : tensor<3xf32>
>   }
> }
> 
> ```
> 
> Note: Currently tflite's experimental_from_jax converter uses this path, generating the xla computation which it then deals with the MHLO repr of
> 
> I think the grand plan is to eliminate HLO and MHLO to be all of it. This check here could be used to be sure all the great optimisations on HLO are transferred to MHLO.
> 

---

### [Failed to build with Jaxlib v0.3.10 (and latest branch) on Windows ](https://github.com/jax-ml/jax/issues/11076)

**Created:** 2022-06-12T21:30:00Z

**Tags:** `bug`

**Content:**

> Attempting to build  jaxlib v0.3.10 (or the latest branch) on windows 10 using python 3.9 / Cuda v11.2 with the following command,
> 
> `python .\build\build.py --enable_cuda --cuda_path="C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2" --cudnn_path="C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2" --cuda_version="11.2" --cudnn_version="8.1.0" --noenable_rocm --noenable_tpu`
> 
> results in the following build error...
> 
> ```
> ERROR: C:/users/adam/_bazel_adam/qubkz24d/external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/BUILD:36:11: Compiling tensorflow/compiler/xla/pjrt/distributed/service.cc failed: (Exit 2): python.exe failed: error executing command
>   cd /d C:/users/adam/_bazel_adam/qubkz24d/execroot/__main__
>   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2
>     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2
>     SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
>     SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
>     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
>     SET PWD=/proc/self/cwd
>     SET RUNFILES_MANIFEST_ONLY=1
>     SET TEMP=C:\Users\Adam\AppData\Local\Temp
>     SET TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80
>     SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2
>     SET TF_CUDA_VERSION=11.2
>     SET TF_CUDNN_VERSION=8.1.0
>     SET TMP=C:\Users\Adam\AppData\Local\Temp
>   C:\Users\Adam\anaconda3\envs\jax_latest\python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/org_tensorflow /Ibazel-out/x64_windows-opt/bin/external/org_tensorflow /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/com_github_grpc_grpc /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/upb /Ibazel-out/x64_windows-opt/bin/external/upb /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/llvm_terminfo /Ibazel-out/x64_windows-opt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazel-out/x64_windows-opt/bin/external/llvm_zlib /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferIntRangeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen /Iexternal/com_github_grpc_grpc/include /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/include /Iexternal/com_github_grpc_grpc/src/core/ext/upb-generated /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/com_github_grpc_grpc/third_party/address_sorting/include /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/org_tensorflow/third_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/external/org_tensorflow/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /DGRPC_ARES=0 /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT=".dll" /DLLVM_PLUGIN_EXT=".dll" /DLLVM_NATIVE_ARCH="X86" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE="x86_64-pc-win32" /DLLVM_DEFAULT_TARGET_TRIPLE="x86_64-pc-win32" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DCURL_STATICLIB /showIncludes /MD /O2 /DNDEBUG /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /Zc:preprocessor -DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. /std:c++17 /Fobazel-out/x64_windows-opt/bin/external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/_objs/service/service.obj /c external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/service.cc
> # Configuration: d8ef4ae28c934a6d5c0a217d5355c714ef527c85e4ff8df181c0e0d4ca69be3c
> # Execution platform: @local_execution_config_platform//:platform
> Target //build:build_wheel failed to build
> INFO: Elapsed time: 7708.621s, Critical Path: 911.70s
> INFO: 10505 processes: 3351 internal, 7154 local.
> FAILED: Build did NOT complete successfully
> FAILED: Build did NOT complete successfully
> b"C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um\\winbase.h(9305): warning C5105: macro expansion producing 'defined' has undefined behavior\r\nexternal/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/service.cc(56): error C2398: Element '1': conversion from 'size_t' to 'const Key &' requires a narrowing conversion\r\n        with\r\n        [\r\n            Key=google::protobuf::int32\r\n        ]\r\n"
> Traceback (most recent call last):
>   File "C:\sdks\jax-main\build\build.py", line 528, in <module>
>     main()
>   File "C:\sdks\jax-main\build\build.py", line 523, in main
>     shell(command)
>   File "C:\sdks\jax-main\build\build.py", line 53, in shell
>     output = subprocess.check_output(cmd)
>   File "C:\Users\Adam\anaconda3\envs\jax_latest\lib\subprocess.py", line 424, in check_output
>     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
>   File "C:\Users\Adam\anaconda3\envs\jax_latest\lib\subprocess.py", line 528, in run
>     raise CalledProcessError(retcode, process.args,
> subprocess.CalledProcessError: Command '['.\\bazel-5.1.1-windows-x86_64.exe', 'run', '--verbose_failures=true', '--config=mkl_open_source_only', '--config=cuda', ':build_wheel', '--', '--output_path=C:\\sdks\\jax-main\\dist', '--cpu=AMD64']' returned non-zero exit status 1.
> ```
> 
> 

---

### [Importing `jax.numpy` leads to Partially Initialised module error ](https://github.com/jax-ml/jax/issues/10605)

**Created:** 2022-05-06T18:31:59Z

**Tags:** `bug`

**Content:**

> Please:
> 
> - [x] Check for duplicate issues.
> 
> I am running `JAX` on a `Fedora 35` system, with `CUDA 11.6`, `CuDNN 8.2`, `Driver version 510.60.02`
> 
> [I installed `CuDNN` based on the `RHEL8` instructions [here](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html), since `Fedora 35` doesn't seem to officially get the builds for it]
> I installed `JAX` by following the instructions present in the README.md file. On doing that, and simply running the import command, I get the following error
> 
> ```python
> import jax.numpy as jnp
> ```
> 
> ```python
> AttributeError                            Traceback (most recent call last)
> Input In [8], in <cell line: 4>()
>       2 import nibabel as nib
>       3 import numpy as np 
> ----> 4 import jax.numpy as jnp
>       6 class NIIHandler():
>       8     def __init__(self, TRAIN_DATASET_PATH):
> 
> File ~/.local/lib/python3.10/site-packages/jax/__init__.py:58, in <module>
>      38 from jax._src.config import (
>      39   config as config,
>      40   enable_checks as enable_checks,
>    (...)
>      55   transfer_guard_device_to_host as transfer_guard_device_to_host,
>      56 )
>      57 from .core import eval_context as ensure_compile_time_eval
> ---> 58 from jax._src.api import (
>      59   ad,  # TODO(phawkins): update users to avoid this.
>      60   block_until_ready,
>      61   checkpoint as checkpoint,
>      62   checkpoint_policies as checkpoint_policies,
>      63   closure_convert as closure_convert,
>      64   curry,  # TODO(phawkins): update users to avoid this.
>      65   custom_gradient as custom_gradient,
>      66   custom_jvp as custom_jvp,
>      67   custom_vjp as custom_vjp,
>      68   default_backend as default_backend,
>      69   device_count as device_count,
>      70   device_get as device_get,
>      71   device_put as device_put,
>      72   device_put_sharded as device_put_sharded,
>      73   device_put_replicated as device_put_replicated,
>      74   devices as devices,
>      75   disable_jit as disable_jit,
>      76   eval_shape as eval_shape,
>      77   flatten_fun_nokwargs,  # TODO(phawkins): update users to avoid this.
>      78   float0 as float0,
>      79   grad as grad,
>      80   hessian as hessian,
>      81   host_count as host_count,
>      82   host_id as host_id,
>      83   host_ids as host_ids,
>      84   jacobian as jacobian,
>      85   jacfwd as jacfwd,
>      86   jacrev as jacrev,
>      87   jit as jit,
>      88   jvp as jvp,
>      89   local_device_count as local_device_count,
>      90   local_devices as local_devices,
>      91   linearize as linearize,
>      92   linear_transpose as linear_transpose,
>      93   make_jaxpr as make_jaxpr,
>      94   mask as mask,
>      95   named_call as named_call,
>      96   pmap as pmap,
>      97   process_count as process_count,
>      98   process_index as process_index,
>      99   pxla,  # TODO(phawkins): update users to avoid this.
>     100   remat as remat,
>     101   shapecheck as shapecheck,
>     102   ShapedArray as ShapedArray,
>     103   ShapeDtypeStruct as ShapeDtypeStruct,
>     104   # TODO(phawkins): hide tree* functions from jax, update callers to use
>     105   # jax.tree_util.
>     106   treedef_is_leaf,
>     107   tree_flatten,
>     108   tree_leaves,
>     109   tree_map,
>     110   tree_multimap,
>     111   tree_structure,
>     112   tree_transpose,
>     113   tree_unflatten,
>     114   value_and_grad as value_and_grad,
>     115   vjp as vjp,
>     116   vmap as vmap,
>     117   xla,  # TODO(phawkins): update users to avoid this.
>     118   xla_computation as xla_computation,
>     119 )
>     120 from jax.experimental.maps import soft_pmap as soft_pmap
>     121 from jax.version import __version__ as __version__
> 
> File ~/.local/lib/python3.10/site-packages/jax/_src/api.py:61, in <module>
>      55 from jax._src import traceback_util
>      56 from jax._src.api_util import (
>      57     flatten_fun, apply_flat_fun, flatten_fun_nokwargs, flatten_fun_nokwargs2,
>      58     argnums_partial, argnums_partial_except, flatten_axes, donation_vector,
>      59     rebase_donate_argnums, _ensure_index, _ensure_index_tuple,
>      60     shaped_abstractify, _ensure_str_tuple, argnames_partial_except)
> ---> 61 from jax._src.lax import lax as lax_internal
>      62 from jax._src.lib import jax_jit
>      63 from jax._src.lib import xla_bridge as xb
> 
> File ~/.local/lib/python3.10/site-packages/jax/_src/lax/lax.py:1653, in <module>
>    1651 tan_p = standard_unop(_float | _complex, 'tan')
>    1652 ad.defjvp2(tan_p, lambda g, ans, x: mul(g, _const(x, 1) + square(ans)))
> -> 1653 if jax._src.lib.mlir_api_version >= 11:
>    1654   mlir.register_lowering(tan_p, partial(_nary_lower_mhlo, chlo.TanOp))
>    1655 else:
> 
> AttributeError: partially initialized module 'jax' has no attribute '_src' (most likely due to a circular import)
> ```
> 
> I am extremely new to `JAX`, so please do let me know if there is something else I should be trying instead. Attaching my `nvidia-smi` and `nvcc -- version` results below.
> 
> ![image](https://user-images.githubusercontent.com/70141886/167195559-3086f698-2dd6-4cd0-8bfc-8d9df4126cb9.png)
> 
> ![image](https://user-images.githubusercontent.com/70141886/167195599-7fe5c522-fada-47bf-a07a-f7630c76993e.png)
> 
> Thank you very much!
> 

---

### [Segmentation fault using jnp.einsum on jax 0.6.0](https://github.com/jax-ml/jax/issues/28896)

**Created:** 2025-05-21T16:51:55Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> After a fresh install, I ran the following in ipython:
> ```python
> import jax.numpy as jnp
> from jax import random
> 
> rng = random.key(42)
> a = random.normal(rng, (10, 100))
> b = random.normal(rng, (20, 100))
> c = jnp.einsum('A D, B D -> A B', a, b) # segfaults
> a @ b.T # also segfaults
> ```
> 
> ### System info (python version, jaxlib version, accelerator, etc.)
> 
> ```
> jax:    0.6.0
> jaxlib: 0.6.0
> numpy:  2.2.6
> python: 3.12.7 (main, Oct  6 2024, 23:32:45) [GCC 13.2.0]
> device info: NVIDIA GeForce RTX 4090-1, 1 local devices"
> process_count: 1
> platform: uname_result(system='Linux', node='ghost', release='6.12.28_1', version='#1 SMP PREEMPT_DYNAMIC Sun May 11 04:22:51 UTC 2025', machine='x86_64')
> 
> 
> $ nvidia-smi
> Wed May 21 17:51:30 2025
> +-----------------------------------------------------------------------------------------+
> | NVIDIA-SMI 570.144                Driver Version: 570.144        CUDA Version: 12.8     |
> |-----------------------------------------+------------------------+----------------------+
> | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
> |                                         |                        |               MIG M. |
> |=========================================+========================+======================|
> |   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0  On |                  Off |
> |  0%   52C    P0             57W /  480W |    1445MiB /  24564MiB |      9%      Default |
> |                                         |                        |                  N/A |
> +-----------------------------------------+------------------------+----------------------+
> 
> +-----------------------------------------------------------------------------------------+
> | Processes:                                                                              |
> |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
> |        ID   ID                                                               Usage      |
> |=========================================================================================|
> |    0   N/A  N/A            1426      G   /usr/libexec/Xorg                       517MiB |
> |    0   N/A  N/A            1481      G   picom                                   102MiB |
> |    0   N/A  N/A            2855      G   ...ersion=20250520-180039.357000        108MiB |
> |    0   N/A  N/A            3273      G   /usr/bin/alacritty                       15MiB |
> |    0   N/A  N/A            3659      G   /usr/bin/alacritty                       15MiB |
> |    0   N/A  N/A            3942      G   ...Ptr --variations-seed-version        175MiB |
> |    0   N/A  N/A            9565      C   ...v/versions/neurips/bin/python        390MiB |
> +-----------------------------------------------------------------------------------------+
> ```

---

### [Jax hangs when using PositionalSharding and fft2.](https://github.com/jax-ml/jax/issues/17404)

**Created:** 2023-09-01T17:53:32Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> I would like to use positional sharding to batch some computation (involving fft2) across gpus. The program hangs at jax.numpy.fft.fft2. As a minimal example:
> 
> ```python
> import jax
> import jax.numpy as jnp
> from jax.experimental import mesh_utils
> from jax.sharding import PositionalSharding
> key = jax.random.PRNGKey(0)
> x = jax.random.normal(key, (8, 64, 64, 64, 2))
> @jax.jit
> def f(x):
>     y = jnp.fft.fft2(x, axes=(0, 1))
>     return y    
> 
> n_devices = jax.local_device_count()
> sharding = PositionalSharding(mesh_utils.create_device_mesh(n_devices,)).reshape((n_devices, 1, 1, 1, 1))
> _x = jax.device_put(x, sharding)
> 
> # This works fine
> print('Non-sharded', jax.vmap(f)(x).shape)
> 
> # This does not
> jax.vmap(f)(_x)
> ```
> 
> Weirdly, the GPU utilization stays at 100% with no deviation. 
> 
> ### What jax/jaxlib version are you using?
> 
> jax v0.4.14, jaxlib v0.4.14+cuda12.cudnn89
> 
> ### Which accelerator(s) are you using?
> 
> GPU
> 
> ### Additional system info
> 
> Python 3.10, Ubuntu 22.04.2 LTS, cuDNN v8.9.4
> 
> ### NVIDIA GPU info
> 
> ```
> +---------------------------------------------------------------------------------------+
> | NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
> |-----------------------------------------+----------------------+----------------------+
> | GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |                                         |                      |               MIG M. |
> |=========================================+======================+======================|
> |   0  NVIDIA RTX A6000                On | 00000000:4F:00.0 Off |                  Off |
> | 30%   37C    P2               91W / 300W|   2855MiB / 49140MiB |    100%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   1  NVIDIA RTX A6000                On | 00000000:52:00.0 Off |                  Off |
> | 30%   42C    P2               98W / 300W|    915MiB / 49140MiB |    100%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   2  NVIDIA RTX A6000                On | 00000000:53:00.0 Off |                  Off |
> | 30%   40C    P2               76W / 300W|   4955MiB / 49140MiB |     39%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   3  NVIDIA RTX A6000                On | 00000000:56:00.0 Off |                  Off |
> | 33%   61C    P2              208W / 300W|  38695MiB / 49140MiB |     99%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   4  NVIDIA RTX A6000                On | 00000000:57:00.0 Off |                  Off |
> | 35%   65C    P2              211W / 300W|  38695MiB / 49140MiB |      7%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   5  NVIDIA RTX A6000                On | 00000000:CE:00.0 Off |                  Off |
> | 36%   64C    P2              200W / 300W|  38695MiB / 49140MiB |     99%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   6  NVIDIA RTX A6000                On | 00000000:D1:00.0 Off |                  Off |
> | 49%   73C    P2              287W / 300W|  37403MiB / 49140MiB |     94%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   7  NVIDIA RTX A6000                On | 00000000:D2:00.0 Off |                  Off |
> | 33%   63C    P2              188W / 300W|  48431MiB / 49140MiB |     99%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   8  NVIDIA RTX A6000                On | 00000000:D5:00.0 Off |                  Off |
> | 30%   26C    P8               13W / 300W|  10097MiB / 49140MiB |      0%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> |   9  NVIDIA RTX A6000                On | 00000000:D6:00.0 Off |                  Off |
> | 30%   29C    P8               20W / 300W|    663MiB / 49140MiB |      0%      Default |
> |                                         |                      |                  N/A |
> +-----------------------------------------+----------------------+----------------------+
> ```

---

### [Build error: jaxlib 0.7.0, GCC 13.2.0](https://github.com/jax-ml/jax/issues/30437)

**Created:** 2025-07-23T14:20:38Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> I'm seeing the following build errors:
> ```
> ERROR: /tmp/root/spack-stage/spack-stage-py-jaxlib-0.7.0-pynifaoelsg7wplni3lijorw3jtxkhsl/spack-src/jaxlib/mosaic/dialect/gpu/BUILD:80:11: Compiling jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc failed: (Exit 1): cc failed: error executing CppCompile command (from target //jaxlib/mosaic/dialect/gpu:mosaic_gpu)
> ...
> jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc:536:30: error: 'cast' was not declared in this scope
>   536 |   auto custom_primitive_op = cast<CustomPrimitiveOp>((*this)->getParentOp());
>       |                              ^~~~
> ...
> jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc:536:52: error: expected primary-expression before '>' token
>   536 |   auto custom_primitive_op = cast<CustomPrimitiveOp>((*this)->getParentOp());
>       |                                                    ^
> ```
> I'm guessing this has something to do with newer C++ standards not being followed. Can we get a quick patch for this?
> 
> ### System info (python version, jaxlib version, accelerator, etc.)
> 
> * Python 3.13.5
> * jaxlib 0.7.0
> * GCC 13.2.0
> 
> * [build log](https://github.com/user-attachments/files/21389605/spack-build-out.txt.gz)
> * [build env](https://github.com/user-attachments/files/21389604/spack-build-env.txt.gz)

---

### [JIT fails on GPU on certain computations (conv + stop_gradient + grad())](https://github.com/jax-ml/jax/issues/13823)

**Created:** 2022-12-29T15:46:02Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> Seeing an issue where trying to jit() run the function results in the following error:
> 
> ```XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/hlo/ir/hlo_computation.cc:962) ShapeUtil::Compatible(old_instruction->shape(), new_instruction->shape()) f32[1,5,3,12] vs f32[1,5,7,12]```
> 
> I've narrowed it down to a combination of:
> 
> - Two Conv2D:s on the same underlying array
> - Recombining them in some fashion (e.g. addition) and blocking gradient flow through one of these packs with stop_gradient
> - Doing a linear projection on the result
> - Taking grad() of all of the above.
> - Running on GPU (succeeds on CPU).
> 
> See notebook where the non-jit version succeeds, but jit() of the same function fails to execute with an internal error. 
> 
> https://colab.research.google.com/drive/1QybGmVwN90UIg7TXgqmwsS9QSiz-GrH7
> 
> ### What jax/jaxlib version are you using?
> 
> 0.3.25
> 
> ### Which accelerator(s) are you using?
> 
> GPU
> 
> ### Additional system info
> 
> Colab GPU kernel
> 
> ### NVIDIA GPU info
> 
> Thu Dec 29 15:45:31 2022       
> +-----------------------------------------------------------------------------+
> | NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
> |-------------------------------+----------------------+----------------------+
> | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
> |                               |                      |               MIG M. |
> |===============================+======================+======================|
> |   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
> | N/A   44C    P0    27W /  70W |  13610MiB / 15109MiB |      0%      Default |
> |                               |                      |                  N/A |
> +-------------------------------+----------------------+----------------------+
>                                                                                
> +-----------------------------------------------------------------------------+
> | Processes:                                                                  |
> |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
> |        ID   ID                                                   Usage      |
> |=============================================================================|

---

### [Weird defjvp behavior when finding grad of a scalar that depends on the primal](https://github.com/jax-ml/jax/issues/25101)

**Created:** 2024-11-25T23:18:14Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> Hello! I was testing the defjvp feature to deal with some stability issues of the SVD, but I noticed that defjvp doesn't work as expected if the output of the function to be differentiated depends on the primal. Let me share some code. I started by defining ``new_svd`` , which should be exactly the same as ``jax.numpy.svd`` but with its derivatives defined with defjvp (code from ``jax._src.lax.linalg.py`` with minor modifications):
> 
>     
>     import jax.lax as lax
>     from jax._src.lax import lax as lax_internal
>     from jax import custom_jvp
>     import jax.numpy as jnp
>     import jax.random as jrandom
>     import jax
> 
>     def _extract_diagonal(s):
>         i = lax.iota("int32", min(s.shape[-2], s.shape[-1]))
>         return s[..., i, i]
> 
>     def _construct_diagonal(s):
>         i = lax.iota("int32", s.shape[-1])
>         return lax.full((*s.shape, s.shape[-1]), 0, s.dtype).at[..., i, i].set(s)
> 
>     def _H(x):
>         return _T(x).conj()
> 
>     def _T(x):
>         return lax.transpose(x, (*range(x.ndim - 2), x.ndim - 1, x.ndim - 2))
>     
>     @custom_jvp
>     def new_SVD(x):
>         return jnp.linalg.svd(x, full_matrices=False)
>     
>     
>     @new_SVD.defjvp
>     def _svd_jvp_rule(primals, tangents):
>         (A,) = primals
>         (dA,) = tangents
>         U, s, Vt = jnp.linalg.svd(A, full_matrices=False)
>     
>         Ut, V = _H(U), _H(Vt)
>         s_dim = s[..., None, :]
>         dS = Ut @ dA @ V
>         ds = _extract_diagonal(dS.real)
>     
>         s_diffs = (s_dim + _T(s_dim)) * (s_dim - _T(s_dim))
>         s_diffs_zeros = lax_internal._eye(s.dtype, (s.shape[-1], s.shape[-1]))
>         s_diffs_zeros = lax.expand_dims(s_diffs_zeros, range(s_diffs.ndim - 2))
>         F = 1 / (s_diffs + s_diffs_zeros) - s_diffs_zeros
>         dSS = s_dim.astype(A.dtype) * dS
>         SdS = _T(s_dim.astype(A.dtype)) * dS
>     
>         s_zeros = (s == 0).astype(s.dtype)
>         s_inv = 1 / (s + s_zeros) - s_zeros
>         s_inv_mat = _construct_diagonal(s_inv)
>         dUdV_diag = 0.5 * (dS - _H(dS)) * s_inv_mat.astype(A.dtype)
>         dU = U @ (F.astype(A.dtype) * (dSS + _H(dSS)) + dUdV_diag)
>         dV = V @ (F.astype(A.dtype) * (SdS + _H(SdS)))
>     
>         m, n = A.shape[-2:]
>         if m > n:
>             dAV = dA @ V
>             dU = dU + (dAV - U @ (Ut @ dAV)) * s_inv.astype(A.dtype)
>         if n > m:
>             dAHU = _H(dA) @ U
>             dV = dV + (dAHU - V @ (Vt @ dAHU)) * s_inv.astype(A.dtype)
>     
>         return (U, s, Vt), (dU, ds, _H(dV))
> 
> Then I wanted to compare the results of ``jax.value_and_grad``, which should be the same since I used the same jvp rule. The code is as follows:
> 
>     def new_SVD_to_scalar(A):
>       U, s, Vt = my_SVD(A)
>       # Case 1:
>       # return jnp.linalg.norm((U*s) @ Vt - A)
>       # Case 2:
>       # return jnp.linalg.norm((U*s) @ Vt)
> 
> 
>     def normal_SVD_to_scalar(A):
>         U, s, Vt = jnp.linalg.svd(A, full_matrices=False)
>         # Case 1:
>         # return jnp.linalg.norm((U*s) @ Vt - A)
>         # Case 2:
>         # return jnp.linalg.norm((U*s) @ Vt)
> 
>     def test_random_normal(length, width):
>         A = jrandom.uniform(jrandom.PRNGKey(0), (length, width))
>         new_res = jax.value_and_grad(new_SVD_to_scalar)(A)
>         normal_res = jax.value_and_grad(normal_SVD_to_scalar)(A)
>         assert jnp.allclose(new_res[0], normal_res[0])
>         assert jnp.allclose(new_res[1], normal_res[1])  # Returns False in Case 1
> 
> Uncomment both statements in either Case 1 or Case 2 to alternate between cases. 
> Basically, no problems are encountered when the output of the function does not depend on A (case 2). But once A is used (e.g. case 1), the gradients computed by defjvp and the original svd are different. 
> 
> Note: The only difference between my jvp and the original is this:
> 
>     # Original
>     s, U, Vt = svd_p.bind(
>            A, full_matrices=False, compute_uv=True, subset_by_index=subset_by_index,
>            algorithm=algorithm,
>        )
> 
>     # In my jvp:
>     U, s, Vt = jnp.linalg.norm(A, full_matrices=False)
> 
> Which, in my opinion, should be different since we don't have access to the primitive svd_p when using defjvp.
> 
> Note: I know the outputs of ``svd_p.bind`` are permutated, but I don't think it is the reason of the difference.
> 
> Am I missing something? Are we not allowed to define an output as a function of the primal? Shouldn't there be an error/warning raised if it is the case?
> 
> 
> ### System info (python version, jaxlib version, accelerator, etc.)
> 
> jax:    0.4.35
> jaxlib: 0.4.35
> numpy:  2.1.2
> python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]
> device info: cpu-1, 1 local devices"
> process_count: 1
> platform: uname_result(system='Windows', node='MSI', release='10', version='10.0.22631', machine='AMD64')
> 
> 
> $ nvidia-smi
> Tue Nov 26 00:13:02 2024       
> +-----------------------------------------------------------------------------------------+
> | NVIDIA-SMI 561.09                 Driver Version: 561.09         CUDA Version: 12.6     |
> |-----------------------------------------+------------------------+----------------------+
> | GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
> | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
> |                                         |                        |               MIG M. |
> |=========================================+========================+======================|
> |   0  NVIDIA GeForce RTX 4080 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
> | N/A   44C    P4             20W /   40W |       0MiB /  12282MiB |      0%      Default |
> |                                         |                        |                  N/A |
> +-----------------------------------------+------------------------+----------------------+
> 
> +-----------------------------------------------------------------------------------------+
> | Processes:                                                                              |
> |  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
> |        ID   ID                                                               Usage      |
> |=========================================================================================|
> |  No running processes found                                                             |
> +-----------------------------------------------------------------------------------------+

---

### [Memory leak when repeatedly saving a variable during a host_callback call.](https://github.com/jax-ml/jax/issues/9970)

**Created:** 2022-03-20T03:13:39Z

**Tags:** `bug`

**Content:**

> I have a use case where I want to use host_callback to save a python object (specifically a scipy SuperLU object) and update it over and over again within a jitted function. These should persist between jit calls. Unfortunately I am running into a memory leak. I have a minimal example below, where the memory consumptions blows up linearly as the code runs. Even if I insert gc.collect() in the host_callback function, it seems that the SuperLU objects are somehow being kept around.
> 
> ```python
> import numpy as np
> import scipy.sparse
> import scipy.sparse.linalg
> 
> import jax.experimental.host_callback as hcb
> import jax
> import jax.numpy as jnp
> 
> class SolveFnContainer:
>     lu = None
> 
> def host_do_something(_):
>     print('solving')
> 
>     n = 100000
>     A = scipy.sparse.diags((-1*np.ones(n-1), -1*np.ones(n-1), 2*np.ones(n)), offsets=(-1, 1, 0), format='csc')
>     SolveFnContainer.lu = scipy.sparse.linalg.spilu(A)
>     return 0.0
> 
> def device_do_something():
>     inputs = (0.0,)
>     return hcb.call(host_do_something, inputs,
>                     result_shape=jax.ShapeDtypeStruct((), np.float64))
> 
> def main():
>     def body_fn(x):
>         return device_do_something()
> 
>     def cond_fn(x):
>         return x == 0.0
> 
>     return jax.lax.while_loop(cond_fn, body_fn, 0.0)
> 
> if __name__ == '__main__':
>     jit_main = jax.jit(main)
>     print(jit_main())
> ```

---

### [`jax.numpy.nan_to_num` function appears twice in Documentation ](https://github.com/jax-ml/jax/issues/12406)

**Created:** 2022-09-18T17:41:09Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> Reading through the documentation and found that the `jax.numpy.nan_to_num`  function seems to appear twice on the list yet it is the same function.
> It first appears here: 
> ![Screenshot_20220918_225953](https://user-images.githubusercontent.com/67042527/190920587-a6226c64-f8d4-41cb-b13e-a5e856325819.png)
> 
> And then appears here again
> ![Screenshot_20220918_230204](https://user-images.githubusercontent.com/67042527/190920661-56b962c6-0fe6-4445-ba5d-7d5c07e960b3.png)
> 
> 
> 
> ### What jax/jaxlib version are you using?
> 
> _No response_
> 
> ### Which accelerator(s) are you using?
> 
> _No response_
> 
> ### Additional System Info
> 
> _No response_

---

### [If CUDA 12.1 is installed, pip-installed ptxas binary is not used and jax throws an error](https://github.com/jax-ml/jax/issues/25718)

**Created:** 2025-01-03T18:00:30Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> Please feel free to close this issue in case this is an expected behavior.
> In case this is expected, it would be great if there would be an easy way to fix it from our (users') side other than installing newer CUDA version.
> 
> ## Summary
> If CUDA version 12.1 is installed to the system and ptxas is already in the system PATH:
> ``` shell
> ptxas --version
> ptxas: NVIDIA (R) Ptx optimizing assembler
> Copyright (c) 2005-2023 NVIDIA Corporation
> Built on Tue_Feb__7_19:30:12_PST_2023
> Cuda compilation tools, release 12.1, V12.1.66
> Build cuda_12.1.r12.1/compiler.32415258_0
> ```
> 
> After installing jax thorugh pip,
> ```sh
> python -m venv venv
> source venv/bin/activate
> pip install -U "jax[cuda12]"
> ```
> 
> The system-installed ptxas binary (instead of pip-installed one) is used and jax throws an error:
> ```python
> import jax
> jax.numpy.zeros(3)
> ```
> 
> <details>
>   <summary>Full Error Log</summary>
> 
>   ```
> E0103 16:41:38.489316 2217885 ptx_compiler_helpers.cc:87] *** WARNING *** Invoking ptxas with version 12.1.66, which corresponds to a CUDA version <=12.6.2. CUDA version
> s 12.x.y up to and including 12.6.2 miscompile certain edge cases around clamping.
> Please upgrade to CUDA 12.6.3 or newer.
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py", line 6149, in zeros
>     return lax.full(shape, 0, _jnp_dtype(dtype), sharding=_normalize_to_sharding(device))
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/lax/lax.py", line 1752, in full
>     return broadcast(fill_value, shape)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/lax/lax.py", line 1244, in broadcast
>     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims,
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/lax/lax.py", line 1278, in broadcast_in_dim
>     return broadcast_in_dim_p.bind(
>            ^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/core.py", line 463, in bind
>     return self.bind_with_trace(prev_trace, args, params)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/core.py", line 468, in bind_with_trace
>     return trace.process_primitive(self, args, params)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/core.py", line 941, in process_primitive
>     return primitive.impl(*args, **params)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/dispatch.py", line 90, in apply_primitive
>     outs = fun(*args)
>            ^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/traceback_util.py", line 180, in reraise_with_filtered_traceback
>     return fun(*args, **kwargs)
>            ^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/pjit.py", line 337, in cache_miss
>     pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)
>                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/pjit.py", line 195, in _python_pjit_helper
>     out_flat, compiled, profiler = _pjit_call_impl_python(*args_flat, **p.params)
>                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/pjit.py", line 1672, in _pjit_call_impl_python
>     ).compile()
>       ^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py", line 2415, in compile
>     executable = UnloadedMeshExecutable.from_hlo(
>                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py", line 2923, in from_hlo
>     xla_executable = _cached_compilation(
>                      ^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py", line 2729, in _cached_compilation
>     xla_executable = compiler.compile_or_get_cached(
>                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/compiler.py", line 452, in compile_or_get_cached
>     return _compile_and_write_cache(
>            ^^^^^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/compiler.py", line 653, in _compile_and_write_cache
>     executable = backend_compile(
>                  ^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/profiler.py", line 333, in wrapper
>     return func(*args, **kwargs)
>            ^^^^^^^^^^^^^^^^^^^^^
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/compiler.py", line 309, in backend_compile
>     raise e
>   File "/home/test/venv/lib/python3.12/site-packages/jax/_src/compiler.py", line 303, in backend_compile
>     return backend.compile(built_c, compile_options=options)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with non-zero error code 65280, output: ptxas /var/tmp/tempfile-###-8aa5641830c7ce6-2217885
> -62acff3e14c12, line 5; fatal   : Unsupported .version 8.3; current version is '8.1'
> ptxas fatal   : Ptx assembly aborted due to errors
> ```
> </details>
> 
> I set `LD_LIBRARY_PATH` to be empty, but still encountered this error.
> 
> ## Related Issues
> #25344: About the same error, but in my case I don't have triton installed; I think this is a separate issue.
> #18578: On ptxas binary priority issue.
> 
> ## Workaround
> We can manually prepend the pip-installed ptxas binary path to PATH to avoid this error:
> ```bash
> export PATH=$(python -c "import site; print(site.getsitepackages()[0] + '/nvidia/cuda_nvcc/bin')"):$PATH
> ```
> 
> ### System info (python version, jaxlib version, accelerator, etc.)
> 
> ```
> CUDA version: 12.1
> 
> jax:    0.4.38
> jaxlib: 0.4.38
> numpy:  2.2.1
> python: 3.12.5 (main, Aug 19 2024, 18:21:17) [GCC 9.4.0]
> device info: NVIDIA H100 80GB HBM3-1, 1 local devices"
> process_count: 1
> platform: uname_result(system='Linux', node='###', release='###', version='#18-Ubuntu SMP Fri Jul 26 14:21:24 UTC 2024', machine='x86_64')
> ```

---

### [UNKNOWN bug](https://github.com/jax-ml/jax/issues/11699)

**Created:** 2022-08-01T21:30:06Z

**Tags:** `bug NVIDIA GPU`

**Content:**

> Please:
> 
> - [x ] Check for duplicate issues.
> - [x ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this:
> 
> ```bash
> # clone jkonet from github
> # follow jkonet setup instructions
> $ python main.py --out_dir results --config_folder configs --task semicircle
> ```
> 
> - [x ] If applicable, include full error messages/tracebacks.
> ```
> /home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.
>   PyTreeDef = type(jax.tree_structure(None))
> Started run.
> 2022-08-01 17:28:07.861579: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc:57] cuLinkAddData fails. This is usually caused by stale driver version.
> 2022-08-01 17:28:07.861611: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:1325] The CUDA linking API did not work. Please use XLA_FLAGS=--xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multi-threading.
> Traceback (most recent call last):
>   File "/home/USER/projects/jkonet/main.py", line 292, in <module>
>     main(args)
>   File "/home/USER/projects/jkonet/main.py", line 266, in main
>     run_jko(config, task_dir=task_dir, logging=args.wandb)
>   File "/home/USER/projects/jkonet/main.py", line 32, in run_jko
>     rng = jax.random.PRNGKey(int(time.time()))
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/random.py", line 125, in PRNGKey
>     key = prng.seed_with_impl(impl, seed)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/prng.py", line 233, in seed_with_impl
>     return PRNGKeyArray(impl, impl.seed(seed))
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/prng.py", line 272, in threefry_seed
>     lax.shift_right_logical(seed_arr, lax_internal._const(seed_arr, 32)))
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/lax/lax.py", line 487, in shift_right_logical
>     return shift_right_logical_p.bind(x, y)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/core.py", line 324, in bind
>     return self.bind_with_trace(find_top_trace(args), args, params)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/core.py", line 327, in bind_with_trace
>     out = trace.process_primitive(self, map(trace.full_raise, args), params)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/core.py", line 684, in process_primitive
>     return primitive.impl(*tracers, **params)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 99, in apply_primitive
>     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/util.py", line 220, in wrapper
>     return cached(config._trace_context(), *args, **kwargs)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/util.py", line 213, in cached
>     return f(*args, **kwargs)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 164, in xla_primitive_callable
>     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 248, in _xla_callable_uncached
>     return lower_xla_callable(fun, device, backend, name, donated_invars, False,
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 827, in compile
>     self._executable = XlaCompiledComputation.from_xla_computation(
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 934, in from_xla_computation
>     compiled = compile_or_get_cached(backend, xla_computation, options,
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 899, in compile_or_get_cached
>     return backend_compile(backend, computation, compile_options, host_callbacks)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/profiler.py", line 294, in wrapper
>     return func(*args, **kwargs)
>   File "/home/USER/anaconda3/envs/jko/lib/python3.9/site-packages/jax/_src/dispatch.py", line 843, in backend_compile
>     return backend.compile(built_c, compile_options=options)
> jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device
> in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc(60): 'status'
> 
> ```

---

### [Partial of a vmap function](https://github.com/jax-ml/jax/issues/22339)

**Created:** 2024-07-09T12:20:17Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> For the following example code:
> 
> 
> ```python
> # Example usage
> import jax
> import jax.numpy as jnp
> from jax import vmap
> 
> @partial(vmap, in_axes=(None, 0))
> def f(y, x):
>     return x + y
> x_var = jnp.arange(10)
> 
> func_ = partial(f, x=x_var)
> func_(1)
> ```
> The following error is raised: 
> ```
> ValueError: vmap in_axes must be an int, None, or a tuple of entries corresponding to the positional arguments passed to the function, but got len(in_axes)=2, len(args)=1
> ```
> ### System info (python version, jaxlib version, accelerator, etc.)
> 
> python 3 (ipykernel)
> jax 0.4.30

---

### [Independent Categorical Sampling](https://github.com/jax-ml/jax/issues/18950)

**Created:** 2023-12-13T03:10:13Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> Hello guys, thank you so much for creating JAX it is really a marvelous tool.
> 
> Sorry if this is basic but I face the following confusing issue.
> 
> I have two Categorical random variables X, Y that are independent. What is the correct way to sample the two distributions?
> I provide a short program that shows what I mean. 
> 
> Say we do the following experiments E1 and E2:
> ```python
> import jax.numpy as jnp
> import jax.random as jrandom
> import matplotlib.pyplot as plt
> 
> n = 10
> key_X = jrandom.PRNGKey(120)
> key_Y = jrandom.PRNGKey(845)
> logits_X = jrandom.uniform(key_X, shape=(n,))
> logits_Y = jrandom.uniform(key_Y, shape=(n,))
> 
> joint_logits = jnp.array([logits_X, logits_Y])
> 
> E1 = []
> E2 = []
> 
> key = jrandom.PRNGKey(0)
> n_s = 50
> for _ in range(n_s):
>     e1 = [jrandom.categorical(key, logits) for logits in joint_logits]
>     e2 = jrandom.categorical(key, joint_logits)
>     
>     E1.append(e1)
>     E2.append(e2)
>     key, subkey = jrandom.split(key)
> 
> E1 = jnp.array(E1)
> E2 = jnp.array(E2)
> 
> plt.hist(E2[:,0], bins=10*n)
> plt.hist(E1[:,0], bins=10*n)
> plt.grid()
> plt.title("Samples of X")
> plt.xlabel("Sample value of X")
> plt.ylabel("Frequency")
> plt.legend(["Experiment 1 (loop)", "Experiment 2 (stacked)"])
> plt.show()
> 
> plt.hist(E1[:,1], bins=10*n)
> plt.hist(E2[:,1], bins=10*n)
> plt.grid()
> plt.title("Samples of Y")
> plt.xlabel("Sample value of Y")
> plt.ylabel("Frequency")
> plt.legend(["Experiment 1 (loop)", "Experiment 2 (stacked)"])
> plt.show()
> ``````
> ![image](https://github.com/google/jax/assets/29761549/717894c1-5602-4c92-8d17-324d71d0ca02)
> ![image](https://github.com/google/jax/assets/29761549/10add5ca-8242-4bda-8b0f-f08f57b86713)
> 
> I looked at the code and I believe that the whole thing boils down to sampling a uniform distribution in two different ways:
> 1) Two times with the same key resulting in Experiment E1
> 2) Two times with a different key but these two keys are somehow connected resulting in Experiment E2.
> 
> 
> Based on what the docs say in [Pseudo Random Numbers in JAX](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) I understand that I need to manually split the key before sampling. Is the the process followed in Experiment 2 essentially doing the same thing under the hood and therefore is the right way to go?
> 
> I do realize that E1 is almost certainly wrong as the same key is used and therefore the two components in `E1` are dependent.
> 
> I appreciate any help.
> Thank you!
> 
> 
> 
> ### What jax/jaxlib version are you using?
> 
> jax 4.21, jaxlib 4.21, (0.4.21 0.4.21)
> 
> ### Which accelerator(s) are you using?
> 
> GPU
> 
> ### Additional system info?
> 
> Python : 3.10
> 
> ### NVIDIA GPU info
> 
> NVIDIA GeForce RTX 3070 Ti
> Driver Version: 535.129.03 
> CUDA Version: 12.2

---

### [mgrid with complex step generates inaccurate results](https://github.com/jax-ml/jax/issues/13782)

**Created:** 2022-12-24T05:44:52Z

**Tags:** `bug`

**Content:**

> ### Description
> 
> Using `jnp.mgrid` with negative `start` and complex `step` generates floats with large errors, e.g.:
> 
> ```python
> >>> import jax.numpy as jnp
> >>> jnp.mgrid[-4:-1:4j]
> Array([-4., -2.9999998, -2. , -1.], dtype=float32)
> >>> jnp.mgrid[-3:0:4j]
> Array([-3., -1.9999999 , -0.99999994,  0.], dtype=float32)
> ```
> 
> ### What jax/jaxlib version are you using?
> 
> jax v0.4.1, jaxlib v0.4.1
> 
> ### Which accelerator(s) are you using?
> 
> CPU
> 
> ### Additional system info
> 
> Python 3.10.8, macOS 12.6.1
> 
> ### NVIDIA GPU info
> 
> _No response_

---

### [jax.tree_util.Partial is not hash-stable in jax>=0.2.22](https://github.com/jax-ml/jax/issues/9429)

**Created:** 2022-02-03T13:54:25Z

**Tags:** `bug`

**Content:**

> Following #8101 , `jax.tree_util.Partial.func` is no more hash-stable:
> 
> jax==0.2.21
> ```python
> >>> import jax
> >>> from jax.tree_util import Partial
> >>> from functools import partial
> >>> fun = partial(print, 1)
> >>> hash(fun)
> 278539187
> >>> hash(Partial(fun, 2).func)
> 278539187
> >>> hash(Partial(fun, 2).func)
> 278539187
> ```
> 
> latest release
> ```python
> >>> from jax.tree_util import Partial
> >>> from functools import partial
> >>> fun = partial(print, 1)
> >>> hash(fun)
> 8749850128694
> >>> hash(Partial(fun, 2).func)
> 8750211514691
> >>> hash(Partial(fun, 2).func)
> 8750211514664
> 
> # instead, if the function wrapped is just a standard function, everything works fine
> >>> hash(Partial(print, 1).func)
> 56249262
> >>> hash(Partial(print, 1).func)
> 56249262
> ```
> 
> This is annoying because in some code in NetKet, we were wrapping user-passed functions into a `tree_util.Partial` in order to simplify some code, but now this triggers recompilation if the user's supplied function is wrapped in a partial or not.
> 
> cc @hawkinsp who committed the changes

---

### [libdevice not found ](https://github.com/jax-ml/jax/issues/14198)

**Created:** 2023-01-29T10:29:37Z

**Tags:** `bug needs info NVIDIA GPU`

**Content:**

> ### Description
> 
> (lambda x: args * jnp.power(2.0, x))(jnp.arange(4))
> 
> 
> ---------------------------------------------------------------------------
> UnfilteredStackTrace                      Traceback (most recent call last)
> [<ipython-input-47-384ff1e31327>](https://localhost:8080/#) in <module>
> ----> 1 (lambda x: args * jnp.power(2.0, x))(jnp.arange(4))
> 
> 14 frames
> UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: libdevice not found at /usr/local/lib/python3.8/dist-packages/jaxlib/cuda/nvvm/libdevice/libdevice.10.bc
> 
> The stack trace below excludes JAX-internal frames.
> The preceding is the original exception that occurred, unmodified.
> 
> --------------------
> 
> The above exception was the direct cause of the following exception:
> 
> XlaRuntimeError                           Traceback (most recent call last)
> [/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/ufuncs.py](https://localhost:8080/#) in power(x1, x2)
>     345       x1, = _promote_dtypes_numeric(x1)
>     346       return lax.integer_pow(x1, x2)
> --> 347   return _power(x1, x2)
>     348 
>     349 
> 
> XlaRuntimeError: INTERNAL: libdevice not found at /usr/local/lib/python3.8/dist-packages/jaxlib/cuda/nvvm/libdevice/libdevice.10.bc
> 
> ### What jax/jaxlib version are you using?
> 
> 0.4.2
> 
> ### Which accelerator(s) are you using?
> 
> gpu
> 
> ### Additional system info
> 
> _No response_
> 
> ### NVIDIA GPU info
> 
> _No response_

---

### [Not available for install for python 3.11](https://github.com/NVIDIA/TensorRT/issues/2744)

**Created:** 2023-03-08T05:26:04Z

**Tags:** `triaged`

**Content:**

> ## Description
> It seems like there's no build of tensorrt available for python 3.11 on pypi. pip grabs tensorrt 8.5.3 under python 3.10 and reports package not found under 3.11, I assume this is just a packaging omission
> 
> 
> ## Environment
> 
> **TensorRT Version**: N/A
> **NVIDIA GPU**: NVIDIA GeForce RTX 3060
> **NVIDIA Driver Version**: 530.30.02
> **CUDA Version**: 12.1
> **CUDNN Version**: 
> **Operating System**: Fedora Linux 37
> **Python Version (if applicable)**: 3.11/10
> **Tensorflow Version (if applicable)**: 
> **PyTorch Version (if applicable)**: 
> **Baremetal or Container (if so, version)**: 
> 
> 
> ## Steps To Reproduce
> Install both python 3.10 and 3.11
> 
> Attempt to `pip install tensorrt` with both.
> 
> 

---

### [TensorRT 10 Availability in NGC PyTorch Container](https://github.com/NVIDIA/TensorRT/issues/3796)

**Created:** 2024-04-12T09:32:56Z

**Tags:** `triaged`

**Content:**

> Hello,
> 
> TensorRT 10 has been rolled out and that's great news as it comes with a lot of nice features! However, this version is not available in NGC PyTorch :/
> I am reaching out to inquire about the plans for integrating TensorRT 10 into the NVIDIA GPU Cloud (NGC) PyTorch container.
> 
> Could you please provide any available information or timeline regarding when TensorRT 10 might be expected to be available in the NGC PyTorch container?
> 
> Thanks a lot !
> 
> 

---

### [project build fail on docker](https://github.com/NVIDIA/TensorRT/issues/2879)

**Created:** 2023-04-15T04:17:28Z

**Tags:** `triaged`

**Content:**

> # reproduce
> ```
> git clone -b main https://github.com/nvidia/TensorRT TensorRT
> cd TensorRT
> git submodule update --init --recursive
> ./docker/build.sh --file docker/ubuntu-20.04.Dockerfile --tag tensorrt-ubuntu20.04-cuda12.0
> ./docker/launch.sh --tag tensorrt-ubuntu20.04-cuda12.0 --gpus all
>  cd $TRT_OSSPATH
>  mkdir -p build && cd build
>  cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out
> make -j4
> cd ./out
> ./sample_onnx_mnist
> 
> ```
> 
> command `watch nvidia-smi` can see the cuda
> 
> 
> # Error
> ```
> ./sample_onnx_mnist
> &&&& RUNNING TensorRT.sample_onnx_mnist [TensorRT v8600] # ./sample_onnx_mnist
> [04/15/2023-04:37:46] [I] Building and running a GPU inference engine for Onnx MNIST
> [04/15/2023-04:37:46] [W] [TRT] Unable to determine GPU memory usage
> [04/15/2023-04:37:46] [W] [TRT] Unable to determine GPU memory usage
> [04/15/2023-04:37:46] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 5, GPU 0 (MiB)
> [04/15/2023-04:37:46] [W] [TRT] CUDA initialization failure with error: 35. Please check your CUDA installation:  http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
> &&&& FAILED TensorRT.sample_onnx_mnist [TensorRT v8600] # ./sample_onnx_mnist
> 
> ```
> 
> 
> 
> AND how to get data
> 
> ```
> Could not find mnist.onnx in data directories:
> 	data/mnist/
> 	data/samples/mnist/
> 
> ```

---

### [addFullyConnected will be  replaced by  addMatrixMultiply  + addElementWise?](https://github.com/NVIDIA/TensorRT/issues/2152)

**Created:** 2022-07-15T03:50:19Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> <!-- A clear and concise description of the bug or issue. -->
> 
> 
> ## Environment
> 
> **TensorRT Version**: 
> **NVIDIA GPU**: 
> **NVIDIA Driver Version**: 
> **CUDA Version**: 
> **CUDNN Version**: 
> **Operating System**: 
> **Python Version (if applicable)**: 
> **Tensorflow Version (if applicable)**: 
> **PyTorch Version (if applicable)**: 
> **Baremetal or Container (if so, version)**: 
> 
> 
> ## Relevant Files
> 
> <!-- Please include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive/Dropbox, etc.) -->
> 
> 
> ## Steps To Reproduce
> 
> <!-- 
>   Craft a minimal bug report following this guide - https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports
> 
>   Please include:
>   * Exact steps/commands to build your repro
>   * Exact steps/commands to run your repro
>   * Full traceback of errors encountered 
> -->
> 
> <img width="1132" alt="image" src="https://user-images.githubusercontent.com/39978853/179146538-cd130d0f-def6-41d4-a6d8-7d9c834001db.png">
> 

---

### [Error Code 1: Cask (Cask convolution execution) during inference with .to("cuda")](https://github.com/NVIDIA/TensorRT/issues/4335)

**Created:** 2025-01-22T12:40:42Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> I am encountering an issue when running inference using a TensorRT engine. Specifically, if I include to("cuda") in my PyTorch tensor operations, the inference results in errors and the outputs are filled with zeros. Here is the error message:
> 
> [TRT] [E] IExecutionContext::executeV2: Error Code 1: Cask (Cask convolution execution)
> tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
>           [0., 0., 0.,  ..., 0., 0., 0.],
>           [0., 0., 0.,  ..., 0., 0., 0.],
>           [0., 0., 0.,  ..., 0., 0., 0.]],
> 
>          [[0., 0., 0.,  ..., 0., 0., 0.],
>           [0., 0., 0.,  ..., 0., 0., 0.],
>           [0., 0., 0.,  ..., 0., 0., 0.],
>           [0., 0., 0.,  ..., 0., 0., 0.]]]])
> 
> If I remove to("cuda") during input tensor preparation, the issue does not occur, and the inference works as expected. I suspect this is related to how tensors are being transferred to the GPU.
> 
> 
> ## Environment
> 
> <!-- Please share any setup information you know. This will help us to understand and address your case. -->
> 
> **TensorRT Version**: 10.3
> 
> **NVIDIA GPU**:
> 
> **NVIDIA Driver Version**:
> 
> **CUDA Version**:  12.6.68
> 
> **CUDNN Version**: 9.3.0.75
> 
> 
> Operating System:
> 
> Python Version (if applicable): 3.10
> 
> Tensorflow Version (if applicable):
> 
> PyTorch Version (if applicable):
> 
> Baremetal or Container (if so, version):
> 
> 
> ## Relevant Files
> 
> <!-- Please include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive/Dropbox, etc.) -->
> 
> import pycuda.driver as cuda
> import pycuda.autoinit
> import time
> import numpy as np
> import tensorrt as trt
> 
> 
> 
> class TensorRTInferencer:
>     def __init__(self, engine_path, device):
>         """Initialize the TensorRT inferencer."""
>         # self.logger = trt.Logger(trt.Logger.WARNING)
>         self.logger = trt.Logger(trt.Logger.VERBOSE)
>         self.engine = self._load_engine(engine_path)
>         self.context = self.engine.create_execution_context()
>         self.device = device
>         print("TensorRT engine loaded and buffers allocated.")
> 
>     def _load_engine(self, engine_path):
>         """Load TensorRT engine from file."""
>         with open(engine_path, 'rb') as f, trt.Runtime(self.logger) as runtime:
>             return runtime.deserialize_cuda_engine(f.read())
> 
>     def _allocate_buffers(self):
>         """Allocate buffers for inputs and outputs."""
>         buffers = {}
>         stream = cuda.Stream()
> 
>         for i in range(self.engine.num_io_tensors):
>             name = self.engine.get_tensor_name(i)
>             mode = self.engine.get_tensor_mode(name)
>             dtype = trt.nptype(self.engine.get_tensor_dtype(name))
>             shape = self.context.get_tensor_shape(name)
>             # print(shape)
> 
>             # Allocate memory with placeholder size for dynamic inputs
>             size = trt.volume(shape) if -1 not in shape else 1
>             host_mem = cuda.pagelocked_empty(size, dtype)
>             device_mem = cuda.mem_alloc(host_mem.nbytes)
> 
>             buffers[name] = {
>                 'host': host_mem,
>                 'device': device_mem,
>                 'dtype': dtype,
>                 'mode': mode
>             }
> 
>         return buffers, stream
>     
>     def allocate_buffers(self, input_shapes):
>         buffers = {}
>         stream = cuda.Stream()
> 
>         for i in range(self.engine.num_io_tensors):
>             name = self.engine.get_tensor_name(i)
>             mode = self.engine.get_tensor_mode(name)
>             dtype = trt.nptype(self.engine.get_tensor_dtype(name))
> 
>             if mode == trt.TensorIOMode.INPUT:
>                 shape = input_shapes[name]
>             else:
>                 shape = self.context.get_tensor_shape(name)
>                 shape = tuple(input_shapes["input:0"][0] if dim == -1 else dim for dim in shape)
>             # print(shape)
> 
>             size = trt.volume(shape)
>             host_mem = cuda.pagelocked_empty(size, dtype)
>             device_mem = cuda.mem_alloc(host_mem.nbytes)
> 
>             buffers[name] = {
>                 'host': host_mem,
>                 'device': device_mem,
>                 'dtype': dtype,
>                 'mode': mode
>             }
> 
>         return buffers, stream
> 
> 
>     def infer(self, inputs):
>         """Run inference with provided inputs."""
>         # Set tensor addresses and copy inputs
>         for name, buffer in self.buffers.items():
>             self.context.set_tensor_address(name, int(buffer['device']))
>             if buffer['mode'] == trt.TensorIOMode.INPUT:
>                 self.context.set_input_shape(name, inputs[name].shape)
>                 np.copyto(buffer['host'], inputs[name].ravel())
>                 cuda.memcpy_htod_async(buffer['device'], buffer['host'], self.stream)
> 
>         # Execute inference
>         self.context.execute_async_v3(stream_handle=self.stream.handle)
> 
>         # Copy outputs back to host
>         results = {}
>         for name, buffer in self.buffers.items():
>             if buffer['mode'] == trt.TensorIOMode.OUTPUT:
>                 cuda.memcpy_dtoh_async(buffer['host'], buffer['device'], self.stream)
>                 results[name] = buffer['host']
>         self.stream.synchronize()
> 
>         return results
>     
>     def __call__(self, inputs):
>         inputs_dict = {}
>         for input in inputs:
>             # Set dynamic shapes
>             self.context.set_input_shape(input["name"], input["data"].shape)
> 
>             # Prepare inputs
>             inputs_dict[input["name"]] = input["data"].cpu().numpy().astype(np.float32)
> 
>         # Allocate buffers
>         self.buffers, self.stream = self._allocate_buffers()
> 
>         # Run inference
>         results = self.infer(inputs_dict)
> 
>         # Reshape outputs based on tensor shapes
>         # reshaped_outputs = {
>         #     name: results[name].reshape(self.context.get_tensor_shape(name))
>         #     for name, buffer in self.buffers.items() if buffer['mode'] == trt.TensorIOMode.OUTPUT
>         # }
>         reshaped_outputs = {
>             name: results[name].reshape(self.context.get_tensor_shape(name))
>             for name, buffer in self.buffers.items() if buffer['mode'] == trt.TensorIOMode.OUTPUT
>         }
>         output = torch.from_numpy([*reshaped_outputs.values()][0])
>         return output
> 
>     def get_tensor_info(self):
>         """Retrieve tensor information from the engine."""
>         tensor_info = []
>         for i in range(self.engine.num_io_tensors):
>             name = self.engine.get_tensor_name(i)
>             mode = "Input" if self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT else "Output"
>             shape = self.context.get_tensor_shape(name)
>             tensor_info.append({'name': name, 'mode': mode, 'shape': shape})
>         return tensor_info
> 
> 
> def speed_test(inferencer, bs=1, run_times=10):
>     tensor_info = inferencer.get_tensor_info()
>     inputs = list()
>     for info in tensor_info:
>         if info['mode'] == "Input":
>             inputs.append(dict(
>                 name=info['name'],
>                 shape=tuple(bs if dim == -1 else dim for dim in info["shape"]),
>                 data=None
>             ))
> 
>     total_time = 0
>     for i in range(run_times+1):
>         for input in inputs:
>             input["data"] = torch.tensor(np.random.randn(*input["shape"]), dtype=torch.float32).to("cuda")
>         t1 = time.time()
>         outputs = inferencer(inputs)
>         
>         if i > 0:
>             print(outputs)
>             total_time += time.time() - t1
> 
>     # for name, output in outputs.items():
>     #     print(f"Output {name}: {output.shape}")
>     #     # print(output)
>     print(f"Average time over {run_times} runs: {total_time / run_times:.3f}s")
> 
> def show_info(inferencer):
>     # Print tensor information
>     tensor_info = inferencer.get_tensor_info()
>     print("\nTensor information:")
>     for info in tensor_info:
>         print(f"Tensor: {info['name']}, Mode: {info['mode']}, Shape: {info['shape']}")
> 
> if __name__ == "__main__":
>     import torch
> 
>     # Initialize the TensorRT inferencer
>     engine_path = (
>         "241203_symmetry_WS&GN_3s.trt"
>         )
>     inferencer = TensorRTInferencer(engine_path, "cuda:0")
> 
>     # show_info(inferencer)
>     speed_test(inferencer, bs=1, run_times=1)
> 
> 
> 
> 

---

### [Low performance when running on GPU in loops](https://github.com/NVIDIA/TensorRT/issues/3083)

**Created:** 2023-06-25T11:14:48Z

**Tags:** `triaged`

**Content:**

> ## Description
> I'm trying to infer ONNX model on A30, when using ONNXRUNTIME/TRT for inference, the single call time is stable at 8ms, but when called multiple times in a loop, the time jitter from 8ms to 80ms. 
> > The model is based on GPT-2, and it uses the output of the previous call as the input of the next call.
> 
> Using IOBinding in ONNX can solve the latency jitter. Does it has anything to do with data copying? Are there similar solutions available in TensorRT?
> ```
> **// case 1. origin code via ONNX**
> def infer(input_ids, past_key_values):
>    sess = onnxruntime.InferenceSession(export_onnx_file, sess_options,
>                                     providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
>    while True:
>         stime = time.time()
>         model_inputs = prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)
>         outputs = sess.run(
>              None,
>              {
>                  'input_ids': model_inputs['input_ids'],
>                  'past_key_values_in': model_inputs['past_key_values'],
>              }
>         )
> 
>         // use output as input in next infer
>         next_token_logits = torch.tensor(outputs[0][:, -1, :]).cuda()
>         past_key_values = outputs[1]
>         past_key_values = torch.cat([past_key_values[:, :, :, :, :1], next_token_logits], dim=-1)
>         etime = time.time()
>         print('\tgenerate', etime - stime)
> 
>         if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
>             break
>      
>     return input_ids
> 
> // the infer time
>         generate 10ms
>         generate 88ms
>         generate 10ms
>         generate 80ms
>         generate 10ms
>         generate 89ms
> 
> 
> **// case 2. ONNX with IOBINDING**
> def infer(input_ids, past_key_values):
>    while True:
>         stime = time.time()
>         model_inputs = prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)
>         binding = sess.io_binding()
>         for k in ['input_ids', 'past_key_values']:
>             binding.bind_input(
>                     name='past_key_values_in' if k == 'past_key_values' else k,
>                     device_type='cuda',
>                     device_id=0,
>                     element_type=np.float32 if k == 'past_key_values' else np.int64,
>                     shape=tuple(model_inputs[k].shape),
>                     buffer_ptr=model_inputs[k].data_ptr()
>             )
>         logits_tensor = torch.empty((1, model_inputs['input_ids'].shape[1], 49280), dtype=torch.float, device='cuda')
>         binding.bind_output(
>             name='logits',
>             device_type='cuda',
>             device_id=0,
>             element_type=np.float32,
>             shape=tuple(logits_tensor.shape),
>             buffer_ptr=logits_tensor.data_ptr(),
>         )
>         past_key_values_out_tensor = torch.empty((24, 2, 1, 128, model_inputs['past_key_values'].shape[-1] + model_inputs['input_ids'].shape[1] - 1), dtype=torch.float, device='cuda')
>         binding.bind_output(
>             name='past_key_values_out',
>             device_type='cuda',
>             device_id=0,
>             element_type=np.float32,
>             shape=tuple(past_key_values_out_tensor.shape),
>             buffer_ptr=past_key_values_out_tensor.data_ptr(),
>         )
>         
>         sess.run_with_iobinding(binding)
>         next_token_logits = torch.tensor(outputs[0][:, -1, :]).cuda()
>         past_key_values = outputs[1]
>         past_key_values = torch.cat([past_key_values[:, :, :, :, :1], next_token_logits], dim=-1)
>         etime = time.time()
>         print('\tgenerate', etime - stime)
> 
>         if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
>             break
>      
>     return input_ids
> 
> // the infer time, it's almost same
>         generate 10ms
>         generate 11ms
>         generate 10ms
>         ...
> 
> 
> **// case 3. origin code via TRT**
> class TrtInst:
>     def __init__(self, engine_file):
>         self.engine = load_engine(engine_file)
>         self.context = self.engine.create_execution_context()
>         
>     def activate_impl(self):
>         def make_buffers_legacy():
>             """
>             Creates empty host and device buffers for the specified engine.
>             Always uses binding names from Profile 0.
>             """
>             device_buffers = OrderedDict()
>             host_output_buffers = OrderedDict()
> 
>             for idx in range(trt_util.get_bindings_per_profile(self.engine)):
>                 binding = self.engine[idx]
>                 dtype = trt_util.np_dtype_from_trt(self.engine.get_binding_dtype(binding))
>                 device_buffers[binding] = cuda.DeviceArray(dtype=dtype)
>                 if not self.engine.binding_is_input(binding):
>                     host_output_buffers[binding] = np.empty(shape=tuple(), dtype=dtype)
> 
>             print(f"Initialized device buffers: {device_buffers}")
>             return device_buffers, host_output_buffers, None
>         
>         self.device_buffers, self.host_output_buffers, self.output_allocator = (
>             make_buffers_legacy()
>         )
>         self.stream = cuda.Stream()
>     
> 
>     
>     def _set_shapes_from_feed_dict_legacy(self, feed_dict):
>         """
>         Sets context shapes according to the provided feed_dict.
> 
>         Note that ``infer()`` will call this function automatically, and hence
>         you should only use it if you plan to use this runner's context manually.
> 
>         Args:
>             feed_dict (OrderedDict[str, numpy.ndarray]):
>                     A mapping of input tensor names to corresponding input NumPy arrays.
> 
>         Returns:
>             Tuple[int, int]: The start and end binding indices of the modified bindings.
>         """
> 
>         def is_dynamic_shape_input(binding):
>             return self.engine.is_shape_binding(binding) and self.engine.binding_is_input(binding)
> 
>         start_binding, end_binding = get_active_profile_bindings(self.context)
>         for name, inp in feed_dict.items():
>             binding = start_binding + self.engine[name]
>             # Only set shapes if required.
>             # get_shape/get_binding_shape will return what a shape input/data input is currently set to.
>             if is_dynamic_shape_input(binding):  # For input shape tensors
>                 if isinstance(inp, cuda.DeviceView):
>                     print(
>                         f"A DeviceView was provided for input: {name}, but since this is a shape tensor, "
>                         "it must reside in host memory. Please use a NumPy array instead. "
>                     )
> 
>                 if tuple(self.context.get_shape(binding)) != tuple(inp):
>                     print(f"Setting shape binding: {name} (index: {binding}) to: {inp}")
>                     if not self.context.set_shape_input(binding, inp):
>                         print(
>                             f"Failed to set shape binding: {name} (index: {binding}) to: {inp}. "
>                             "Are these values valid for the binding?"
>                         )
> 
>             elif is_shape_dynamic(self.engine.get_binding_shape(binding)):
>                 shape = inp.shape
>                 if tuple(self.context.get_binding_shape(binding)) != tuple(shape):
>                     print(f"Setting binding: {name} (index: {binding}) to shape: {shape}")
>                     if not self.context.set_binding_shape(binding, shape):
>                         print(
>                             f"Failed to set binding: {name} (index: {binding}) to shape: {shape}. "
>                             "Is this shape valid for the binding?"
>                         )
> 
>         if not self.context.all_binding_shapes_specified:
>             print(
>                 f"Some input shapes were not specified.\nNote: Network inputs are: {self.get_input_metadata()}"
>             )
>         if not self.context.all_shape_inputs_specified:
>             print(
>                 f"Some shape inputs were not specified.\nNote: Network inputs are: {self.get_input_metadata()}"
>             )
> 
>         return start_binding, end_binding
>         
>     def _infer_impl_legacy(self, feed_dict, copy_outputs_to_host=False):
>         start_binding, end_binding = self._set_shapes_from_feed_dict_legacy(feed_dict)
> 
>         # Resize output device buffers - host buffers will be automatically resized by copy_to
>         for binding in range(start_binding, end_binding):
>             if not self.engine.binding_is_input(binding):
>                 name = self.engine[binding - start_binding]  # Use profile 0 binding names for all buffers.
>                 shape = tuple(self.context.get_binding_shape(binding))
>                 self.device_buffers[name].resize(shape)
> 
>         # Use a shallow copy in case we need to replace our allocated buffers with provided DeviceViews.
>         self.dev_bufs = copy.copy(self.device_buffers)
>         for name, buffer in feed_dict.items():
>             if isinstance(buffer, cuda.DeviceView):
>                 self.dev_bufs[name] = buffer
>             elif isinstance(buffer, np.ndarray):
>                 self.dev_bufs[name].resize(buffer.shape)
>                 buffer = util.make_contiguous(buffer)
>                 self.dev_bufs[name].copy_from(buffer, self.stream)
>             else:
>                 print(
>                     f"For input: {name}, unrecognized type in feed_dict: {type(buffer).__name__}.\n"
>                     "Please provide either a NumPy array or Polygraphy DeviceView. "
>                 )
> 
>         # Need to offset bindings in case the active profile is not 0.
>         self.bindings = [0] * start_binding + [buf.ptr for buf in self.dev_bufs.values()]
>         
>     def predict(self, copy_outputs_to_host=False):
>         ts_start = time.time()
>         
>         success = self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.ptr)
>         if not success:
>             G_LOGGER.critical("Model execution failed. Please see the log messages above for details")
> 
>         ts_infer_end = time.time()
>         output_buffers = OrderedDict()
>         for name, buffer in self.host_output_buffers.items():
>             if copy_outputs_to_host:
>                 self.host_output_buffers[name] = util.resize_buffer(buffer, self.dev_bufs[name].shape)
>                 self.dev_bufs[name].copy_to(self.host_output_buffers[name], self.stream)
>                 output_buffers[name] = self.host_output_buffers[name]
>             else:
>                 output_buffers[name] = self.dev_bufs[name].view()
> 
>         self.stream.synchronize()
>         ts_sync_end = time.time()
>         print('\tgenerate', etime - stime)
>         return output_buffers
> 
> def infer(input_ids, past_key_values):
>    trt_engine = TrtInst("xxx.engine")
>    trt_engine.activate_impl()
>    while True:
>         stime = time.time()
>         model_inputs = prepare_inputs_for_generation(input_ids, past_key_values=past_key_values)
>         trt_engine._infer_impl_legacy(model_inputs)
>         outputs = trt_engine.predict()
> 
>         next_token_logits = torch.Tensor(outputs['logits'].numpy())[:, -1, :]  
>         past_key_values = outputs['past_key_values_out']
>         past_key_values = torch.cat([past_key_values[:, :, :, :, :1], next_token_logits], dim=-1)
>         etime = time.time()
>         print('\tgenerate', etime - stime)
> 
>         if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
>             break
>      
>     return input_ids
> 
> // the infer time
>         generate   8ms
>         generate   88ms
>         generate   7ms
>         generate   90ms
>         generate   7ms
>         generate   89ms
>         ...
> ```
> 
> 
> ## Environment
> 
> <!-- Please share any setup information you know. This will help us to understand and address your case. -->
> 
> **TensorRT Version**: 8.6
> 
> **NVIDIA GPU**: A30
> 
> **NVIDIA Driver Version**: 515.65.01
> 
> **CUDA Version**: 11.7
> 
> **CUDNN Version**:
> 
> 
> Operating System: ubuntu-20.04.1
> 
> Python Version (if applicable): 3.9
> 
> 
> 

---

### [mAPs of TensorRT engine are higher than mAPs of TF ckpt](https://github.com/NVIDIA/TensorRT/issues/2568)

**Created:** 2022-12-28T08:36:44Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> I compared mAPs between TF ckpt and TensorRT engine (FP32).
> But I can't figure out why mAPs of the TensorRT engine are higher.
> Why TRT engine is better than TF ckpt?
> 
> | nms score thresh | 0.01  | 0.2   | 0.4   | 0.6   | 0.8   | 1.0 |
> |-----------------|-------|-------|-------|-------|-------|-----|
> | TensorFlow ckpt (FP32)            | 0.401 | 0.378 | 0.328 | 0.249 | 0.129 | 0.0 |
> | TensorRT engine (FP32)            | 0.4 | 0.381 |0.334  |0.268  |0.195  |0.0  |
> 
> - Model: **efficientdet-d1**
> - Dataset: **COCO 2017** (val2017)
> - I changed nms score thresholds of TF ckpt using argument `hparams="nms_configs.score_thresh=0.01"` in [here](https://github.com/google/automl/tree/master/efficientdet#7-eval-on-coco-2017-val-or-test-dev).
> - I made different nms score thresholds of ONNX models using argument `--nms_threshold [...]`. I checked each onnx models have different nms score thresholds by using Netron. And then, I converted each onnx models to TRT engines with precision FP32 using below commands.
>   ```
>   python3 build_engine.py \
>     --onnx /path/to/model_${THRESHOLD}.onnx \
>     --engine /path/to/engine_${THRESHOLD}.trt \
>     --precision fp32
>   ```
> - I wrote values from ` Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ]`
> 
> 
> ## Environment
> 
> **TensorRT Version**: 8.5.1.7
> 
> 
> 
> 

---

### [How to assign a torch tensor to inputs.device directly?](https://github.com/NVIDIA/TensorRT/issues/2456)

**Created:** 2022-11-04T03:44:18Z

**Tags:** `triaged`

**Content:**

> Hi, 
> I write following code to speed up the inference of transformer from PyTorch.
> ```
>     logger = trt.Logger(trt.Logger.ERROR)
>     trt.init_libnvinfer_plugins(logger, namespace="")
>     with open("./test.engine", 'rb') as fr, trt.Runtime(logger) as runtime:
>         engine = runtime.deserialize_cuda_engine(fr.read())
>     
>     inputs, outputs, bindings, stream = common.allocate_buffers(engine)
> 
>     ### input is a torch tensor on gpu
>     inputs[0].host = input.cpu().numpy()
>     [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]
> 
>     ### it failed for different type between torch.tensor and pycuda._driver.DeviceAllocation
>     # inputs[0].device = input
> 
>     context = engine.create_execution_context()
>     context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)
> 
>     cuda.memcpy_dtoh_async(outputs[0].host, outputs[0].device, stream)
>     # stream.synchronize()
>     output = torch.from_numpy(outputs[0].host).reshape((-1, 3))
> ```
> Evidently, I have a torch tensor on gpu as input. 
> To adapt the interface, I have to transfer it to cpu and assign it to inputs[0].host, and then it is transferred to gpu again, which is just so superfluous.
> So I'd like to ask how to assign it to inputs[0].device directly?

---

### [fp16 ouputs error of TensorRT 8.2.1.8 when running BERT](https://github.com/NVIDIA/TensorRT/issues/3041)

**Created:** 2023-06-06T03:36:59Z

**Tags:** `triaged`

**Content:**

> ## Description
> I tried to convert bert onnx model to trt engine. When using FP32, everything is fine. But when I using FP16 mode, the outputs are different from fp32 and onnx, here is the polygraphy outputs.
> 
> ```
> polygraphy run /data/project/debug.onnx --trt --fp16 --onnxrt --atol 0.01 --workspace 10G
> 
> 
> [I] onnxrt-runner-N0-06/06/23-11:22:29 
>     ---- Inference Input(s) ----
>     {input_ids [dtype=int64, shape=(1, 256)],
>      attention_mask [dtype=int64, shape=(1, 256)],
>      token_type_ids [dtype=int64, shape=(1, 256)]}
> [I] onnxrt-runner-N0-06/06/23-11:22:29 
>     ---- Inference Output(s) ----
>     {output [dtype=float32, shape=(1, 768)]}
> [I] onnxrt-runner-N0-06/06/23-11:22:29  | Completed 1 iteration(s) in 151.7 ms | Average inference time: 151.7 ms.
> [I] Accuracy Comparison | trt-runner-N0-06/06/23-11:22:29 vs. onnxrt-runner-N0-06/06/23-11:22:29
> [I]     Comparing Output: 'output' (dtype=float32, shape=(1, 768)) with 'output' (dtype=float32, shape=(1, 768))
> [I]         Tolerance: [abs=0.01, rel=1e-05] | Checking elemwise error
> [I]         trt-runner-N0-06/06/23-11:22:29: output | Stats: mean=-0.00094657, std-dev=0.7291, var=0.53158, median=-0.032455, min=-2.377 at (0, 703), max=2.9258 at (0, 294), avg-magnitude=0.58449
> [I]             ---- Histogram ----
>                 Bin Range        |  Num Elems | Visualization
>                 (-2.38 , -1.85 ) |          4 | 
>                 (-1.85 , -1.32 ) |         17 | ###
>                 (-1.32 , -0.786) |         88 | #################
>                 (-0.786, -0.255) |        182 | ###################################
>                 (-0.255, 0.275 ) |        206 | ########################################
>                 (0.275 , 0.805 ) |        165 | ################################
>                 (0.805 , 1.34  ) |         81 | ###############
>                 (1.34  , 1.87  ) |         21 | ####
>                 (1.87  , 2.4   ) |          2 | 
>                 (2.4   , 2.93  ) |          2 | 
> [I]         onnxrt-runner-N0-06/06/23-11:22:29: output | Stats: mean=-0.00083125, std-dev=0.72895, var=0.53137, median=-0.030038, min=-2.3729 at (0, 703), max=2.9271 at (0, 294), avg-magnitude=0.58444
> [I]             ---- Histogram ----
>                 Bin Range        |  Num Elems | Visualization
>                 (-2.38 , -1.85 ) |          4 | 
>                 (-1.85 , -1.32 ) |         16 | ###
>                 (-1.32 , -0.786) |         89 | #################
>                 (-0.786, -0.255) |        180 | ##################################
>                 (-0.255, 0.275 ) |        208 | ########################################
>                 (0.275 , 0.805 ) |        165 | ###############################
>                 (0.805 , 1.34  ) |         81 | ###############
>                 (1.34  , 1.87  ) |         21 | ####
>                 (1.87  , 2.4   ) |          2 | 
>                 (2.4   , 2.93  ) |          2 | 
> [I]         Error Metrics: output
> [I]             Minimum Required Tolerance: elemwise error | [abs=0.010602] OR [rel=0.94295] (requirements may be lower if both abs/rel tolerances are set)
> [I]             Absolute Difference | Stats: mean=0.002353, std-dev=0.0017993, var=3.2376e-06, median=0.0019512, min=3.8147e-06 at (0, 63), max=0.010602 at (0, 603), avg-magnitude=0.002353
> [I]                 ---- Histogram ----
>                     Bin Range           |  Num Elems | Visualization
>                     (3.81e-06, 0.00106) |        222 | ########################################
>                     (0.00106 , 0.00212) |        189 | ##################################
>                     (0.00212 , 0.00318) |        145 | ##########################
>                     (0.00318 , 0.00424) |         91 | ################
>                     (0.00424 , 0.0053 ) |         65 | ###########
>                     (0.0053  , 0.00636) |         34 | ######
>                     (0.00636 , 0.00742) |         13 | ##
>                     (0.00742 , 0.00848) |          5 | 
>                     (0.00848 , 0.00954) |          1 | 
>                     (0.00954 , 0.0106 ) |          3 | 
> [I]             Relative Difference | Stats: mean=0.015485, std-dev=0.062056, var=0.003851, median=0.0040911, min=5.3182e-06 at (0, 63), max=0.94295 at (0, 468), avg-magnitude=0.015485
> [I]                 ---- Histogram ----
>                     Bin Range          |  Num Elems | Visualization
>                     (5.32e-06, 0.0943) |        749 | ########################################
>                     (0.0943  , 0.189 ) |         12 | 
>                     (0.189   , 0.283 ) |          2 | 
>                     (0.283   , 0.377 ) |          0 | 
>                     (0.377   , 0.471 ) |          1 | 
>                     (0.471   , 0.566 ) |          0 | 
>                     (0.566   , 0.66  ) |          1 | 
>                     (0.66    , 0.754 ) |          0 | 
>                     (0.754   , 0.849 ) |          2 | 
>                     (0.849   , 0.943 ) |          1 | 
> [E]         FAILED | Output: 'output' | Difference exceeds tolerance (rel=1e-05, abs=0.01)
> [E]     FAILED | Mismatched outputs: ['output']
> [E] Accuracy Summary | trt-runner-N0-06/06/23-11:22:29 vs. onnxrt-runner-N0-06/06/23-11:22:29 | Passed: 0/1 iterations | Pass Rate: 0.0%
> [E] FAILED | Runtime: 64.315s | Command: /data/miniconda3/bin/polygraphy run /data/project/debug.onnx --trt --fp16 --onnxrt --atol 0.01 --workspace 10G
> ```
> 
> I alse tries to set some layer to FP32 using following c++ code.  But when I set some layer precision to FP32, the inference speed become the same as not using FP16 mode. I think the bert model is common used, and should not have such a problem, did I miss something important ? By the way, the bert model I build from transformers==4.29.2, and the onnx is exported by torch.onnx.export.
> ```
>   Logger gLogger_ = Logger(nvinfer1::ILogger::Severity::kVERBOSE);
>   pbuilder_ = nvinfer1::createInferBuilder(gLogger_);
>   pnetwork_ = pbuilder_->createNetworkV2(1U << static_cast<int>(
>       nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));
>   config_ = pbuilder_->createBuilderConfig();
>   config_->setMaxWorkspaceSize(1 << 30);
> 
>   config_->setFlag(nvinfer1::BuilderFlag::kSTRICT_TYPES);
> 
>   nvinfer1::IOptimizationProfile* profile = pbuilder_->createOptimizationProfile();
>   for (decltype(input_names_.size()) i = 0; i < input_names_.size(); i++) {
>     nvinfer1::Dims2 min_dim = nvinfer1::Dims2(min_bs_, input_dims_[i][0]);
>     nvinfer1::Dims2 max_dim = nvinfer1::Dims2(max_bs_, input_dims_[i][0]);
>     nvinfer1::Dims2 opt_dim = nvinfer1::Dims2(opt_bs_, input_dims_[i][0]);
> 
>     profile->setDimensions(input_names_[i].c_str(), nvinfer1::OptProfileSelector::kMIN, min_dim);
>     profile->setDimensions(input_names_[i].c_str(), nvinfer1::OptProfileSelector::kOPT, opt_dim);
>     profile->setDimensions(input_names_[i].c_str(), nvinfer1::OptProfileSelector::kMAX, max_dim);
>   }
>   config_->addOptimizationProfile(profile);
> 
>   
> 
>   auto parser = nvonnxparser::createParser(*pnetwork_, gLogger_.getTRTLogger());
>   parser->parseFromFile(onnx_file_.c_str(), static_cast<int32_t>(gLogger_.getReportableSeverity()));
> 
>   std::vector<std::string> not_fp16_names = {"/bert/Constant_2_output_0", "(Unnamed Layer* 17) [Shuffle]"};
>  
>   for (int i = 0; i < pnetwork_->getNbLayers(); i++)
>   {
>     auto layer = pnetwork_->getLayer(i);
> 
>     for (auto& name : not_fp16_names) {
>         if (layer->getName() == name) {
>             std::cout << layer->getName() << " " << name << std::endl;
>             layer->setPrecision(nvinfer1::DataType::kFLOAT);
>             continue;
>         }
>     }   
>   }
> 
>   config_->setFlag(nvinfer1::BuilderFlag::kFP16); 
> 
>   engine_ = pbuilder_->buildEngineWithConfig(*pnetwork_, *config_);
>   context_ = engine_->createExecutionContext();
> 
> 
>   pbuilder_->setMaxBatchSize(max_bs_);
> 
>   nvinfer1::IHostMemory* serializedModel = pbuilder_->buildSerializedNetwork(*pnetwork_, *config_);
>   std::ofstream engineFile("debug.trt", std::ios::binary);
>   engineFile.write(static_cast<char*>(serializedModel->data()), serializedModel->size());
> ```
> 
> 
> ## Environment
> 
> <!-- Please share any setup information you know. This will help us to understand and address your case. -->
> 
> **TensorRT Version**: 8.2.1.8
> 
> **NVIDIA GPU**: t4
> 
> **NVIDIA Driver Version**: 450.102.04
> 
> **CUDA Version**: 11.0
> 
> 
> Operating System: centos7
> 
> 
> 

---

### [grpc multi threading with TensorRT](https://github.com/NVIDIA/TensorRT/issues/1819)

**Created:** 2022-02-28T17:10:29Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> <!-- A clear and concise description of the bug or issue. -->
> 
> 
> ## Environment
> 
> **TensorRT Version**: 8.2.3.0
> **NVIDIA GPU**: gtx 1080ti
> **NVIDIA Driver Version**: 470.103.01
> **CUDA Version**: 11.4
> **CUDNN Version**: 8.2
> **Operating System**: Linux 18.06
> **Python Version (if applicable)**: 3.8.0
> **Tensorflow Version (if applicable)**: 
> **PyTorch Version (if applicable)**: 1.10
> **Baremetal or Container (if so, version)**: 
> 
> 
> ## grpc server code
> 
>     server = grpc.server(
>         futures.ThreadPoolExecutor(),
>         options=[
>             ("grpc.max_send_message_length", -1),
>             ("grpc.max_receive_message_length", -1),
>             ("grpc.so_reuseport", 1),
>             ("grpc.use_local_subchannel_pool", 1),
>         ],
>     )
> 
> ## grpc stub init
> 
>     grpcObject(encoder=trt_model, decoder=decoder)
> 
> ## trt_model init code
> 
>     def __init__(self):
>           cuda_ctx = cuda.Device(0).make_context()
>           self.cuda_ctx = cuda_ctx
>           if self.cuda_ctx:
>               self.cuda_ctx.push() 
>           ...
> 
> 
> Hello.
> I'm using TensorRT via grpc.
> However, after setting max_worker in the multi-threading function of grpc, the following error occurs when requests come in from multiple clients.
> In case of max_worker=1, no error occurs. Can you help?
> 
> ## infer method
>     def infer(self, wav_path):
>     
>             input_signal = preprocess_stt(wav_path)
>     
>             if self.cuda_ctx:
>                 self.cuda_ctx.push()
>             self.context.set_binding_shape(0, input_signal.shape)
>     
>             assert self.context.all_binding_shapes_specified
>             h_output = cuda.pagelocked_empty(tuple(self.context.get_binding_shape(1)), dtype=np.float32)
>     
>             h_input_signal = cuda.register_host_memory(np.ascontiguousarray(to_numpy(input_signal)))
>             cuda.memcpy_htod_async(self.d_input, h_input_signal, self.stream)
>             self.context.execute_async(bindings=[int(self.d_input), int(self.d_output)], stream_handle=self.stream.handle)
>             cuda.memcpy_dtoh_async(h_output, self.d_output, self.stream)
>             self.stream.synchronize()
>     
>             if self.cuda_ctx:
>                 self.cuda_ctx.pop()
>             return h_output
> 
> ## error
> 
>     pycuda._driver.LogicError: cuMemHostAlloc failed: an illegal memory access was encountered
>     E0228 17:02:30.063214 140249774667520 _server.py:471] Exception iterating responses: cuMemHostAlloc failed: an illegal memory access was encountered
>     Traceback (most recent call last):
>       File "/usr/local/lib/python3.8/dist-packages/grpc/_server.py", line 461, in _take_response_from_response_iterator
>         return next(response_iterator), True
>       File "/data/grpc/stt_grpc/grpc_class/dummy_grpc_core.py", line 116, in getStream
>         stt_result = trt_inference(self.trt_model, 'aaa.wav', self.decoder)
>       File "/data/grpc/stt_grpc/stt_package/stt_func.py", line 525, in trt_inference
>         model_output = actor.infer('aaa.wav')
>       File "/data/grpc/stt_grpc/grpc_class/tensorrt_stt.py", line 153, in infer
>         h_output = cuda.pagelocked_empty(tuple(self.context.get_binding_shape(1)), dtype=np.float32)
>     pycuda._driver.LogicError: cuMemHostAlloc failed: an illegal memory access was encountered
> 

---

### [the softmax result is differnet with pytorch nn.softmax(-1)](https://github.com/NVIDIA/TensorRT/issues/1713)

**Created:** 2022-01-12T02:27:44Z

**Tags:** `triaged`

**Content:**

> 
> My input dim is 1 16 65 65,then I use the 	
> nvinfer1::ISoftMaxLayer *softmax = m_network->addSoftMax(*Layers[inputName]);
> softmax->setAxes(3);
> 
> but whatever the axe I use(0,1,2,3),I can not got the same result with pytorc

---

### [inference BUG](https://github.com/NVIDIA/TensorRT/issues/1588)

**Created:** 2021-11-04T02:06:30Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> <!-- A clear and concise description of the bug or issue. -->
> 
> 
> ## Environment
> 
> **TensorRT Version**: 8.2
> **NVIDIA GPU**: 
> **NVIDIA Driver Version**: 
> **CUDA Version**: 10.2
> **CUDNN Version**: 8.0
> **Operating System**: 
> **Python Version (if applicable)**: 3.6
> 
> **PyTorch Version (if applicable)**: 
> **Baremetal or Container (if so, version)**: 
> 
> 
> 
> when the inference is done, and inference results is right , but at last, a wired error occurs:
> 
> [11/04/2021-10:00:07] [TRT] [E] 1: [graphContext.h::~MyelinGraphContext::39] Error Code 1: Myelin (Error 709 destroying stream '0x24ec72b0'.)
> [11/04/2021-10:00:07] [TRT] [E] 1: [graphContext.h::~MyelinGraphContext::39] Error Code 1: Myelin (Error 709 destroying stream '0x254b4bf0'.)
> [11/04/2021-10:00:07] [TRT] [E] 1: [graphContext.h::~MyelinGraphContext::39] Error Code 1: Myelin (Error 709 destroying stream '0x25c949d0'.)
> [11/04/2021-10:00:07] [TRT] [E] 1: [graphContext.h::~MyelinGraphContext::39] Error Code 1: Myelin (Error 709 destroying stream '0x26324310'.)
> [11/04/2021-10:00:07] [TRT] [E] 1: [graphContext.h::~MyelinGraphContext::39] Error Code 1: Myelin (Error 709 destroying stream '0x26870720'.)
> [11/04/2021-10:00:07] [TRT] [E] 1: [defaultAllocator.cpp::deallocate::35] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [defaultAllocator.cpp::deallocate::35] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaStream::47] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [cudaResources.cpp::~ScopedCudaEvent::24] Error Code 1: Cuda Runtime (invalid device context)
> [11/04/2021-10:00:07] [TRT] [E] 1: [defaultAllocator.cpp::deallocate::35] Error Code 1: Cuda Runtime (invalid device context)
> @pbridger @aaronp24 @ttyio @lukeyeager
> 
> 

---

### [tao-converter3.22.05 engine deploy failure](https://github.com/NVIDIA/TensorRT/issues/4343)

**Created:** 2025-02-01T10:53:05Z

**Tags:** `Module:Engine Build triaged`

**Content:**

> ## Description
> 
> I tried to run tao-converter on this file from ngc https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet/files?version=deployable_v1.0  but it gives error
> 
> 
> ## Environment
> 
> **TensorRT Version**:8.2.5
> 
> **NVIDIA GPU**:GTX1650
> 
> **NVIDIA Driver Version**:470
> 
> **CUDA Version**:11.4
> 
> **CUDNN Version**:8.2
> 
> 
> Operating System:
> 
> Python Version: 3.8
> 
>  ./tao-converter -k $KEY  -e /home/osman/trt.engine -p points,1x204800x4,1x204800x4,1x204800x4 -p num_points,1,1,1 -t fp16 ../pointpillars_deployable.etlt 
> [INFO] [MemUsageChange] Init CUDA: CPU +337, GPU +0, now: CPU 348, GPU 202 (MiB)
> [INFO] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 348 MiB, GPU 202 MiB
> [INFO] [MemUsageSnapshot] End constructing builder kernel library: CPU 483 MiB, GPU 234 MiB
> [libprotobuf ERROR google/protobuf/text_format.cc:298] Error parsing text-format onnx2trt_onnx.ModelProto: 1:1: Interpreting non ascii codepoint 191.
> [libprotobuf ERROR google/protobuf/text_format.cc:298] Error parsing text-format onnx2trt_onnx.ModelProto: 1:1: Expected identifier, got: �
> [ERROR] ModelImporter.cpp:735: Failed to parse ONNX model from file: /tmp/file4IbqiP
> [ERROR] Failed to parse the model, please check the encoding key to make sure it's correct
> [ERROR] Number of optimization profiles does not match model input node number.
> Aborted (core dumped)
> 
> 
> 

---

### [PyTorch usage of INMSLayer on TensorRT](https://github.com/NVIDIA/TensorRT/issues/3807)

**Created:** 2024-04-19T18:33:23Z

**Tags:** `triaged`

**Content:**

> I need help implementing the INMSLayer and stop using the Efficient NMS Plugin. Do you have any implementation examples for this layer INMSLayer  in Python and PyTorch/Onnx?

---

### [error occurs when build engine from onnx to trt in tensorRT 8.0.3.4 with dynamic shape support and float16 datatype.](https://github.com/NVIDIA/TensorRT/issues/1644)

**Created:** 2021-11-29T10:40:29Z

**Tags:** `triaged`

**Content:**

> Description
> I try to build an engine that supports dynamic input shape input with tensorRT 8.0.3.4, when I set --fp16, error occurs and the error message is as follow. when I don't set --fp16, the script build the engine successfully.
> I also tried --fp16 without dynamic shape(set kMIN kMAX kOPT to be the same), the script can also build the engine successfully.
> so the bug occurs only when dynamic input shape and --fp16 are set.
> 
> jucic@dual2080B:~/my_code/sampleRAFT_trt8/build$ ./raft --fp16
> &&&& RUNNING TensorRT.sample_onnx_mnist [TensorRT v8003] # ./raft --fp16
> [11/29/2021-17:46:04] [I] Building and running a GPU inference engine for Onnx MNIST
> [11/29/2021-17:46:05] [I] [TRT] [MemUsageChange] Init CUDA: CPU +334, GPU +0, now: CPU 341, GPU 480 (MiB)
> [11/29/2021-17:46:05] [I] [TRT] ----------------------------------------------------------------
> [11/29/2021-17:46:05] [I] [TRT] Input filename: /home/jucic/my_code/RAFT_Opti/sampleRAFT/../raft_iter6_bilinear_dynamic.onnx
> [11/29/2021-17:46:05] [I] [TRT] ONNX IR version: 0.0.6
> [11/29/2021-17:46:05] [I] [TRT] Opset version: 12
> [11/29/2021-17:46:05] [I] [TRT] Producer name: pytorch
> [11/29/2021-17:46:05] [I] [TRT] Producer version: 1.7
> [11/29/2021-17:46:05] [I] [TRT] Domain:
> [11/29/2021-17:46:05] [I] [TRT] Model version: 0
> [11/29/2021-17:46:05] [I] [TRT] Doc string:
> [11/29/2021-17:46:05] [I] [TRT] ----------------------------------------------------------------
> [11/29/2021-17:46:06] [W] [TRT] onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> [11/29/2021-17:46:26] [W] [TRT] Output type must be INT32 for shape outputs
> mParams.fp16: 1 mParams.int8: 0
> [11/29/2021-17:46:26] [I] [TRT] [MemUsageSnapshot] Builder begin: CPU 357 MiB, GPU 480 MiB
> [11/29/2021-17:46:31] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +489, GPU +206, now: CPU 859, GPU 686 (MiB)
> [11/29/2021-17:46:32] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +399, GPU +172, now: CPU 1258, GPU 858 (MiB)
> [11/29/2021-17:46:32] [W] [TRT] Detected invalid timing cache, setup a local cache instead
> [11/29/2021-17:46:42] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.
> [11/29/2021-17:46:44] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1660, GPU 1000 (MiB)
> [11/29/2021-17:46:44] [E] [TRT] 2: [reformatRunner.cpp::getNbSpatialDims::635] Error Code 2: Internal Error (Assertion nbSpatialDimsOK(sr, dr, context) failed.)
> [11/29/2021-17:46:44] [E] [TRT] 2: [builder.cpp::buildSerializedNetwork::417] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed.)
> &&&& FAILED TensorRT.sample_onnx_mnist [TensorRT v8003] # ./raft --fp16
> 
> Environment
> TensorRT Version: 8.0.3.4
> ONNX-TensorRT Version / Branch: opset12
> GPU Type: 2080Ti
> Nvidia Driver Version: 460.73.01
> CUDA Version: 11.3
> CUDNN Version:
> Operating System + Version: ubuntu18.04
> Python Version (if applicable): 3.7
> TensorFlow + TF2ONNX Version (if applicable):
> PyTorch Version (if applicable): 1.6
> Baremetal or Container (if container which image + tag):
> 
> Relevant Files
> I write my code by following the official code below
> https://github.com/NVIDIA/TensorRT/blob/main/samples/sampleOnnxMNIST/sampleOnnxMNIST.cpp
> 
> and adding following lines to set dynamic input shape
> profile->setDimensions("img1", nvinfer1::OptProfileSelector::kMIN, nvinfer1::Dims4(1, 3,224,448));
> profile->setDimensions("img1", nvinfer1::OptProfileSelector::kOPT, nvinfer1::Dims4(1, 3,224,448));
> profile->setDimensions("img1", nvinfer1::OptProfileSelector::kMAX, nvinfer1::Dims4(1, 3,600,600));
> 
> profile->setDimensions("img2", nvinfer1::OptProfileSelector::kMIN, nvinfer1::Dims4(1, 3,224,448));
> profile->setDimensions("img2", nvinfer1::OptProfileSelector::kOPT, nvinfer1::Dims4(1, 3,224,448));
> profile->setDimensions("img2", nvinfer1::OptProfileSelector::kMAX, nvinfer1::Dims4(1, 3,600,600));
> 
> config->addOptimizationProfile(profile);
> Steps To Reproduce

---

### [python api documents error](https://github.com/NVIDIA/TensorRT/issues/2461)

**Created:** 2022-11-07T11:09:03Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> <!-- A clear and concise description of the bug or issue. -->
> 
> 
> ## Environment
> 
> **TensorRT Version**: 
> TensorRT 8.5.1.7
> 
> As [TRT python api](https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Graph/Layers.html#tensorrt.IGridSampleLayer) described , IGridSampleLayer has  variable named ` padding_mode`,but this variable is `sample_mode` rather than `padding_mode`. The detail as follow:
> 
> ```python
> dir(tensorrt.IGridSampleLayer)
> ['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'align_corners', 'get_input', 'get_output', 'get_output_type', 'interpolation_mode', 'name', 'num_inputs', 'num_outputs', 'output_type_is_set', 'precision', 'precision_is_set', 'reset_output_type', 'reset_precision', 'sample_mode', 'set_input', 'set_output_type', 'type']
> ```
> 
> 
> 

---

### [Valgrind reports memory leak in createInferBuilder()](https://github.com/NVIDIA/TensorRT/issues/2883)

**Created:** 2023-04-17T22:51:16Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> My application has an issue where Valgrind finds a lot of possible memory leaks in TensorRT's `nvinfer1::createInferBuilder()` function. My code instantiates the builder like this:
> 
> `std::unique_ptr<nvinfer1::IBuilder> builder{nvinfer1::createInferBuilder(gLogger)};`
> 
> So the builder should be freed when the unique_ptr goes out of scope. But when running Googletest, I get thousands of potential leaks in Valgrind which all seem to be related to `createInferBuilder()`:
> 
> ```
> ==16998== 152 bytes in 1 blocks are possibly lost in loss record 1,152 of 2,026
> ==16998==    at 0x4C31922: calloc (vg_replace_malloc.c:1340)
> ==16998==    by 0x2A9A9DFD: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A9AA17D: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A9AA337: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A9E2C4F: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AB4B261: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AB4BED2: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AA057D7: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AA1A5EC: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AA22B47: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AA34C90: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AA357BB: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A84EED2: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A84F155: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2AB81AB0: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A889B2D: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x2A980109: ??? (in /usr/lib64/libcuda.so.510.47.03)
> ==16998==    by 0x1CB3E73F: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1CB3FDCF: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1CB4086B: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1CB33EBB: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1CB2361D: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1CB53F2B: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1ABAC654: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1AB47E01: ??? (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x1AB515F9: createInferBuilder_INTERNAL (in /local/home/bnandano/brazil-pkg-cache/packages/TensorRT/TensorRT-8.5.3.x.4.0/AL2_x86_64/DEV.STD.PTHREAD/build/lib/libnvinfer.so.8.5.3)
> ==16998==    by 0x521A538: createInferBuilder (NvInfer.h:9677)
> ==16998==    by 0x521A538: MyFunction (my-source-file.cc:xxx)
> ==16998==    by 0x43BA09: MyTestSuite_TENSORRT_SanityCheck_Test::TestBody() (my-test-file.cc:xxx)
> ==16998==    by 0x46C6F9: HandleSehExceptionsInMethodIfSupported<testing::Test, void> (gtest.cc:2433)
> ==16998==    by 0x46C6F9: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (gtest.cc:2469)
> ==16998==    by 0x45CA2C: testing::Test::Run() (gtest.cc:2508)
> ==16998==    by 0x45CB97: testing::TestInfo::Run() (gtest.cc:2684)
> ==16998==    by 0x45CDE8: testing::TestSuite::Run() (gtest.cc:2816)
> ==16998==    by 0x462CEC: testing::internal::UnitTestImpl::RunAllTests() (gtest.cc:5338)
> ==16998==    by 0x462ED9: HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (gtest.cc:2433)
> ==16998==    by 0x462ED9: HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (gtest.cc:2469)
> ==16998==    by 0x462ED9: testing::UnitTest::Run() (gtest.cc:4925)
> ==16998==    by 0x40BE8A: RUN_ALL_TESTS (gtest.h:2473)
> ==16998==    by 0x40BE8A: main (gtest_main.cc:45)
> ```
> 
> In cmake I set up valgrind with the following arguments:
> 
> ```
> set(MEMORYCHECK_COMMAND_OPTIONS "--leak-check=full --num-callers=64 --track-origins=yes --gen-suppressions=all --error-exitcode=99")
> ```
> 
> I know Valgrind can sometimes turn up spurious leaks. Is that likely the case here?
> 
> ## Environment
> 
> **TensorRT Version**: 8.5
> **NVIDIA GPU**: Tesla V100
> **NVIDIA Driver Version**: 
> **CUDA Version**: 11.7
> **CUDNN Version**: 8.8
> **Operating System**: Linux (Amazon Linux 2)
> **Baremetal or Container (if so, version)**: Baremetal
> 
> 
> ## Relevant Files
> 
> <!-- Please include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive/Dropbox, etc.) -->
> 
> 
> ## Steps To Reproduce
> 
> This is the weird part: I can remove all other TensorRT code from my app except `createInferBuilder()` and I still have the memory leak, but when I create a C++ program which has nothing in it besides this function call (and instantiation of gLogger), I do not get the leak. I don't know why this function only shows a memory leak in a certain context, which makes me lean toward it being a spurious detection from valgrind.
> 

---

### [Erronious driver mismatch report](https://github.com/NVIDIA/TensorRT/issues/1786)

**Created:** 2022-02-08T16:35:58Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> Attempting to load an ONNX model with the TensorRT execution provider on the NVidia `TensorRT` docker image (from AWS marketplace, hosted on the NVidia Deep Learning AMI also from AWS marketplace) fails with:
> ```
> RuntimeError: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:122 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=0 ; hostname=8f2a5528ea84 ; expr=cudaSetDevice(device_id_);
> ```
> 
> 
> ## Environment
> 
> **TensorRT Version**: 8.2.2
> **NVIDIA GPU**: V100
> **NVIDIA Driver Version**: 460.73.01
> **CUDA Version**: 11.6
> **CUDNN Version**: 8.3.2
> **Operating System**: Ubuntu 20.04.3 LTS
> **Python Version (if applicable)**: 3.8.10
> **Tensorflow Version (if applicable)**: N/A
> **PyTorch Version (if applicable)**: N/A
> **Baremetal or Container (if so, version)**: NVidia TensorRT 22.01 (https://aws.amazon.com/marketplace/pp/prodview-jbbleexnwpstk)
> 
> 
> ## Relevant Files
> 
> Minimal ONNX model created on a CPU installation of pytorch using the script:
> 
> ```
> import torch
> import argparse
> 
> 
> class MinimalModel(torch.nn.Module):
>     def __init__(self):
>         super(MinimalModel, self).__init__()
>         self.linear = torch.nn.Linear(5, 2)
> 
>     def forward(self, x):
>         return self.linear(x)
> 
> 
> if __name__ == "__main__":
>     # Parse arguments
>     parser = argparse.ArgumentParser()
>     parser.add_argument('--file', type=str, help='Path to save ONNX model to')
>     parser.add_argument('--dummy')
>     args = parser.parse_args()
> 
>     model = MinimalModel()
> 
>     # Export to ONNX
>     x = torch.randn(5)
>     # traced_script_module = torch.jit.trace(self.model, waveform, strict=False, check_trace=True)
>     torch.onnx.export(model,                     # model being run
>                       x,                         # model input (or a tuple for multiple inputs)
>                       args.file,                 # where to save the model (can be a file or file-like object)
>                       export_params=True,        # store the trained parameter weights inside the model file
>                       opset_version=11,          # the ONNX version to export the model to
>                       do_constant_folding=True,  # whether to execute constant folding for optimization
>                       input_names = ['input'],   # the model's input names
>                       output_names = ['output']  # the model's output names
>                       )
> ```
> 
> Minimal test loader (that is run on the tensorRT environment as detailed below):
> 
> ```
> import os
> import argparse
> import onnxruntime
> 
> 
> def run(onnx):
>     if not os.path.exists(onnx):
>         raise ValueError("Specified model file not found")
> 
>     #  'TensorrtExecutionProvider',
>     ort_session = onnxruntime.InferenceSession(onnx,
>                                                providers=['TensorrtExecutionProvider'])
>     print(f"ONNX device: {onnxruntime.get_device()}")
>     print(f"Session providers: {ort_session.get_providers()}")
> 
> 
> if __name__ == "__main__":
>     # Parse arguments
>     parser = argparse.ArgumentParser()
>     parser.add_argument('--onnx', type=str, help='Optional path to save/restore ONNX to and run with ONNX runtime')
>     parser.add_argument('--dummy')
>     args = parser.parse_args()
> 
>     run(args.onnx)
> ```
> 
> ## Steps To Reproduce
> 
> 1. Build minimal ONNX model (see script above) to serve as test model (any ONNX model should do really).  I did this on my Mac locally and then copied it to a mounted volume on the test VM.  The env that built this should not be terribly relevant, but I kept it separate from the tensorRT VM so as not to have to install pytorch there.  For ref it was python 3.7.10 with torch 1.1
> 
> 2. Shell into TensorRT docker container.  The setup for this was an EC2 VM running he NVidia Deep Learning AMI (https://aws.amazon.com/marketplace/pp/prodview-e7zxdqduz4cbs?qid=1606075638508&sr=0-1&ref_=srh_res_product_title#pdp-overview), hosting docker with the container being the NVidia `TensorRT` container image from the AWS marketplace (https://aws.amazon.com/marketplace/pp/prodview-jbbleexnwpstk):
> ```
> docker run -it -v /ebs:/ebs 709825985650.dkr.ecr.us-east-1.amazonaws.com/nvidia/containers/nvidia/tensorrt:22.01-py3 bash
> ```
> Here `709825985650.dkr.ecr.us-east-1.amazonaws.com/nvidia/containers/nvidia/tensorrt:22.01-py3` is just my local copy of the TensorRT base image (no modifications) and `/ebs` is just an external mount containing the test runtime script (above) and the test model image (as generated by the script above)
> 
> 3. Install the ONNX runtime (ephemerally for the sake of a minimally intrusive test that doesn't require me to build a bespoke container on top of the base TensorRT image):
> ```
> pip install onnxruntime-gpu
> ```
> 
> 4.  Attempt to run the minimal loader script :
> ```
> python onnx_load_test.py /ebs/models/minimal_model.onnx
> ```
> Result is a runtime-driver mismatch error being reported:
> ```
> Traceback (most recent call last):
>   File "onnx_load_test.py", line 37, in <module>
>     run(args.onnx)
>   File "onnx_load_test.py", line 14, in run
>     ort_session = onnxruntime.InferenceSession(onnx,
>   File "/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 335, in __init__
>     self._create_inference_session(providers, provider_options, disabled_optimizers)
>   File "/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py", line 379, in _create_inference_session
>     sess.initialize_session(providers, provider_options, disabled_optimizers)
> RuntimeError: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:122 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:116 bool onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*) [with ERRTYPE = cudaError; bool THRW = true] CUDA failure 35: CUDA driver version is insufficient for CUDA runtime version ; GPU=0 ; hostname=21b052b97708 ; expr=cudaSetDevice(device_id_);
> ```
> 
> Since its driver version `460.73.01` and CUDA `11.6`, which requires driver `>=450.80.02` this seems like it should not occur...
> 

---

### [[bug] Issue to build an engine with TRT 8.6.1 with unsupported 'Sign' node](https://github.com/NVIDIA/TensorRT/issues/3124)

**Created:** 2023-07-12T08:31:23Z

**Tags:** `triaged`

**Content:**

> ## Description
> 
> I cannot build an engine with TensorRT Python 8.6.1 because of an unsupported `Sign` node. Nevertheless, according to the supported ONNX operators, the op `Sign` is available in `onnx-tensorrt` since v 8.2. I have seen all the supported ops here : https://github.com/onnx/onnx-tensorrt/blob/8.6-GA/docs/operators.md
> 
> Also this op exists in ONNX since the opset 13 : https://onnx.ai/onnx/operators/onnx__Sign.html
> 
> Hence I'm asking if this is an actual bug or I did something wrong.
> 
> ## Environment
> 
> <!-- Please share any setup information you know. This will help us to understand and address your case. -->
> 
> **TensorRT Version**: 8.6.1
> 
> **NVIDIA GPU**: RTX 3080 mobile
> 
> **NVIDIA Driver Version**: 536.40
> 
> **CUDA Version**: 11.8
> 
> **CUDNN Version**: not installed
> 
> 
> Operating System: Linux Ubuntu 22.04.2 LTS
> 
> Python Version (if applicable): 3.10.11
> 
> Tensorflow Version (if applicable): not installed
> 
> PyTorch Version (if applicable): 2.0.1
> 
> Baremetal or Container (if so, version): none
> 
> ## Steps To Reproduce
> 
> I have run the following `pip install` commands:
> ```
> pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118
> pip install tensorrt
> pip install transformers
> ```
> 
> The installed version I have are:
> * torch==2.0.1
> * transformers==4.30.2
> * tensorrt==8.6.1
> 
> Here the Python script to run:
> ```python
> from transformers import DebertaV2ForSequenceClassification
> import tensorrt as trt
> import torch
> 
> 
> batch_size = 1
> seq_len = 12
> deberta_model = DebertaV2ForSequenceClassification.from_pretrained("microsoft/mdeberta-v3-base")
> vocab_size = deberta_model.config.vocab_size
> 
> deberta_model.eval()
> 
> input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), dtype=torch.long)
> attention_mask = torch.randint(0, 2, (batch_size, seq_len), dtype=torch.long)
> input_names = ['input_ids', 'attention_mask']
> output_names = ['output']
> dynamic_axes={'input_ids'   : {0 : 'batch_size'},
>               'attention_mask'   : {0 : 'batch_size'},
>               'output' : {0 : 'batch_size'}}
> 
> torch.onnx.export(deberta_model,
>                   (input_ids, attention_mask),
>                   "model.onnx",
>                   export_params=True,
>                   opset_version=13,
>                   do_constant_folding=True,
>                   input_names = input_names,
>                   output_names = output_names,
>                   dynamic_axes = dynamic_axes
>                  )
> 
> TRT_LOGGER = trt.Logger(trt.Logger.INFO)
> TRT_BUILDER = trt.Builder(TRT_LOGGER)
> network = TRT_BUILDER.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
> onnx_parser = trt.OnnxParser(network, TRT_LOGGER)
> parse_success = onnx_parser.parse_from_file("model.onnx")
> 
> for idx in range(onnx_parser.num_errors):
>     print(onnx_parser.get_error(idx))
> ```
> 
> The output logs are:
> ```
> Some weights of the model checkpoint at microsoft/mdeberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.word_embeddings._weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']
> - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
> - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
> Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias']
> You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:560: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
>   torch.tensor(mid - 1).type_as(relative_pos),
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:564: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
>   torch.ceil(torch.log(abs_pos / mid) / torch.log(torch.tensor((max_position - 1) / mid)) * (mid - 1)) + mid
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:723: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
>   scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:723: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
>   scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:802: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
>   scale = torch.sqrt(torch.tensor(pos_key_layer.size(-1), dtype=torch.float) * scale_factor)
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:802: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
>   scale = torch.sqrt(torch.tensor(pos_key_layer.size(-1), dtype=torch.float) * scale_factor)
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:814: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
>   scale = torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:814: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
>   scale = torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:815: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
>   if key_layer.size(-2) != query_layer.size(-2):
> /home/jplu/miniconda3/envs/work/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:112: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.
>   output = input.masked_fill(rmask, torch.tensor(torch.finfo(input.dtype).min))
> ============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============
> verbose: False, log level: Level.ERROR
> ======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================
> 
> [07/12/2023-10:13:28] [TRT] [I] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 1267, GPU 1116 (MiB)
> [07/12/2023-10:13:36] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1444, GPU +268, now: CPU 2788, GPU 1384 (MiB)
> [libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
> [libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1116186214
> [07/12/2023-10:13:36] [TRT] [I] ----------------------------------------------------------------
> [07/12/2023-10:13:36] [TRT] [I] Input filename:   model.onnx
> [07/12/2023-10:13:36] [TRT] [I] ONNX IR version:  0.0.7
> [07/12/2023-10:13:36] [TRT] [I] Opset version:    13
> [07/12/2023-10:13:36] [TRT] [I] Producer name:    pytorch
> [07/12/2023-10:13:36] [TRT] [I] Producer version: 2.0.1
> [07/12/2023-10:13:36] [TRT] [I] Domain:
> [07/12/2023-10:13:36] [TRT] [I] Model version:    0
> [07/12/2023-10:13:36] [TRT] [I] Doc string:
> [07/12/2023-10:13:36] [TRT] [I] ----------------------------------------------------------------
> [libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.
> [libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1116186214
> [07/12/2023-10:13:37] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
> [07/12/2023-10:13:37] [TRT] [E] ModelImporter.cpp:771: While parsing node number 66 [Sign -> "/deberta/encoder/Sign_output_0"]:
> [07/12/2023-10:13:37] [TRT] [E] ModelImporter.cpp:772: --- Begin node ---
> [07/12/2023-10:13:37] [TRT] [E] ModelImporter.cpp:773: input: "/deberta/encoder/Sub_output_0"
> output: "/deberta/encoder/Sign_output_0"
> name: "/deberta/encoder/Sign"
> op_type: "Sign"
> 
> [07/12/2023-10:13:37] [TRT] [E] ModelImporter.cpp:774: --- End node ---
> [07/12/2023-10:13:37] [TRT] [E] ModelImporter.cpp:777: ERROR: onnx2trt_utils.cpp:1779 In function unaryHelper:
> [8] Assertion failed: validUnaryType && "This version of TensorRT does not support the given operator with the given input data type."
> In node 66 (unaryHelper): UNSUPPORTED_NODE: Assertion failed: validUnaryType && "This version of TensorRT does not support the given operator with the given input data type."
> ```
> 
> Thanks in advance for any help you can provide.

---

### [Trt ouput mismatch with onnx output.](https://github.com/NVIDIA/TensorRT/issues/4361)

**Created:** 2025-02-20T14:58:24Z

**Tags:** `triaged Module:Accuracy`

**Content:**

> ## Description
> Trt ouput mismatch with onnx output. Use polygraph run will failed with "Difference exceeds tolerance".
> 
> Onnx file: https://pan.baidu.com/s/1qd3NSrqIU-aJ4ZrHxO97Ag?pwd=43rg 提取码: 43rg
> 
> Polygraph cmd:
> ```
> polygraphy run /tmp/Janus-Pro-7B/vision_encoder_bfp16.onnx --onnxrt --trt \
>                                                                        --save-engine=/tmp/Janus-Pro-7B/vision_encoder_bfp16.trt \
>                                                                        --trt-min-shapes 'input:[1,3,384,384]' \
>                                                                        --trt-opt-shapes 'input:[1,3,384,384]' \
>                                                                        --trt-max-shapes 'input:[8,3,384,384]' \
>                                                                        --input-shapes   'input:[-1,3,384,384]' \
>                                                                        --atol 1e-1 --rtol 1e-1 \
>                                                                        --fail-fast
> ```
> Failed log:
> ```
> [I] trt-runner-N0-02/20/25-22:26:11     | Completed 1 iteration(s) in 1723 ms | Average inference time: 1723 ms.
> [I] Accuracy Comparison | onnxrt-runner-N0-02/20/25-22:26:11 vs. trt-runner-N0-02/20/25-22:26:11
> [I]     Comparing Output: 'output' (dtype=float16, shape=(1, 576, 4096)) with 'output' (dtype=float16, shape=(1, 576, 4096))
> [I]         Tolerance: [abs=0.1, rel=0.1] | Checking elemwise error
> [I]         onnxrt-runner-N0-02/20/25-22:26:11: output | Stats: mean=-0.035358, std-dev=4.1854, var=17.517, median=-0.0059319, min=-303.75 at (0, 121, 2526), max=102.44 at (0, 121, 411), avg-magnitude=2.5026
> [I]             ---- Histogram ----
>                 Bin Range      |  Num Elems | Visualization
>                 (-304 , -263 ) |          2 |
>                 (-263 , -222 ) |          1 |
>                 (-222 , -182 ) |          1 |
>                 (-182 , -141 ) |          1 |
>                 (-141 , -101 ) |         13 |
>                 (-101 , -60  ) |        107 |
>                 (-60  , -19.4) |       5218 |
>                 (-19.4, 21.2 ) |    2350202 | ########################################
>                 (21.2 , 61.8 ) |       3692 |
>                 (61.8 , 102  ) |         59 |
> [I]         trt-runner-N0-02/20/25-22:26:11: output | Stats: mean=-0.015617, std-dev=1.5098, var=2.2795, median=-0.0024776, min=-94.25 at (0, 121, 2526), max=43.531 at (0, 121, 3649), avg-magnitude=0.99653
> [I]             ---- Histogram ----
>                 Bin Range      |  Num Elems | Visualization
>                 (-304 , -263 ) |          0 |
>                 (-263 , -222 ) |          0 |
>                 (-222 , -182 ) |          0 |
>                 (-182 , -141 ) |          0 |
>                 (-141 , -101 ) |          0 |
>                 (-101 , -60  ) |          1 |
>                 (-60  , -19.4) |         79 |
>                 (-19.4, 21.2 ) |    2359190 | ########################################
>                 (21.2 , 61.8 ) |         26 |
>                 (61.8 , 102  ) |          0 |
> [I]         Error Metrics: output
> [I]             Minimum Required Tolerance: elemwise error | [abs=251.31] OR [rel=3.9922e+06] (requirements may be lower if both abs/rel tolerances are set)
> [I]             Absolute Difference | Stats: mean=2.4379, std-dev=3.0746, var=9.453, median=1.4648, min=0 at (0, 0, 3844), max=251.31 at (0, 77, 2526), avg-magnitude=2.4379
> [I]                 ---- Histogram ----
>                     Bin Range    |  Num Elems | Visualization
>                     (0   , 25.1) |    2355495 | ########################################
>                     (25.1, 50.3) |       3576 |
>                     (50.3, 75.4) |        194 |
>                     (75.4, 101 ) |         21 |
>                     (101 , 126 ) |          4 |
>                     (126 , 151 ) |          1 |
>                     (151 , 176 ) |          1 |
>                     (176 , 201 ) |          1 |
>                     (201 , 226 ) |          2 |
>                     (226 , 251 ) |          1 |
> [I]             Relative Difference | Stats: mean=28.596, std-dev=3638.5, var=1.3238e+07, median=1.9318, min=0 at (0, 0, 3844), max=3.9922e+06 at (0, 66, 2211), avg-magnitude=28.596
> [I]                 ---- Histogram ----
>                     Bin Range            |  Num Elems | Visualization
>                     (0       , 3.99e+05) |    2359282 | ########################################
>                     (3.99e+05, 7.98e+05) |          7 |
>                     (7.98e+05, 1.2e+06 ) |          4 |
>                     (1.2e+06 , 1.6e+06 ) |          1 |
>                     (1.6e+06 , 2e+06   ) |          0 |
>                     (2e+06   , 2.4e+06 ) |          1 |
>                     (2.4e+06 , 2.79e+06) |          0 |
>                     (2.79e+06, 3.19e+06) |          0 |
>                     (3.19e+06, 3.59e+06) |          0 |
>                     (3.59e+06, 3.99e+06) |          1 |
> [E]         FAILED | Output: 'output' | Difference exceeds tolerance (rel=0.1, abs=0.1)
> [E] FAILED | Runtime: 24.394s | Command: /usr/local/bin/polygraphy run /tmp/Janus-Pro-7B/vision_encoder_bfp16.onnx --onnxrt --trt --save-engine=/tmp/Janus-Pro-7B/vision_encoder_bfp16.trt --trt-min-shapes input:[1,3,384,384] --trt-opt-shapes input:[1,3,384,384] --trt-max-shapes input:[8,3,384,384] --input-shapes input:[-1,3,384,384] --atol 1e-1 --rtol 1e-1 --fail-fast
> ```
> 
> 
> ## Environment
> 
> <!-- Please share any setup information you know. This will help us to understand and address your case. -->
> 
> **TensorRT Version**: 10.7.0
> 
> **NVIDIA GPU**: A10
> 
> **NVIDIA Driver Version**: 550.90.07
> 
> **CUDA Version**: 12.6
> 
> **CUDNN Version**: 9.6.0
> 
> 
> Operating System: Ubuntu 24.04.1 LTS \n \l
> 
> Python Version (if applicable):  3.12.3
> 
> 

---

### [python: ../../../lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp:36: int mlir::triton::gpu::(anonymous namespace)::getMMAVersionSafe(int, DotOp): Assertion `false && "computeCapability not supported"' failed.](https://github.com/triton-lang/triton/issues/6087)

**Created:** 2025-03-03T04:58:47Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> 我在使用5080的运行Ktransformers的时候，出现这种报错
> 
> python: ../../../lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp:36: int mlir::triton::gpu::(anonymous namespace)::getMMAVersionSafe(int, DotOp): Assertion `false && "computeCapability not supported"' failed.
> 
> 经查验，问题出现在这里
> // Get the highest version supported for the hardware and the dot.
> static int getMMAVersionSafe(int computeCapability, DotOp op) {
>   // List supported mma version in order of preference.
>   SmallVector<int> versionsSupported;
>   if (computeCapability < 75) {
>     versionsSupported = {1};
>   } else if (computeCapability < 90) {
>     versionsSupported = {2};
>   } else if (computeCapability < 100) {
>     versionsSupported = {3, 2};
>   } else if (computeCapability < 110) {
>     versionsSupported = {5, 2};
>   } else {
>     assert(false && "computeCapability not supported");
>   }
>   for (int baseVersion : versionsSupported) {
>     if (supportMMA(op, baseVersion))
>       return baseVersion;
>     if (baseVersion == 3)
>       op.emitRemark() << "Warning: can't use MMA V3 for the dot op";
>   }
>   return 0;
> }
> 
> 能否更改computeCapability < 110
> 改为computeCapability 支持120以及最新的显卡能力
> 谢谢！
> 
> ### Environment details
> 
> triton版本: 3.2.0
> GPU：Nvidia RTX 5080
> Python版本：3.11.11
> PyTorch版本: 2.7.0.dev20250302+cu128, CUDA可用: True
> Ktransformers版本: 0.2.2rc1

---

### [LLVM upgrade broke gcc/glibc++ 11.5](https://github.com/triton-lang/triton/issues/6747)

**Created:** 2025-05-08T04:24:50Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> Compiling from source:
> ```
> /triton $ pip install -e .
> ...
> /home/gabeferns/.triton/llvm/llvm-092b6e73-ubuntu-x64/bin/mlir-tblgen: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/gabeferns/.triton/llvm/llvm-092b6e73-ubuntu-x64/bin/mlir-tblgen)
> ...
> /triton $ strings /lib64/libstdc++.so.6 | grep GLIBCXX
> ...
> GLIBCXX_3.4.28
> GLIBCXX_3.4.29
> GLIBCXX_DEBUG_MESSAGE_LENGTH
> ...
> /triton $ gcc --version
> gcc (GCC) 11.5.0 20240719 (Red Hat 11.5.0-5)
> ```
> 
> bisected to this commit: https://github.com/triton-lang/triton/commit/6633170d54dc65e17b307d2bba15f8c3492178c7
> 
> ### Environment details
> 
> Triton: commit hash above ^
> GPU: H100

---

### [LinearEncodingAttr::getShapePerCTATile() is wrong for amd_mfma layout](https://github.com/triton-lang/triton/issues/6219)

**Created:** 2025-03-17T20:54:43Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> getShapePerCTATile(RankedTensorType type) seems incorrect for type = 
> ```
> tensor<128x64xf32, #ttg.amd_mfma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 1], instrShape = [32, 32],   isTransposed = true}>>
> ```
> 
> In this case, it looks like the function `LinearEncodingAttr::getShapePerCTATile()` is calculating
> ```
> sizePerThread: 1x4
> threadsPerWarp: 32x2
> warpsPerCTA: 4x1
> shape: 128x8
> ```
> Whereas it should be calculating
> ```
> sizePerThread: 1x32 ***
> threadsPerWarp: 32x2
> warpsPerCTA: 4x1
> shape: 128x64
> ```
> Because the linear layout basis vectors are
> ```
> register = [[0, 1], [0, 2], [0, 8], [0, 16], [0, 32]],
> lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 4]],
> warp = [[32, 0], [64, 0]],
> ```
> So all the register basis lie within the warp bases, so they are not sepparate tiles.
> 
> 
> ### Environment details
> 
> Triton:
> commit 7380904496abc8e86ed2234113637bd8929f11eb
> Author: Thomas Raoux <thomas.raoux@openai.com>
> Date:   Tue Mar 11 19:09:31 2025 -0700
> 
>     [AMD] Disable loop pipelining when there is assert or print (#6180)

---

### [Inconsistent Behavior of tl.reduce Operator between Interpreter and Codegen Modes](https://github.com/triton-lang/triton/issues/6490)

**Created:** 2025-04-15T02:36:48Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> I have found that the following test case can pass in the codegen mode, but in the interpreter mode, it will report triton.runtime.errors.InterpreterError: TypeError("'int' object is not iterable"). 
> ```python
> @triton.jit
> def add(a, b):
>     return a + b
> 
> @triton.jit
> def sanitize_sum_kernel(Z, X, BLOCK: tl.constexpr):
>     vals = tl.load(X + tl.arange(0, BLOCK))
>     z, = tl.reduce((vals,), 0, add)
>     tl.store(Z, z)
> 
> BLOCK = 512
> torch.manual_seed(42)
> X = torch.randint(0, 10, [BLOCK], device="cuda", dtype=torch.int32)
> X[:300] = 32 
> X[300:] = 0
> Z = torch.zeros((), device="cuda", dtype=torch.int32)
> sanitize_sum_kernel[(1, )](Z, X, BLOCK=BLOCK)
> torch.testing.assert_close(Z, X.sum().to(torch.int32))
> ```
> If the following modifications are made, the interpreter can pass, but the codegen will report:
> 
> triton.compiler.errors.CompilationError: at 4:4:
> def sanitize_sum_kernel(Z, X, BLOCK: tl.constexpr):
>     vals = tl.load(X + tl.arange(0, BLOCK))
>     z = tl.reduce((vals,), 0, add)
>     tl.store(Z, z)
>     ^
> 
> ```python
> @triton.jit
> def add(a, b):
>     return a + b
> 
> @triton.jit
> def sanitize_sum_kernel(Z, X, BLOCK: tl.constexpr):
>     vals = tl.load(X + tl.arange(0, BLOCK))
>     z = tl.reduce((vals,), 0, add)
>     tl.store(Z, z)
> 
> BLOCK = 512
> torch.manual_seed(42)
> X = torch.randint(0, 10, [BLOCK], device="cuda", dtype=torch.int32)
> X[:300] = 32 
> X[300:] = 0
> Z = torch.zeros((), device="cuda", dtype=torch.int32)
> sanitize_sum_kernel[(1, )](Z, X, BLOCK=BLOCK)
> torch.testing.assert_close(Z, X.sum().to(torch.int32))
> ```
> I'd like to confirm which usage is in line with the design in an ideal situation. 
> 
> ### Environment details
> 
> Triton: 3.3.x
> commit id: 65c6b88165a169a659935443dafe887f24f1592a

---

### [[Triton-MLIR] TritonGPUCombineOps failed with scf.for](https://github.com/triton-lang/triton/issues/748)

**Created:** 2022-10-08T03:38:53Z

**Tags:** `bug`

**Content:**

> input IR to TritonGPUCombineOps pass:
> 
> ```
> #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
> #blocked1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [16], warpsPerCTA = [4], order = [0]}>
> module attributes {"triton_gpu.num-warps" = 4 : i32} {
>   func @kernel_0d1d2d3d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {
>     %c256_i32 = arith.constant 256 : i32
>     %c256 = arith.constant 256 : index
>     %c0 = arith.constant 0 : index
>     %0 = tt.get_program_id {axis = 0 : i32} : i32
>     %1 = arith.index_cast %arg3 : i32 to index
>     %2:3 = scf.for %arg4 = %c0 to %1 step %c256 iter_args(%arg5 = %arg0, %arg6 = %arg1, %arg7 = %arg2) -> (!tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>) {
>       %3 = arith.muli %0, %arg3 : i32
>       %4 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>
>       %5 = tt.splat %3 : (i32) -> tensor<256xi32, #blocked0>
>       %6 = arith.addi %5, %4 : tensor<256xi32, #blocked0>
>       %7 = tt.splat %arg5 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %8 = tt.addptr %7, %6 : tensor<256x!tt.ptr<f32>, #blocked0>
>       %9 = tt.splat %arg6 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %10 = tt.addptr %9, %6 : tensor<256x!tt.ptr<f32>, #blocked0>
>       %11 = triton_gpu.convert_layout %8 : (tensor<256x!tt.ptr<f32>, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked1>
>       %12 = tt.load %11 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked1>
>       %13 = triton_gpu.convert_layout %12 : (tensor<256xf32, #blocked1>) -> tensor<256xf32, #blocked0>
>       %14 = triton_gpu.convert_layout %10 : (tensor<256x!tt.ptr<f32>, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked1>
>       %15 = tt.load %14 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked1>
>       %16 = triton_gpu.convert_layout %15 : (tensor<256xf32, #blocked1>) -> tensor<256xf32, #blocked0>
>       %17 = arith.addf %13, %16 : tensor<256xf32, #blocked0>
>       %18 = tt.splat %arg7 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %19 = tt.addptr %18, %6 : tensor<256x!tt.ptr<f32>, #blocked0>
>       %20 = triton_gpu.convert_layout %19 : (tensor<256x!tt.ptr<f32>, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked1>
>       %21 = triton_gpu.convert_layout %17 : (tensor<256xf32, #blocked0>) -> tensor<256xf32, #blocked1>
>       tt.store %20, %21 : tensor<256xf32, #blocked1>
>       %22 = tt.addptr %arg5, %c256_i32 : !tt.ptr<f32>
>       %23 = tt.addptr %arg6, %c256_i32 : !tt.ptr<f32>
>       %24 = tt.addptr %arg7, %c256_i32 : !tt.ptr<f32>
>       scf.yield %22, %23, %24 : !tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>
>     }
>     return
>   }
> }
> 
> ```
> 
> Reported error:
> 
> ```
> error: 'triton_gpu.convert_layout' op operand #0 must be tensor of floating-point values or tensor of integer values or tensor of pointer type values, but got '!tt.ptr<f32>'
> // -----// IR Dump After TritonGPUCombineOps Failed //----- //
> #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [16], warpsPerCTA = [4], order = [0]}>
> #blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
> "builtin.module"() ({
>   "builtin.func"() ({
>   ^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32):
>     %0 = "arith.constant"() {value = 256 : i32} : () -> i32
>     %1 = "arith.constant"() {value = 256 : index} : () -> index
>     %2 = "arith.constant"() {value = 0 : index} : () -> index
>     %3 = "tt.get_program_id"() {axis = 0 : i32} : () -> i32
>     %4 = "arith.index_cast"(%arg3) : (i32) -> index
>     %5 = "triton_gpu.convert_layout"(%arg0) : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>     %6:3 = "scf.for"(%2, %4, %1, %5, %arg1, %arg2) ({
>     ^bb0(%arg4: index, %arg5: tensor<256x!tt.ptr<f32>, #blocked0>, %arg6: !tt.ptr<f32>, %arg7: !tt.ptr<f32>):
>       %7 = "arith.muli"(%3, %arg3) : (i32, i32) -> i32
>       %8 = "tt.splat"(%7) : (i32) -> tensor<256xi32, #blocked0>
>       %9 = "tt.make_range"() {end = 256 : i32, start = 0 : i32} : () -> tensor<256xi32, #blocked0>
>       %10 = "tt.splat"(%7) : (i32) -> tensor<256xi32, #blocked0>
>       %11 = "tt.make_range"() {end = 256 : i32, start = 0 : i32} : () -> tensor<256xi32, #blocked0>
>       %12 = "tt.splat"(%7) : (i32) -> tensor<256xi32, #blocked0>
>       %13 = "tt.make_range"() {end = 256 : i32, start = 0 : i32} : () -> tensor<256xi32, #blocked0>
>       %14 = "tt.splat"(%arg5) : (tensor<256x!tt.ptr<f32>, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %15 = "arith.addi"(%12, %13) : (tensor<256xi32, #blocked0>, tensor<256xi32, #blocked0>) -> tensor<256xi32, #blocked0>
>       %16 = "tt.splat"(%arg6) : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %17 = "arith.addi"(%10, %11) : (tensor<256xi32, #blocked0>, tensor<256xi32, #blocked0>) -> tensor<256xi32, #blocked0>
>       %18 = "tt.addptr"(%14, %15) : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %19 = "tt.load"(%18) {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : (tensor<256x!tt.ptr<f32>, #blocked0>) -> tensor<256xf32, #blocked0>
>       %20 = "triton_gpu.convert_layout"(%19) : (tensor<256xf32, #blocked0>) -> tensor<256xf32, #blocked1>
>       %21 = "tt.addptr"(%16, %17) : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %22 = "tt.load"(%21) {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : (tensor<256x!tt.ptr<f32>, #blocked0>) -> tensor<256xf32, #blocked0>
>       %23 = "triton_gpu.convert_layout"(%22) : (tensor<256xf32, #blocked0>) -> tensor<256xf32, #blocked1>
>       %24 = "arith.addf"(%20, %23) : (tensor<256xf32, #blocked1>, tensor<256xf32, #blocked1>) -> tensor<256xf32, #blocked1>
>       %25 = "tt.splat"(%arg7) : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %26 = "arith.addi"(%8, %9) : (tensor<256xi32, #blocked0>, tensor<256xi32, #blocked0>) -> tensor<256xi32, #blocked0>
>       %27 = "tt.addptr"(%25, %26) : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %28 = "triton_gpu.convert_layout"(%24) : (tensor<256xf32, #blocked1>) -> tensor<256xf32, #blocked0>
>       "tt.store"(%27, %28) : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>) -> ()
>       %29 = "tt.addptr"(%arg5, %0) : (tensor<256x!tt.ptr<f32>, #blocked0>, i32) -> tensor<256x!tt.ptr<f32>, #blocked0>
>       %30 = "tt.addptr"(%arg6, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
>       %31 = "tt.addptr"(%arg7, %0) : (!tt.ptr<f32>, i32) -> !tt.ptr<f32>
>       "scf.yield"(%29, %30, %31) : (tensor<256x!tt.ptr<f32>, #blocked0>, !tt.ptr<f32>, !tt.ptr<f32>) -> ()
>     }) : (index, index, index, tensor<256x!tt.ptr<f32>, #blocked0>, !tt.ptr<f32>, !tt.ptr<f32>) -> (tensor<256x!tt.ptr<f32>, #blocked0>, !tt.ptr<f32>, !tt.ptr<f32>)
>     "std.return"() : () -> ()
>   }) {arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], sym_name = "kernel_0d1d2d3d", type = (!tt.ptr<f32>, !tt.ptr<f32>, !tt.ptr<f32>, i32) -> ()} : () -> ()
> }) {"triton_gpu.num-warps" = 4 : i32} : () -> ()
> 
> ```

---

### [statically_implemented_functions as a class variable?](https://github.com/triton-lang/triton/issues/1443)

**Created:** 2023-03-30T00:15:22Z

**Tags:** `bug`

**Content:**

> Hi @mcskatkat , we found that you defined `statically_implemented_functions` as a class variable, which could be overwritten when a triton jit function calls another jit function. But we don't understand the purpose here, so would like to consult you if there's any specific reason?
> 
> [statically_implemented_functions](https://github.com/openai/triton/blob/43eed392df38f2e1b5477333444c7d1235c3ec49/python/triton/compiler.py#L165)
> 
> Test case:
> 
> ```
> import triton
> import triton.language as tl
> import torch
> 
> @triton.jit
> def error(input, output):
>   a = tl.load(input + tl.arange(0, 4))
>   b = tl.zeros([4], dtype=tl.float16)
>   tl.static_print(b)
>   tl.store(output + tl.arange(0, 4), b)
> 
> input = torch.ones([4], dtype=torch.float16, device="cuda:0")
> output = torch.empty([4], dtype=torch.float16, device="cuda:0")
> 
> num_warps = 2
> error[(1,)](input, output)
> print(output)
> ```

---

### [ModuleNotFoundError: No module named 'triton.ops'](https://github.com/triton-lang/triton/issues/5471)

**Created:** 2024-12-20T04:11:20Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> I installed triton from source following the commands on git hub. It installed successfully but I get this error:
> 
> ---------------------------------------------------------------------------
> ModuleNotFoundError                       Traceback (most recent call last)
> Cell In[1], line 1
> ----> 1 from unsloth import FastLanguageModel
>       2 import torch
>       3 max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
> 
> File ~/.local/lib/python3.13/site-packages/unsloth/__init__.py:107
>     104 pass
>     106 # Try loading bitsandbytes and triton
> --> 107 import bitsandbytes as bnb
>     109 if "SPACE_AUTHOR_NAME" not in os.environ and "SPACE_REPO_NAME" not in os.environ:
>     111     import triton
> 
> File ~/.local/lib/python3.13/site-packages/bitsandbytes/__init__.py:15
>       6 from . import research, utils
>       7 from .autograd._functions import (
>       8     MatmulLtState,
>       9     bmm_cublas,
>    (...)
>      13     mm_cublas,
>      14 )
> ---> 15 from .nn import modules
>      16 from .optim import adam
>      18 __pdoc__ = {
>      19     "libbitsandbytes": False,
>      20     "optim.optimizer.Optimizer8bit": False,
>      21     "optim.optimizer.MockArgs": False,
>      22 }
> 
> File ~/.local/lib/python3.13/site-packages/bitsandbytes/nn/__init__.py:21
>       1 # Copyright (c) Facebook, Inc. and its affiliates.
>       2 #
>       3 # This source code is licensed under the MIT license found in the
>       4 # LICENSE file in the root directory of this source tree.
>       5 from .modules import (
>       6     Embedding,
>       7     Embedding4bit,
>    (...)
>      19     SwitchBackLinearBnb,
>      20 )
> ---> 21 from .triton_based_modules import (
>      22     StandardLinear,
>      23     SwitchBackLinear,
>      24     SwitchBackLinearGlobal,
>      25     SwitchBackLinearVectorwise,
>      26 )
> 
> File ~/.local/lib/python3.13/site-packages/bitsandbytes/nn/triton_based_modules.py:7
>       4 import torch.nn as nn
>       6 from bitsandbytes.triton.dequantize_rowwise import dequantize_rowwise
> ----> 7 from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
>       8     int8_matmul_mixed_dequantize,
>       9 )
>      10 from bitsandbytes.triton.int8_matmul_rowwise_dequantize import (
>      11     int8_matmul_rowwise_dequantize,
>      12 )
>      13 from bitsandbytes.triton.quantize_columnwise_and_transpose import (
>      14     quantize_columnwise_and_transpose,
>      15 )
> 
> File ~/.local/lib/python3.13/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py:12
>      10 import triton
>      11 import triton.language as tl
> ---> 12 from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
>      14 # This is a matmul kernel based on triton.ops.matmul
>      15 # It is modified to support rowwise quantized input and global quantized weight
>      16 # It's purpose is fused matmul then dequantize
>      17 # It does support bias.
>      19 def init_to_zero(name):
> 
> ModuleNotFoundError: No module named 'triton.ops'
> 
> when I run:
> from unsloth import FastLanguageModel
> 
> ### Environment details
> 
> I am using Linux Ubuntu 24.01 LTS. I have a RTX 3050 TI GPU. Cuda Version 12.4 and Pytorch 2.5.1

---

### [AttributeError when implementing a triton.jit func with multiple decorators](https://github.com/triton-lang/triton/issues/5224)

**Created:** 2024-11-22T01:47:18Z

**Tags:** `bug needs reproducer`

**Content:**

> ### Describe the bug
> 
> Hi, I encountered an error when implementing a triton.jit function and its seems to be caused by multiple decorators. Below is the detailed error and the code. Could you please help me?
> 
> The error info:
> 
> ```
>     def _cce_backward_kernel(
>   File "/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py", line 885, in jit
>     return decorator(fn)
>   File "/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py", line 874, in decorator
>     return JITFunction(
>   File "/opt/conda/lib/python3.10/site-packages/triton/runtime/jit.py", line 720, in __init__
>     **self.src = self.src[re.search(r"^def\s+\w+\s*\(", self.src, re.MULTILINE).start():]
> AttributeError: 'NoneType' object has no attribute 'start'**
> ```
> 
> 
> The code: ( two parts: the _cce_backward_kernel and one of its decorator cce_backward_autotune)
> 
> ```
> @cce_backward_autotune()
> @triton.heuristics(
>     {
>         "EVEN_D": lambda args: (args["D"] % args["BLOCK_D"]) == 0,
>         "MM_BACK_BLOCK_D": lambda args: args["BLOCK_D"] * 2,
>         "MM_BACK_EVEN_D": lambda args: (args["D"] % (args["BLOCK_D"] * 2)) == 0,
>         "HAS_VALIDS": lambda args: args["Valids"] is not None,
>         "HAS_VOCAB_ORDERING": lambda args: args["VocabOrdering"] is not None,
>         "FILTER_GRAD": lambda args: args["filter_eps"] is not None,
>         "HAS_TARGETS": lambda args: args["Targets"] is not None,
>         "HAS_SOFTCAP": lambda args: args["softcap"] is not None,
>         "ITEM_DO": lambda args: args["dOut"].numel() == 1,
>         "GROUP_B": lambda args: 8,
>     }
> )
> @triton.jit
> def _cce_backward_kernel(
>     E,
>     C,
>     LSE,
>     dOut,
>     grad_scale,
>     Valids,
>     VocabOrdering,
>     softcap,
>     Targets,
>     dE,
>     dELocks,
>     dC,
>     dCLocks,
>     B,
>     D,
>     V,
>     n_de_locks_0,
>     n_de_locks_1,
>     n_dc_locks_0,
>     n_dc_locks_1,
>     stride_eb,
>     stride_ed,
>     stride_cv,
>     stride_cd,
>     stride_vb,
>     filter_eps,
>     B_BIN,
>     BLOCK_B: tl.constexpr,
>     BLOCK_V: tl.constexpr,
>     BLOCK_D: tl.constexpr,
>     MM_BACK_BLOCK_D: tl.constexpr,
>     GROUP_B: tl.constexpr,
>     EVEN_D: tl.constexpr,
>     MM_BACK_EVEN_D: tl.constexpr,
>     ITEM_DO: tl.constexpr,
>     HAS_VALIDS: tl.constexpr,
>     HAS_VOCAB_ORDERING: tl.constexpr,
>     FILTER_GRAD: tl.constexpr,
>     HAS_TARGETS: tl.constexpr,
>     HAS_SOFTCAP: tl.constexpr,
>     SHIFT: tl.constexpr,
> ):
>     pid = tl.program_id(axis=0)
>     num_b_chunks = tl.cdiv(B, BLOCK_B)
>     num_v_chunks = tl.cdiv(V, BLOCK_V)
>     num_v_in_group = GROUP_B * num_v_chunks
>     group_id = pid // num_v_in_group
>     first_pid_b = group_id * GROUP_B
>     group_size_b = min(num_b_chunks - first_pid_b, GROUP_B)
>     pid_b = first_pid_b + ((pid % num_v_in_group) % group_size_b)
>     pid_v = (pid % num_v_in_group) // group_size_b
> 
>     offs_b = (pid_b * BLOCK_B + tl.arange(0, BLOCK_B)) % B
>     if HAS_VALIDS:
>         offs_b = tl.load(Valids + stride_vb * offs_b)
> 
>     offs_v = (pid_v * BLOCK_V + tl.arange(0, BLOCK_V)) % V
>     if HAS_VOCAB_ORDERING:
>         offs_v = tl.load(VocabOrdering + offs_v)
> 
>     offs_d = tl.arange(0, BLOCK_D)
>     e_ptrs = E + (offs_b[:, None] * stride_eb + offs_d[None, :] * stride_ed)
>     c_ptrs = C + (offs_v[None, :] * stride_cv + offs_d[:, None] * stride_cd)
> 
>     accum = tl.zeros((BLOCK_B, BLOCK_V), dtype=tl.float32)
>     for d in range(0, tl.cdiv(D, BLOCK_D)):
>         if EVEN_D:
>             e = tl.load(e_ptrs)
>             c = tl.load(c_ptrs)
>         else:
>             e = tl.load(e_ptrs, mask=offs_d[None, :] < D - d * BLOCK_D, other=0.0)
>             c = tl.load(c_ptrs, mask=offs_d[:, None] < D - d * BLOCK_D, other=0.0)
> 
>         accum = tl.dot(e, c, accum)
> 
>         e_ptrs += BLOCK_D * stride_ed
>         c_ptrs += BLOCK_D * stride_cd
> 
>     if HAS_SOFTCAP:
>         accum = tl_softcapping(accum, softcap)
> 
>     if HAS_VALIDS:
>         lse = tl.load(LSE + (pid_b * BLOCK_B + tl.arange(0, BLOCK_B)) % B)
>     else:
>         lse = tl.load(LSE + offs_b)
> 
>     d_accum = tl.exp(accum - lse[:, None])
> 
>     if HAS_TARGETS:
>         targets = tl.load(Targets + ((offs_b + 1) if SHIFT else offs_b))
>         is_target = targets[:, None] == offs_v[None, :]
>         d_accum += tl.where(is_target, -1.0, 0.0)
>     else:
>         is_target = None
> 
>     accum_valid_mask = ((pid_b * BLOCK_B + tl.arange(0, BLOCK_B))[:, None] < B) & (
>         (pid_v * BLOCK_V + tl.arange(0, BLOCK_V))[None, :] < V
>     )
>     d_accum = tl.where(accum_valid_mask, d_accum, 0.0)
> 
>     if FILTER_GRAD:
>         if _block_is_filtered(tl.abs(d_accum), filter_eps):
>             return
> 
>     if HAS_SOFTCAP:
>         d_accum = tl_softcapping_grad(d_accum, accum, softcap)
> 
>     if ITEM_DO:
>         d_out = tl.load(dOut)
>     else:
>         d_out = tl.load(dOut + ((offs_b + 1) if SHIFT else offs_b))[:, None]
> 
>     d_out = grad_scale * d_out
> 
>     d_accum = (d_accum * d_out).to(e_ptrs.dtype.element_ty)
> 
>     b_mask = (pid_b * BLOCK_B + tl.arange(0, BLOCK_B)[:, None]) < B
>     v_mask = (pid_v * BLOCK_V + tl.arange(0, BLOCK_V)[:, None]) < V
> 
>     lock_offset = (pid_b // tl.cdiv(B, BLOCK_B * n_de_locks_0)) * n_de_locks_1
>     dELocks += lock_offset
> 
>     _mm_backward(
>         d_accum,
>         dE + (offs_b[:, None] * stride_eb),
>         b_mask,
>         dELocks,
>         n_de_locks_1,
>         C + offs_v[:, None] * stride_cv,
>         v_mask,
>         stride_ed,
>         stride_cd,
>         D,
>         MM_BACK_BLOCK_D,
>         MM_BACK_EVEN_D,
>     )
> 
>     lock_offset = (pid_v // tl.cdiv(V, BLOCK_V * n_dc_locks_0)) * n_dc_locks_1
>     dCLocks += lock_offset
> 
>     _mm_backward(
>         tl.trans(d_accum),
>         dC + (offs_v[:, None] * stride_cv),
>         v_mask,
>         dCLocks,
>         n_dc_locks_1,
>         E + (offs_b[:, None] * stride_eb),
>         b_mask,
>         stride_cd,
>         stride_ed,
>         D,
>         MM_BACK_BLOCK_D,
>         MM_BACK_EVEN_D,
>     )
> ```
> 
> 
> cce_backward_autotune : 
> 
> 
> ```
> def cce_backward_autotune() -> Callable[..., autotuner.Autotuner | autotuner.Heuristics]:
>    kwargs = Config(dict(BLOCK_B=128, BLOCK_V=128, BLOCK_D=32), num_warps=4, num_stages=4)
>    return triton.heuristics({k: (lambda args, _v=v: _v) for k, v in config.all_kwargs().items()})
> ```
> `
> 
> I check the content of the **self.src** reported in the error info, and it's like this, which only contains the decorator:
> `
> ```
> @cce_backward_autotune()
> @triton.heuristics(
>     {
>         "EVEN_D": lambda args: (args["D"] % args["BLOCK_D"]) == 0,
>         "MM_BACK_BLOCK_D": lambda args: args["BLOCK_D"] * 2,
>         "MM_BACK_EVEN_D": lambda args: (args["D"] % (args["BLOCK_D"] * 2)) == 0,
>         "HAS_VALIDS": lambda args: args["Valids"] is not None,
>         "HAS_VOCAB_ORDERING": lambda args: args["VocabOrdering"] is not None,
>         "FILTER_GRAD": lambda args: args["filter_eps"] is not None,
>         "HAS_TARGETS": lambda args: args["Targets"] is not None,
>         "HAS_SOFTCAP": lambda args: args["softcap"] is not None,
>         "ITEM_DO": lambda args: args["dOut"].numel() == 1,
>         "GROUP_B": lambda args: 8,
>     }
> )
> ```
> 
> 
> 
> 
> ### Environment details
> 
> Triton: 3.1.0
> 
> GPU: A800-SXM
> 
> PyTorch: 2.5.1
> 
> python: 3.10.9
> 

---

### [Triton code cannot be imported on a CPU machine](https://github.com/triton-lang/triton/issues/6150)

**Created:** 2025-03-08T07:29:30Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> The following code:
> ```
> import triton
> import triton.language as tl
> 
> 
> @triton.autotune(
>     configs=[triton.Config({'BLOCK_SIZE': 512}, num_warps=4)],
>     key=['BLOCK_SIZE'],
> )
> @triton.jit
> def add_kernel(x, BLOCK_SIZE: tl.constexpr):
>     pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
> ```
> 
> On a CPU machine, run it under pytorch 2.6, it throws at module import time:
> ```
> Traceback (most recent call last):
>   File "/home/xxx/test-triton-import.py", line 10, in <module>
>     def add_kernel(x, BLOCK_SIZE: tl.constexpr):
>   File "/home/xxx/.venv26/lib/python3.10/site-packages/triton/runtime/autotuner.py", line 368, in decorator
>     return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, restore_value, pre_hook=pre_hook,
>   File "/home/xxx/.venv26/lib/python3.10/site-packages/triton/runtime/autotuner.py", line 130, in __init__
>     self.do_bench = driver.active.get_benchmarker()
>   File "/home/xxx/.venv26/lib/python3.10/site-packages/triton/runtime/driver.py", line 23, in __getattr__
>     self._initialize_obj()
>   File "/home/xxx/.venv26/lib/python3.10/site-packages/triton/runtime/driver.py", line 20, in _initialize_obj
>     self._obj = self._init_fn()
>   File "/home/xxx/.venv26/lib/python3.10/site-packages/triton/runtime/driver.py", line 8, in _create_driver
>     raise RuntimeError(f"{len(actives)} active drivers ({actives}). There should only be one.")
> RuntimeError: 0 active drivers ([]). There should only be one.
> ```
> 
> With pytorch 2.5, it finishes without error. So this is a regression. From the call stack, this might be related to https://github.com/triton-lang/triton/pull/4496 
> 
> Being able to import code on a CPU machine is important - it allows many jobs, tests, small snippets that do not require GPU to run with cheap resources. 
> 
> ### Environment details
> 
> triton.__version__ == 3.2.0

---

### [Broadcasting before tl.dot](https://github.com/triton-lang/triton/issues/584)

**Created:** 2022-07-18T08:26:17Z

**Tags:** `bug`

**Content:**

> Hi,
> 
> I was trying to implement a kernel for computing a quadratic form with a Cholesky factor and got the below code working on my local machine with a GTX 1080 Ti (for all problem and block sizes I tried). However, trying to run the code with a A100 gives wrong results and running it on a V100 causes a segfault regardless of what block sizes / number of warps I use. I am using the latest preview build (2.0.0.dev20220713).
> 
> I suspect its due to the broadcasting before the tl.dot - if I try to replace the dot function broadcasted multiply and sum it also segfaults on all the machines I tried.
> 
> ```python
> import torch
> 
> import triton
> import triton.language as tl
> 
> @triton.jit
> def _chol_quad_form(W_TRIU, XN, XM, OUT,
>                     M, D,
>                     W_triu_n, W_triu_d1, W_triu_d2,
>                     xn_n, xn_d,
>                     xm_m, xm_d,
>                     out_n, out_m,
>                     BLOCK_M: tl.constexpr,
>                     BLOCK_D: tl.constexpr):
>     n = tl.program_id(0)
>     m = tl.program_id(1) * BLOCK_M
>     
>     W_TRIU = W_TRIU + n * W_triu_n
>     XN = XN + n * xn_n
>     
>     out = tl.zeros((BLOCK_M,), OUT.dtype.element_ty)
>     rm = m + tl.arange(0, BLOCK_M)
>     rd = tl.arange(0, BLOCK_D)
>     
>     for ci in range(0, D, BLOCK_D):
>         acc = tl.zeros((BLOCK_D, BLOCK_M), OUT.dtype.element_ty)
>         for cj in range(0, D, BLOCK_D):
>             xn = tl.load(XN + (cj + rd) * xn_d, mask=cj + rd < D, other=0)
>             xm = tl.load(XM + rm[None, :] * xm_m + (cj + rd)[:, None] * xm_d,
>                          mask=(rm < M)[None, :] & (cj + rd < D)[:, None],
>                          other=0)
>             w_triu = tl.load(W_TRIU + (ci + rd)[:, None] * W_triu_d2 + (cj + rd)[None, :] * W_triu_d1,
>                              mask=(ci + rd < D)[:, None] & (cj + rd < D)[None, :],
>                              other=0)
>             acc += tl.dot(w_triu, xm - xn[:, None])
>             # acc += tl.sum(w_triu[:, :, None] * (xm[None, :, :] - xn[None, :, None]), axis=1)
>         out += tl.sum(acc * acc, axis=0)
>     OUT = OUT + n * out_n + rm * out_m
>     tl.store(OUT, out, mask=rm < M)
> 
> def chol_quad_form(W_triu, Xn, Xm, out=None, BLOCK_M=128, BLOCK_D=16, num_warps=4):
>     N = Xn.size(0)
>     M = Xm.size(0)
>     D = W_triu.size(-1)
>     
>     assert N == W_triu.size(0)
>     assert Xn.size(1) == Xm.size(1) == W_triu.size(1) == W_triu.size(2)
>     
>     if out is None:
>         out = W_triu.new_empty(N, M)
>     
>     grid = lambda META: (N, triton.cdiv(M, META['BLOCK_M']))
>     _chol_quad_form[grid](W_triu, Xn, Xm, out,
>                           M, D,
>                           W_triu.stride(0), W_triu.stride(1), W_triu.stride(2),
>                           Xn.stride(0), Xn.stride(1),
>                           Xm.stride(0), Xm.stride(1),
>                           out.stride(0), out.stride(1),
>                           BLOCK_M=BLOCK_M, BLOCK_D=BLOCK_D,
>                           num_warps=num_warps)
>     return out
> 
> def chol_quad_form_torch(W_triu, Xn, Xm):
>     diff = Xm[None, :] - Xn[:, None]
>     LX = W_triu.transpose(-1, -2) @ diff.transpose(-1, -2)
>     return LX.pow_(2).sum(dim=-2)
> 
> if __name__ == '__main__':
>     torch.manual_seed(0)
>     N = 5000
>     M = 5000
>     D = 8
>     
>     W_inv = torch.rand(N, D, D, device='cuda')
>     W_inv = W_inv @ W_inv.transpose(-1, -2)
>     W_inv += torch.eye(D, device='cuda').expand_as(W_inv)
>     
>     W_inv_tril = torch.linalg.cholesky(W_inv)
>     W_triu = torch.linalg.inv(W_inv_tril).transpose(-1, -2)
>     
>     Xn = torch.randn(N, D, device='cuda')
>     Xm = torch.randn(M, D, device='cuda')
>     
>     res1 = chol_quad_form(W_triu, Xn, Xm)
>     res2 = chol_quad_form_torch(W_triu, Xn, Xm)
>     
>     print(res1)
>     print(res2)
>     assert torch.allclose(res1, res2)
> ```
> 
> Hope this proves useful :)
> 
> Best regards,
> Kenny

---

### [Interpreter doesn't handle `max()` properly](https://github.com/triton-lang/triton/issues/3310)

**Created:** 2024-03-07T16:03:22Z

**Tags:** `bug interpreter`

**Content:**

> Code to reproduce:
> 
> 
> ```python
> import torch
> import triton
> import triton.language as tl
> 
> 
> @triton.jit
> def max_error(input_ptr, input_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):
>     row_idx = tl.program_id(0)
>     row_start_ptr = input_ptr + row_idx * input_row_stride
>     col_offsets = tl.arange(0, BLOCK_SIZE)
>     input_ptrs = row_start_ptr + col_offsets
>     row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float("inf"))
>     max_value = tl.max(row, axis=0)
> 
> 
> dim0 = 1024
> dim1 = 1024
> x = torch.randn((dim0, dim1), dtype=torch.float32, device="cuda")
> 
> max_error[(dim0,)](x, x.stride(0), x.shape[1], BLOCK_SIZE=dim0)
> ```
> 
> Output:
> ```
> $ TRITON_INTERPRET=1 python max_error.py 
> Traceback (most recent call last):
>   File "max_error.py", line 20, in <module>
>     max_error[(dim0,)](x, x.stride(0), x.shape[1], BLOCK_SIZE=dim0)
>   File "/home/gw66/kern/env/lib/python3.8/site-packages/triton/runtime/interpreter.py", line 511, in __call__
>     self.fn(**args)
>   File "max_error.py", line 13, in max_error
>     max_value = tl.max(row, axis=0)
>   File "/home/gw66/kern/env/lib/python3.8/site-packages/triton/runtime/interpreter.py", line 544, in __call__
>     return self.fn(*args, **kwargs)
>   File "/home/gw66/kern/env/lib/python3.8/site-packages/triton/language/standard.py", line 169, in max
>     return core.reduce(input, axis, maximum)
>   File "/home/gw66/kern/env/lib/python3.8/site-packages/triton/runtime/interpreter.py", line 402, in _new_reduce
>     ret_type = tl.block_type(input.dtype, ret.shape)
>   File "/home/gw66/kern/env/lib/python3.8/site-packages/triton/language/core.py", line 324, in __init__
>     raise TypeError('0d block_type is forbidden')
> TypeError: 0d block_type is forbidden
> ```
> 
> Running without TRITON_INTERPRET throws no error. I am new to triton so I apologise if this is an error on my part. 

---

### [Pipeline prologue doesn't mask out lut loads correctly](https://github.com/triton-lang/triton/issues/1765)

**Created:** 2023-06-09T17:47:07Z

**Tags:** `bug`

**Content:**

> If the upper bound of a loop is small, while we are using a large numStages, it's possible that we access invalid memory addresses in the prologue. So we have to mask these loads out as we did in the for loop body.

---

### [Pipeline pass hanging](https://github.com/triton-lang/triton/issues/1652)

**Created:** 2023-05-11T00:40:54Z

**Tags:** `bug`

**Content:**

> ```python
> import torch
> 
> import triton
> import triton.language as tl
> 
> @triton.jit
> def matmul_kernel(
>     input_ptr,  # *Pointer* to indices tensor[N, ZIN]
>     kernel_ptr,  # [K, ZIN, ZOUT]
>     output_ptr, # [N, ZOUT]
>     kernel_mask_ptr,
>     n_indices,
>     K: tl.constexpr,
>     ZIN: tl.constexpr,
>     ZOUT: tl.constexpr,
>     BLOCK_SIZE: tl.constexpr,
> ):
>     # There are multiple 'programs' processing different data. We identify which program
>     # we are here:
>     pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
>     block_start = pid * BLOCK_SIZE
>     offsets = block_start + tl.arange(0, BLOCK_SIZE)
> 
>     # assume power-of-2
>     offset_zout = tl.arange(0, ZOUT)
>     offset_zin = tl.arange(0, ZIN)
> 
>     # output features: [num_voxels, ZOUT]
>     mask_indices = offsets < n_indices
>     mask_zin = offset_zin < ZIN
>     mask_zout = offset_zout < ZOUT
> 
>     input_mask = mask_indices[:, None] * mask_zin[None, :]
>     output_mask = mask_indices[:, None] * mask_zout[None, :]
> 
>     input_a_offsets = offsets[:, None] * ZIN + offset_zin[None, :]
>     input_a = tl.load(
>         input_ptr + input_a_offsets,
>         mask=input_mask,
>         other=0,
>     )
>     output_features = tl.zeros((BLOCK_SIZE, ZOUT), dtype=tl.float32)
>     for k in range(0,K):
>         mask_val = tl.load(kernel_mask_ptr + k)
>         if mask_val > 0:
>             input_b_offsets = offset_zin[:, None] * ZOUT + offset_zout[None, :]
>             input_b_offsets += k*ZIN*ZOUT
>             input_b = tl.load(kernel_ptr + input_b_offsets)
>             output_features += tl.dot(input_a, input_b)
> 
>     output_offsets = offsets[:, None] * ZOUT + offset_zout[None, :]
>     tl.store(output_ptr + output_offsets, output_features, mask=output_mask)
> 
> 
> def matmul_fn(input: torch.Tensor, kernel: torch.Tensor):
>     (n_elements, ZIN) = input.shape
>     (K, ZIN, ZOUT) = kernel.shape
>     output = torch.zeros((n_elements, ZOUT), dtype=torch.float16, device="cuda")
>     mask = torch.ones(K, dtype=torch.int32, device="cuda")
>     mask[0] = 0
>     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
>     matmul_kernel[grid](input, kernel, output, mask, n_elements, K, ZIN, ZOUT, BLOCK_SIZE=32)
>     return output
> 
> 
> if __name__ == '__main__':
>     ZIN=16
>     ZOUT=16
>     K=3
>     n_elements=32
>     input = torch.randn((n_elements, ZIN), dtype=torch.float16, device="cuda")
>     kernel = torch.randn((K, ZIN, ZOUT), dtype=torch.float16, device="cuda")
> 
>     output = matmul_fn(input, kernel)
> ```

---

### [fixed path for cuda header path](https://github.com/triton-lang/triton/issues/731)

**Created:** 2022-10-02T03:27:48Z

**Tags:** `bug`

**Content:**

> Hi, I find that `cu_include_dir` fixed to `/usr/local/cuda/include` which make cuda with custom installation path not working.
> 
> https://github.com/openai/triton/blob/998fd5f9afe166247f441999c605dfe624ca9331/python/triton/compiler.py#L1121
> 
> Maybe it's better that use environment variable `CUDA_HOME` or `TRITON_CUDA_PATH` with default value `/usr/local/cuda/include`.

---

### [Store or somthing has a bug](https://github.com/triton-lang/triton/issues/2231)

**Created:** 2023-09-02T02:22:29Z

**Tags:** `bug`

**Content:**

> ```
> import torch
> import torch.nn as nn
> import triton
> import triton.language as tl
> 
> 
> @triton.jit
> def copy(
>     output_ptr: tl.tensor,
>     input_ptr: tl.tensor,
>     y_size: tl.int32,
>     x_size: tl.int32,
>     y_stride: tl.int32,
>     x_stride: tl.int32,
>     x_block_size: tl.constexpr,
> ):
>     y_offset = tl.program_id(0)
>     output_block_ptr = tl.make_block_ptr(
>         output_ptr,
>         shape=(y_size, x_size),
>         strides=(y_stride, x_stride),
>         offsets=(y_offset, 0),
>         block_shape=(1, x_block_size),
>         order=(1, 0),
>     )
>     input_block_ptr = tl.make_block_ptr(
>         input_ptr,
>         shape=(y_size, x_size),
>         strides=(y_stride, x_stride),
>         offsets=(y_offset, 0),
>         block_shape=(1, x_block_size),
>         order=(1, 0),
>     )
>     input = tl.load(input_block_ptr, boundary_check=(1,), padding_option="zero")
>     tl.store(output_block_ptr, input, boundary_check=(1,))
> 
> 
> def size_and_stride(input: torch.Tensor, dim: int):
>     if dim == 0:
>         x_size, y_size = input.shape
>         y_stride = input.stride(1)
>         x_stride = input.stride(0)
>     else:
>         y_size, x_size = input.shape
>         y_stride = input.stride(0)
>         x_stride = input.stride(1)
> 
>     return y_size, x_size, y_stride, x_stride
> 
> 
> def passthrough(input, dim):
>     y_size, x_size, y_stride, x_stride = size_and_stride(input, dim)
>     output = torch.empty_like(input)
> 
>     copy[(y_size,)](output, input, y_size, x_size, y_stride, x_stride, triton.next_power_of_2(x_size))
> 
>     return output
> 
> y_size, x_size, dim = 2000, 4, 0
> # y_size, x_size, dim = 1000, 4, 0 # In this case, second passthrough is ok.
> input = torch.randn(y_size, x_size, device="cuda")
> 
> print(input)
> print(passthrough(input, dim))
> 
> print("-" * 20)
> 
> y_size, x_size, dim = 4, 2000, 1
> input = torch.randn(y_size, x_size, device="cuda")
> 
> print(input)
> print(passthrough(input, dim))
> ```
> 
> In this code, the result of second `passthrough` is wrong. Very starnge point is that if the first `passthrough` is not calling then second `passthrough` is correct. Or if the tensor size for first `passthrough` is [1000, 4] then the second `passthrough` is fine.
> 
> The incorrect result looks like below.
> input:
> ```
> tensor([[ 0.0677, -0.0106,  0.0849, -0.6194],
>         [ 1.1846, -1.4212,  0.4589, -0.1710],
>         [ 1.2587, -1.0013, -0.0025, -1.0008],
>         ...,
>         [ 1.4612, -0.3583,  0.5590,  0.6434],
>         [-0.0154,  0.8189,  0.7846, -1.0485],
>         [-1.3262,  0.1130,  0.2466, -0.1957]], device='cuda:0')
> ```
> output:
> ```
> tensor([[ 0.0677, -0.0106,  0.0849, -0.6194],
>         [ 1.1846, -1.4212,  0.4589, -0.1710],
>         [ 1.2587, -1.0013, -0.0025, -1.0008],
>         ...,
>         [ 2.0056, -0.1293,  1.9449,  1.2617],
>         [ 0.0473, -1.2987,  1.4613, -1.6349],
>         [ 1.7378,  1.1663,  1.0022, -0.2531]], device='cuda:0')
> ```

---

### [pointer should use int64 as its latent operating data type by default to avoid overflow](https://github.com/triton-lang/triton/issues/6748)

**Created:** 2025-05-08T04:39:42Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> Its common to do offsetting on pointers for a block of elements, e.g. loading / storing.
> 
> However, Triton doesn't use int64 as the latent data format for pointers, which leads to overflow in offsetting those pointers, and resulting in illegal memory access.
> 
> For example, 
> ```python
> @triton.autotune(
>     configs=[
>         triton.Config({
>             "BLOCK_SIZE_M": 128,
>             "BLOCK_SIZE_N": 256,
>         },
>         num_stages=3,
>         num_warps=8)
>     ],
>     key=["num_rows", "num_cols"]
> )
> @triton.jit
> def kernel_1(pointer, 
>              stride_m, 
>              stride_n,
>              num_rows: int,
>              num_cols: int,
>              BLOCK_SIZE_M: tl.constexpr,
>              BLOCK_SIZE_N: tl.constexpr):
>     pid = tl.program_id(0)
>     num_pid_m = tl.cdiv(num_rows, BLOCK_SIZE_M)
>     pid_m = pid % num_pid_m
>     pid_n = pid // num_pid_m
> 
>     offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
>     offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
> 
>     values = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
> 
>     pointers = pointer + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n
>     mask = (offs_m[:, None] < num_rows) & (offs_n[None, :] < num_cols)
>     tl.store(pointers, values, mask=mask)
> ```
> This kernel assumes stride_m and stride_n to be int32, and there is no upcasting in `pointers = pointer + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n`, resulting in wrong pointer offsetting for illegal memory access.
> 
> 
> If we manually upcast the `stride_m` and `stride_n` to int64 first, then `pointers = pointer + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n` will also be upcast implicitly, obtaining correct offsetting.
> ```python
> @triton.autotune(
>     configs=[
>         triton.Config({
>             "BLOCK_SIZE_M": 128,
>             "BLOCK_SIZE_N": 256,
>         },
>         num_stages=3,
>         num_warps=8)
>     ],
>     key=["num_rows", "num_cols"]
> )
> @triton.jit
> def kernel_2(pointer, 
>              stride_m: tl.int64, 
>              stride_n: tl.int64,
>              num_rows: int,
>              num_cols: int,
>              BLOCK_SIZE_M: tl.constexpr,
>              BLOCK_SIZE_N: tl.constexpr):
>     pid = tl.program_id(0)
>     num_pid_m = tl.cdiv(num_rows, BLOCK_SIZE_M)
>     pid_m = pid % num_pid_m
>     pid_n = pid // num_pid_m
> 
>     offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
>     offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
> 
>     values = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
> 
>     pointers = pointer + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n
>     mask = (offs_m[:, None] < num_rows) & (offs_n[None, :] < num_cols)
>     tl.store(pointers, values, mask=mask)
> ```
> 
> 
> To achieve the same offsetting effect akin to other languages, like C++, the pointer should use int64 as the latent data type, automatically upcast when doing offsetting.
> 
> 
> 
> To reproduce this issue, execute this mini-demo:
> ```python
> 
> import torch
> import triton
> import triton.language as tl
> 
> @triton.autotune(
>     configs=[
>         triton.Config({
>             "BLOCK_SIZE_M": 128,
>             "BLOCK_SIZE_N": 256,
>         },
>         num_stages=3,
>         num_warps=8)
>     ],
>     key=["num_rows", "num_cols"]
> )
> @triton.jit
> def kernel_1(pointer, 
>              stride_m, 
>              stride_n,
>              num_rows: int,
>              num_cols: int,
>              BLOCK_SIZE_M: tl.constexpr,
>              BLOCK_SIZE_N: tl.constexpr):
>     pid = tl.program_id(0)
>     num_pid_m = tl.cdiv(num_rows, BLOCK_SIZE_M)
>     pid_m = pid % num_pid_m
>     pid_n = pid // num_pid_m
> 
>     offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
>     offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
> 
>     values = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
> 
>     pointers = pointer + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n
>     mask = (offs_m[:, None] < num_rows) & (offs_n[None, :] < num_cols)
>     tl.store(pointers, values, mask=mask)
> 
> 
> @triton.autotune(
>     configs=[
>         triton.Config({
>             "BLOCK_SIZE_M": 128,
>             "BLOCK_SIZE_N": 256,
>         },
>         num_stages=3,
>         num_warps=8)
>     ],
>     key=["num_rows", "num_cols"]
> )
> @triton.jit
> def kernel_2(pointer, 
>              stride_m: tl.int64, 
>              stride_n: tl.int64,
>              num_rows: int,
>              num_cols: int,
>              BLOCK_SIZE_M: tl.constexpr,
>              BLOCK_SIZE_N: tl.constexpr):
>     pid = tl.program_id(0)
>     num_pid_m = tl.cdiv(num_rows, BLOCK_SIZE_M)
>     pid_m = pid % num_pid_m
>     pid_n = pid // num_pid_m
> 
>     offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
>     offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
> 
>     values = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
> 
>     pointers = pointer + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n
>     mask = (offs_m[:, None] < num_rows) & (offs_n[None, :] < num_cols)
>     tl.store(pointers, values, mask=mask)
>     
> 
> def main():
>     num_rows = 46605
>     num_cols = 152064
> 
>     tensor = torch.empty((num_rows, num_cols), device="cuda", dtype=torch.bfloat16).contiguous()
>     assert tensor.is_contiguous()
> 
>     def grid(meta):
>         return (triton.cdiv(num_rows, meta["BLOCK_SIZE_M"]) * triton.cdiv(num_cols, meta["BLOCK_SIZE_N"]),)
> 
>     torch.cuda.synchronize()
>     # kernel_1[grid](tensor, tensor.stride(0), tensor.stride(1),
>     #                num_rows, num_cols)
>     kernel_2[grid](tensor, tensor.stride(0), tensor.stride(1),
>                    num_rows, num_cols)
>     torch.cuda.synchronize()
> 
> if __name__ == "__main__":
>     main()
> 
> ```
> 
> ### Environment details
> 
> Triton: 3.0.0
> GPU: H100

---

### [atomic_add complains "'tt.atomic_rmw' op failed to verify that ptr type matches value type"](https://github.com/triton-lang/triton/issues/7381)

**Created:** 2025-07-02T17:19:09Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> I use the following example code to do `atomic_add` to a tensor, but get error `'tt.atomic_rmw' op failed to verify that ptr type matches value type`:
> 
> ```python
> import triton
> import triton.language as tl
> import torch
> 
> @triton.jit
> def func(
>     x_ptr, 
>     stride_row, 
>     stride_col, 
>     num_rows: tl.constexpr, 
>     num_cols: tl.constexpr, 
>     
> ):
>     
>     x_block_ptr = tl.make_block_ptr(
>         base=x_ptr, 
>         shape=(num_rows, num_cols), 
>         strides=(stride_row, stride_col), 
>         offsets=(0, 0), 
>         block_shape=(num_rows, num_cols), 
>         order=(1, 0)
>     )
> 
>     x_data = tl.load(
>         pointer=x_block_ptr, 
>         boundary_check=(), 
>         padding_option="", 
>     )
>     y_data = x_data * 2.5
> 
>     # this fails
>     tl.atomic_add(
>         pointer=x_block_ptr, 
>         val=y_data, 
>     )
> 
>      
> 
> if __name__ == "__main__":
>     num_rows = 2
>     num_cols = 4
>     x = torch.ones(size=(num_rows, num_cols), dtype=torch.float64, device="cuda")
>     print(f"before x: ", x)
> 
>     func[(1, )](
>         x_ptr=x, 
>         num_rows=num_rows, 
>         num_cols=num_cols, 
>         stride_row=x.stride(0), 
>         stride_col=x.stride(1), 
>     )
>     print(f"after x: ", x)
> ```
> 
> The full error log is the following:
> 
> ```
> error: 'tt.atomic_rmw' op failed to verify that ptr type matches value type
>         val=y_data, 
>        ^
> "builtin.module"() ({
>   "tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {}], function_type = (!tt.ptr<f64>, i32) -> (), sym_name = "func", sym_visibility = "public"}> ({
>   ^bb0(%arg0: !tt.ptr<f64>, %arg1: i32):
>     %0 = "arith.constant"() <{value = 2 : i64}> : () -> i64
>     %1 = "arith.constant"() <{value = 4 : i64}> : () -> i64
>     %2 = "arith.extsi"(%arg1) : (i32) -> i64
>     %3 = "arith.constant"() <{value = 1 : i64}> : () -> i64
>     %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
>     %5 = "arith.constant"() <{value = 0 : i32}> : () -> i32
>     %6 = "tt.make_tensor_ptr"(%arg0, %0, %1, %2, %3, %4, %5) <{order = array<i32: 1, 0>}> : (!tt.ptr<f64>, i64, i64, i64, i64, i32, i32) -> !tt.ptr<tensor<2x4xf64>>
>     %7 = "tt.load"(%6) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> : (!tt.ptr<tensor<2x4xf64>>) -> tensor<2x4xf64>
>     %8 = "arith.constant"() <{value = 2 : i32}> : () -> i32
>     %9 = "arith.constant"() <{value = 2.000000e+00 : f64}> : () -> f64
>     %10 = "arith.constant"() <{value = dense<2.000000e+00> : tensor<2x4xf64>}> : () -> tensor<2x4xf64>
>     %11 = "arith.mulf"(%7, %10) <{fastmath = #arith.fastmath<none>}> : (tensor<2x4xf64>, tensor<2x4xf64>) -> tensor<2x4xf64>
>     %12 = "arith.constant"() <{value = true}> : () -> i1
>     %13 = "tt.atomic_rmw"(%6, %11, %12) <{atomic_rmw_op = 5 : i32, scope = 1 : i32, sem = 4 : i32}> : (!tt.ptr<tensor<2x4xf64>>, tensor<2x4xf64>, i1) -> tensor<2x4xf64>
>     "tt.return"() : () -> ()
>   }) {noinline = false} : () -> ()
> }) : () -> ()
> 
> {-#
>   external_resources: {
>     mlir_reproducer: {
>       pipeline: "builtin.module(inline{default-pipeline=canonicalize inlining-threshold=4294967295 max-iterations=4 }, triton-rewrite-tensor-pointer, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-combine, triton-reorder-broadcast, cse, symbol-dce, triton-loop-unroll)",
>       disable_threading: false,
>       verify_each: true
>     }
>   }
> #-}
> my_root_dir/test_triton.py:6:0: error: Failures have been detected while processing an MLIR pass pipeline
> my_root_dir/test_triton.py:6:0: note: Pipeline failed while executing [`Inliner` on 'builtin.module' operation, `Canonicalizer` on 'tt.func' operation: @func]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
> Traceback (most recent call last):
>   File "my_root_dir/test_triton.py", line 53, in <module>
>     func[(1, )](
>   File "my_doc_dir/triton/python/triton/runtime/jit.py", line 393, in <lambda>
>     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
>   File "my_doc_dir/triton/python/triton/runtime/jit.py", line 593, in run
>     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
>   File "my_doc_dir/triton/python/triton/runtime/jit.py", line 773, in _do_compile
>     kernel = self.compile(src, target=target, options=options.__dict__)
>   File "my_doc_dir/triton/python/triton/compiler/compiler.py", line 322, in compile
>     next_module = compile_ir(module, metadata)
>   File "my_doc_dir/triton/python/triton/backends/nvidia/compiler.py", line 460, in <lambda>
>     stages["ttir"] = lambda src, metadata: self.make_ttir(src, metadata, options, capability)
>   File "my_doc_dir/triton/python/triton/backends/nvidia/compiler.py", line 227, in make_ttir
>     pm.run(mod)
> RuntimeError: PassManager::run failed
> ```
> 
> ### Environment details
> 
> GPU: NVIDIA RTX 5090
> Triton: the same error occurs in both 3.3.1 (installed by pip) and 3.4.0 version (installed from source) of triton
> PyTorch version: 2.8.0.dev20250511+cu128
> CUDA version: 12.8

---

### [Histogram fails.](https://github.com/triton-lang/triton/issues/6987)

**Created:** 2025-05-29T19:45:21Z

**Tags:** `bug`

**Content:** _(empty)_

---

### [triton.language.log does not support float64](https://github.com/triton-lang/triton/issues/543)

**Created:** 2022-06-11T00:17:24Z

**Tags:** `bug`

**Content:**

> Repro:
> ```
> import torch
> import triton
> import triton.language as tl
> 
> 
> @triton.jit
> def kernel2(in_ptr0, out_ptr5, xnumel: tl.constexpr, XBLOCK: tl.constexpr):
>     xoffset = tl.program_id(0) * XBLOCK
>     xindex = xoffset + tl.reshape(tl.arange(0, XBLOCK), [XBLOCK])
>     xmask = xindex < xnumel
>     tmp2 = tl.load(in_ptr0 + xindex, xmask)
> 
>     # VERSION 1: LLVM ERROR: Broken function found, compilation aborted!
>     tmp39 = tl.log(tmp2)
> 
>     # VERSION 2: works
>     # tmp39 = tl.log(tmp2.to(tl.float32))
> 
>     tl.store(out_ptr5 + xindex, tmp39, xmask)
> 
> 
> def call():
>     arg1 = torch.zeros((204, 204, 26), device="cuda", dtype=torch.float64)
>     buf29 = torch.zeros((204, 204, 26), device="cuda", dtype=torch.float64)
>     kernel2[(triton.cdiv(1082016, 1024),)](arg1, buf29, 1082016, 1024)
> 
> 
> call()
> ```
> 
> Output:
> ```
> $ python repro.py
> Call parameter type does not match function signature!
>   %74 = extractelement <1 x double> %71, i64 0
>  float  %97 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %74)
> Call parameter type does not match function signature!
>   %75 = extractelement <1 x double> %73, i64 0
>  float  %99 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %75)
> Call parameter type does not match function signature!
>   %81 = extractelement <1 x double> %78, i64 0
>  float  %101 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %81)
> Call parameter type does not match function signature!
>   %82 = extractelement <1 x double> %80, i64 0
>  float  %103 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %82)
> Call parameter type does not match function signature!
>   %88 = extractelement <1 x double> %85, i64 0
>  float  %105 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %88)
> Call parameter type does not match function signature!
>   %89 = extractelement <1 x double> %87, i64 0
>  float  %107 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %89)
> Call parameter type does not match function signature!
>   %95 = extractelement <1 x double> %92, i64 0
>  float  %109 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %95)
> Call parameter type does not match function signature!
>   %96 = extractelement <1 x double> %94, i64 0
>  float  %111 = call float asm "lg2.approx.f32 $0, $1;", "=f,f"(double %96)
> Invalid bitcast
>   %130 = bitcast float %98 to double
> Invalid bitcast
>   %132 = bitcast float %100 to double
> Invalid bitcast
>   %137 = bitcast float %102 to double
> Invalid bitcast
>   %139 = bitcast float %104 to double
> Invalid bitcast
>   %144 = bitcast float %106 to double
> Invalid bitcast
>   %146 = bitcast float %108 to double
> Invalid bitcast
>   %151 = bitcast float %110 to double
> Invalid bitcast
>   %153 = bitcast float %112 to double
> in function kernel2__Pfp64_Pfp64__2c1082016_3c1024
> LLVM ERROR: Broken function found, compilation aborted!
> zsh: abort (core dumped)  python repro.py
> ```

---

### [Lowering to llir fails for F16 dots with pipelining enabled on Blackwell](https://github.com/triton-lang/triton/issues/7033)

**Created:** 2025-06-03T14:54:50Z

**Tags:** `bug`

**Content:**

> ### Describe the bug
> 
> The following module fails during `ConvertTritonGPUToLLVM` for Blackwell when pipelining is enabled:
> 
> ```
> module {
>   tt.func @gemm_fusion_dot_5_impl(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {
>     %cst = arith.constant dense<1.000000e+00> : tensor<64x64xf16>
>     %c1_i64 = arith.constant 1 : i64
>     %c64_i32 = arith.constant 64 : i32
>     %c128_i32 = arith.constant 128 : i32
>     %c2_i64 = arith.constant 2 : i64
>     %c128_i64 = arith.constant 128 : i64
>     %c256_i64 = arith.constant 256 : i64
>     %c0_i32 = arith.constant 0 : i32
>     %c2_i32 = arith.constant 2 : i32
>     %c8_i32 = arith.constant 8 : i32
>     %c32_i32 = arith.constant 32 : i32
>     %cst_0 = arith.constant dense<0.000000e+00> : tensor<64x32xf32>
>     %0 = tt.get_program_id x : i32
>     %1 = arith.divsi %0, %c32_i32 : i32
>     %2 = arith.muli %1, %c8_i32 : i32
>     %3 = arith.subi %c2_i32, %2 : i32
>     %4 = arith.cmpi slt, %3, %c8_i32 : i32
>     %5 = arith.select %4, %3, %c8_i32 : i32
>     %6 = arith.remsi %0, %5 : i32
>     %7 = arith.addi %2, %6 : i32
>     %8 = arith.remsi %0, %c32_i32 : i32
>     %9 = arith.divsi %8, %5 : i32
>     %10 = arith.muli %9, %c32_i32 : i32
>     %11 = tt.get_program_id y : i32
>     %12 = tt.addptr %arg0, %11 : !tt.ptr<f16>, i32
>     %13 = tt.make_tensor_ptr %12, [%c128_i64, %c128_i64], [%c256_i64, %c2_i64], [%c0_i32, %c0_i32] {order = array<i32: 1, 0>} : <tensor<64x32xf16>>
>     %14 = tt.advance %13, [%c0_i32, %10] : <tensor<64x32xf16>>
>     %15:2 = scf.for %arg2 = %c0_i32 to %c128_i32 step %c64_i32 iter_args(%arg3 = %14, %arg4 = %cst_0) -> (!tt.ptr<tensor<64x32xf16>>, tensor<64x32xf32>)  : i32 {
>       %22 = tt.load %arg3 : !tt.ptr<tensor<64x32xf16>>
>       %23 = tt.advance %arg3, [%c64_i32, %c0_i32] : <tensor<64x32xf16>>
>       %24 = tt.dot %cst, %22, %arg4, inputPrecision = tf32 : tensor<64x64xf16> * tensor<64x32xf16> -> tensor<64x32xf32>
>       scf.yield %23, %24 : !tt.ptr<tensor<64x32xf16>>, tensor<64x32xf32>
>     }
>     %16 = arith.truncf %15#1 : tensor<64x32xf32> to tensor<64x32xf16>
>     %17 = arith.muli %7, %c64_i32 : i32
>     %18 = arith.muli %11, %c128_i32 : i32
>     %19 = tt.addptr %arg1, %18 : !tt.ptr<f16>, i32
>     %20 = tt.make_tensor_ptr %19, [%c128_i64, %c128_i64], [%c1_i64, %c256_i64], [%c0_i32, %c0_i32] {order = array<i32: 1, 0>} : <tensor<64x32xf16>>
>     %21 = tt.advance %20, [%17, %10] : <tensor<64x32xf16>>
>     tt.store %21, %16 : !tt.ptr<tensor<64x32xf16>>
>     tt.return
>   }
> }
> ```
> 
> ----
> 
> The error:
> 
> ```
> /tmp/test.ttir:31:13: error: cp.async does not support transfers smaller than 4 bytes; calculated this as 2 bytes                    
>       %22 = tt.load %arg3 : !tt.ptr<tensor<64x32xf16>>                                  
>             ^                                                                                                                                                                                                   
> /tmp/test.ttir:31:13: error: failed to legalize operation 'ttg.async_copy_global_to_local' that was explicitly marked illegal
>       %22 = tt.load %arg3 : !tt.ptr<tensor<64x32xf16>>                                                                                                                                                          
>             ^
> ```
> 
> ----
> 
> Configuration: 
> 
> ```
>     target=GPUTarget(backend="cuda", arch=100, warp_size=32),
>     options={
>         "num_warps": 4,
>         "num_stages": 3,
>     },
> ```
> 
> ----
> 
> Bisected culprit: https://github.com/triton-lang/triton/commit/5b7b86ae500276dab9460dbd614462450e0c45db
> 
> ----
> 
> Reproduced it with triton.compile (including make_ttir step).
> 
> I tried to narrow down the problem, followed the rewriter of [triton::gpu::AsyncCopyGlobalToLocalOp](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/LoadStoreOpToLLVM.cpp#L1128). 
> 
> I figured out that `maxVec` ends up being 1, which then leads to `vecTy = <1xf16>`. Then `vecBytes` becomes `1 * 16 / 8 = 2` bytes. Which triggers the error by the subsequent check `if (vecBytes < 4)` => `cp.async instruction requires a minimum transfer size of 4 bytes`. However, it is unclear why `vecBytes` is being set to 1 here.
> 
> 
> 
> 
> 
> ### Environment details
> 
> Triton: head (https://github.com/triton-lang/triton/commit/87660247cbdc37611b306048f61a1b6b7d52a853), 
> GPU: B200

---

